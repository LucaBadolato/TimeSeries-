---
title: "20236 Time Series Analysis - Assignment 3"
author:
- Simone Arrigoni (1794692)
- Luca Badolato (3086040)
- Simone Valle (3088281)
subtitle: "Bocconi University"
date: March 16, 2020
output: 
  pdf_document
header-includes:
  \usepackage[utf8]{inputenc}
  \usepackage{setspace}
  \usepackage{algpseudocode}
  \usepackage{algorithm}
  \usepackage{bm}
  \usepackage{amsmath}
  \usepackage{amssymb}
  \usepackage{graphicx}
  \usepackage{subfig}
  \usepackage{booktabs, caption}
  \usepackage{array}
  \usepackage{threeparttable}
  \usepackage{listings}
  \usepackage{physics}
  \usepackage{float}
  \floatplacement{figure}{H}
---

```{r, include=FALSE}
knitr::opts_chunk$set(fig.pos="H")
# Load useful packages
library(utf8)
library(labeling)
library(rmarkdown)
library(knitr)
library(tseries)
library(dlm)
library(ggplot2)
library(tidyr)
```

\section{Exercise 1}
Let's consider a time series $(Y_{t})_{t\geq0}$ with $Y_{t} \in \{1,2,...,K\}$.
\begin{itemize}
\item The time series $(Y_{t})$ is a Markov chain if $Y_{0} \sim p_{0}(\cdot)$ and $Y_{t}$ depends on the past observations through $Y_{t-1}$ and only through it. In other words, if $Y_{t}$ is conditionally independent on $(Y_{0}, ..., Y_{t-2})$ given $Y_{t-1}$.
\item An example of a Markov chain is...
\end{itemize}

\section{Exercise 2}
Let's consider a homogeneous Markov chain $(Y_{t})_{t\geq0}$ with finite state-space $\{1,2,...,K\}$.
\begin{itemize}
\item The initial distribution is... The transition matrix is... For non-homogeneous Markov chains the transition matrix can be different for any $t$, i.e. $\textbf{P}_{t_{1}} \ne \textbf{P}_{t_{2}}$.
\item Supposing that $K=3$, we have that:
\begin{equation*}
P(Y_{2}=1 \; | \; Y_{1}=2, Y_{0}=2) = P(Y_{2}=1 \; | \; Y_{1}=2) = p_{2,1}
\end{equation*}
\item We also have that:
\begin{equation*}
\begin{split}
P(Y_{0}&=2, Y_{1}=2, Y_{3}=1, Y_{4}=2, Y_{5}=2, Y_{6}=2, Y_{7}=1, Y_{8}=3) \\
= & \; P(Y_{1}=2 \; | \; Y_{0}=2) \; P(Y_{3}=1 \; | \; Y_{1}=2, Y_{0}=2) \; P(Y_{4}=2 \; | \; Y_{3}=1, Y_{1}=2, Y_{0}=2) \\ & \; P(Y_{5}=2 \; | \; Y_{4}=2, Y_{3}=1,  Y_{1}=2, Y_{0}=2) \; P(Y_{6}=2 \; | \; Y_{5}=2, Y_{4}=2, Y_{3}=1, Y_{1}=2, Y_{0}=2) \\ & \; P(Y_{7}=1 \; | \; Y_{6}=2, Y_{5}=2, Y_{4}=2, Y_{3}=1, Y_{1}=2, Y_{0}=2) \\ & \; P(Y_{8}=3 \; | \; Y_{7}=1, Y_{6}=2, Y_{5}=2, Y_{4}=2, Y_{3}=1, Y_{1}=2, Y_{0}=2) \\
= & \; P(Y_{1}=2 \; | \; Y_{0}=2) \; P(Y_{3}=1 \; | \; Y_{1}=2) P(Y_{4}=2 \; | \; Y_{3}=1) \; P(Y_{5}=2 \; | \; Y_{4}=2) \; P(Y_{6}=2 \; | \; Y_{5}=2) \\ & \; P(Y_{7}=1 \; | \; Y_{6}=2) \; P(Y_{8}=3 \; | \; Y_{7}=1) \\
= & \; (p_{2,2})(p_{2,1})(p_{1,2})(p_{2,2})(p_{2,2})(p_{2,1})(p_{1,3}) \\
= & \; (p_{1,2})(p_{1,3})(p_{2,1}^{2})(p_{2,2}^{3})
\end{split}
\end{equation*}
\end{itemize}

\section{Exercise 3}
We are now considering a simple random walk.
\begin{itemize}
\item The process $(Y_{t})_{t\geq0}$ is not stationary. Indeed, the variance is not constant: it explodes with increasing $t$.
\item The process $(Y_{t})_{t\geq0}$ is a Markov chain, as it has the Markov property, namely $Y_{t}$ depends on the past observation only through $Y_{t-1}$. Indeed, conditionally on the trajectory $(y_{0}, Y_{1}=y_{1},...,Y_{t-1}=i)$, the probability law of the capital $Y_{t}$ at the next step does not depend on the entire trajectory but only on the position reached at time $t-1$, that is $Y_{t-1}=i$.
\end{itemize}

\section{Exercise 4}
Let's consider a random walk:
\begin{equation*}
Y_{t}=Y_{t+1}+Z_{t} \qquad Z_{t} \overset{i.i.d.}{\sim} N(\mu,\sigma^{2}).
\end{equation*}
\begin{itemize}
\item The mean function is... The variance function is... The process $(Y_{t})_{t\geq0}$ is not stationary because...
\item The process $(Y_{t})_{t\geq0}$ satisfies the Markov property and is thus a Markov chain.
\item The process $(R_{t})$ is stationary because...
\end{itemize}

\section{Exercise 5}
\begin{itemize}
\item Knowing the state-space, the initial value and the transition matrix, the probability that $Y_{2}=2$ can be obtained as follows:
\begin{equation*}
\begin{split}
P(Y_{2}=2)
& = \sum_{i=1}^{3} P(Y_{2}=2 \; | \; Y_{1}=i) \; P(Y_{1}=i \; | \; Y_{0}=1) \\
& = \sum_{i=1}^{3} p_{1,i} \; p_{i,2} = p_{1,1} \; p_{1,2} + p_{1,2} \; p_{2,2} + p_{1,3} \; p_{3,2} \\
& = 0.6 \cdot 0.4 + 0.4 \cdot 0.7 + 0 \cdot 0.1 = 0.52
\end{split}
\end{equation*}
\item We also have that:
\begin{equation*}
\begin{split}
P(Y_{1}=1 \; | \; Y_{2}=2, Y_{0}=1) 
& = \frac{P(Y_{2}=2 \; | \; Y_{1}=1, Y_{0}=1) \; P(Y_{1}=1 \; | \; Y_{0}=1)}{P(Y_{2}=2)} \\
& = \frac{P(Y_{2}=2 \; | \; Y_{1}=1) \; P(Y_{1}=1 \; | \; Y_{0}=1)}{P(Y_{2}=2)} \\
& = \frac{p_{1,1} \; p_{1,2}}{P(Y_{2}=2)} = \frac{0.6 \cdot 0.4}{0.52} = 0.4615
\end{split}
\end{equation*}
\end{itemize}

\section{Exercise 6}
\begin{itemize}
\item The state-space of the given Markov chain is $\mathcal{Y}=\{1,2,3\}$.
\item The marginal probability distribution of $Y_{2}$, given the starting value $Y_{0}=1$ is:
\begin{equation*}
\begin{split}
P(Y_{2}=i \; | \; Y_{0}=1)
& =
\begin{cases}
\sum_{j=1}^{3} P(Y_{2}=1 \; | \; Y_{1}=j) \; P(Y_{1}=j \; | \; Y_{0}=1) \qquad \text{if $i=1$} \\
\sum_{j=1}^{3} P(Y_{2}=2 \; | \; Y_{1}=j) \; P(Y_{1}=j \; | \; Y_{0}=1) \qquad \text{if $i=2$} \\
\sum_{j=1}^{3} P(Y_{2}=3 \; | \; Y_{1}=j) \; P(Y_{1}=j \; | \; Y_{0}=1) \qquad \text{if $i=3$}
\end{cases} \\
& = 
\begin{cases}
0.48 \qquad \text{if $i=1$} \\
0.48 \qquad \text{if $i=2$} \\
0.04 \qquad \text{if $i=3$}
\end{cases}
\end{split}
\end{equation*}
\end{itemize}

\section{Exercise 7}
\begin{itemize}
\item We have that:
\begin{equation*}
P(Y_{1}=2, Y_{2}=1 \; | \; Y_{0}=1) = 0.4 \cdot 0 = 0
\end{equation*}
\item The above Markov chain is irreducible... because...
\end{itemize}
f
\section{Exercise 8}
Let's consider a Markov chain $(Y_{t})_{t \geq 0}$ with finite state-space $\mathcal{Y}$. Our aim is to study the probability distribution of $Y_n$, given the initial state $Y_0 = j$, that is $P(Y_n = k | Y_0 = j)$ for $K = 1, ..., N$, the possible states. 

In general the random variables $Y_n$ are not identically distributed, since the distribution of $Y_n$ varies with $n$. However, if the Markov chain is eergotic, the conditional distribution of $Y_n$ given the initial state $j$ converge to the limit distribution $P ^ {\ast} (.)$. This result is provided by the Ergotic theorem, a fundamental result in Markov chain theory: 
\medskip


**Th** Let $(Y_{t})_{t \geq 0}$ be an \textbf{irriducible} and \textbf{aperiodic} Markov chain with finite state space $\textit{Y}\{ 1, ..., N \}$. Then:


$(i)$ The conditional distribution $P(Y_n = . | Y_0 = j)$ converges to a limit distribution $p ^ {\ast} (.)$, independent of $j$. More specifically, for any $j$ and $k$ :

\begin{center}
$\lim_{n \to \inf} P(Y_n = k | Y_0 = j) = p ^ {\ast} (k) > 0$
\end{center}


where $p ^ {\ast} (1), ... ,p ^ {\ast} (N)$ is a probability distribution: $p ^ {\ast} (k) > 0 \forall k \in \textit{Y}$ and $\sum_{k \in \textit{Y}} p^{\ast} = 1$.

$(ii)$ Moreover, the $p^{\ast} (k)$ satisfy
\begin{center}
$p^{\ast} (k) = \sum_{i \in \textit{Y}} p^{\ast} (i) p_{i, k}.$
\end{center}


\bigskip

The required conditions are irreducibility and aperiodicity: 

A state $j$ is *periodic* with period $t$ if a return to state $j$ is possible only in $7, 2t, 3t ...$ steps. 

A Markov chain is *irreducible* if  there are no closed set, that is, for every set $C$ of states no one-step transition in possible from any state inside $C$ to any outside $C$, i.e. $p_{j,k} = 0$ if $j \in C$ and $k \notin C$.


\section{Exercise 9}
\begin{itemize}

\item The MLE of $p_{3,1}$ is:
\begin{equation*}
\hat{p}_{3,1}=\frac{25}{150}=\frac{1}{6}=0.1\overline{6}.
\end{equation*}

\item The asymptotic confidence interval of level 0.95 for $p_{3,1}$ is:
\begin{equation*}
\begin{split}
\bigg[\hat{p}_{3,1}-1.96\sqrt{\frac{\hat{p}_{3,1}(1-\hat{p}_{3,1})}{n_{3,+}}}\text{;} \; \hat{p}_{3,1}+1.96\sqrt{\frac{\hat{p}_{3,1}(1-\hat{p}_{3,1})}{n_{3,+}}}\bigg]
& = \bigg[\frac{1}{6}-1.96\sqrt{\frac{\frac{1}{6}\cdot\frac{5}{6}}{150}}\text{;} \; \frac{1}{6}+1.96\sqrt{\frac{\frac{1}{6}\cdot\frac{5}{6}}{150}}\bigg] \\
& = \big[0.16667-1.96 \cdot 0.05270 \text{;} \; 0.16667+1.96 \cdot 0.05270\big] \\
& = \big[0.063378 \text{;} \; 0.269962\big]
\end{split}
\end{equation*}

\item Relaxing the homogeneity assumption for the Markov chain, the MLEs of $p_{3,1}(t)$ for $t=1,2$ are:
\begin{equation*}
\begin{split}
& \hat{p}_{3,1}(1)=\frac{10}{55}=0.\overline{18} \\
& \hat{p}_{3,1}(2)=\frac{10}{40}=0.25
\end{split}
\end{equation*}

\end{itemize}

\section{Exercise 10}
\begin{itemize}
\item We have that:
\begin{equation*}
P(Y_{1,1}=1, Y_{1,2}=3, Y_{1,3}=3, Y_{1,4}=2 \; | \; Y_{1,0}=1; \text{\textbf{P}}) =
\end{equation*}
\item MLE of $p_{i,j}$
\item
\end{itemize}

