---
title: "20236 Time Series Analysis - Final project"
author:
- Simone Arrigoni (1794692)
- Luca Badolato (3086040)
- Simone Valle (3088281)
subtitle: "Bocconi University"
date: "5 June 2020"
output: pdf_document
geometry: margin=2cm
nocite: '@*'
bibliography: References.bib
header-includes:
  \usepackage[utf8]{inputenc}
  \usepackage{setspace}
  \usepackage{apacite}
  \usepackage{natbib}
  \usepackage{algpseudocode}
  \usepackage{algorithm}
  \usepackage{bm}
  \usepackage{amsmath}
  \usepackage{amssymb}
  \usepackage{graphicx}
  \usepackage{subfig}
  \usepackage{booktabs, caption}
  \usepackage{array}
  \usepackage{threeparttable}
  \usepackage{listings}
  \usepackage{physics}
  \usepackage{float}
  
  \floatplacement{figure}{H}
  \usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
  \definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
  \definecolor{mylilas}{RGB}{170,55,241}
  \definecolor{codegray}{RGB}{0.5,0.5,0.5}
  \definecolor{orange}{RGB}{255,165,0}
  \DeclareMathOperator*{\E}{\mathbb{E}}
  \DeclareMathOperator*{\Ec}{\mathbb{E}_t}
  \setlength\parindent{0pt}
  \newcommand{\indep}{\perp \!\!\! \perp}
---

```{r, include=FALSE}

# Load useful packages
library(utf8)
library(labeling)
library(rmarkdown)
library(readxl)
library(httr)
library(knitr)
library(tseries)
library(tinytex)
library(scales)
library(dlm)
library(magrittr)
library(stringr)
library(depmixS4)
library(tidyverse)
library(ggthemes)
library(latex2exp)
library(kableExtra)
library(ggpubr)
library(reshape2)
library(dlm)

# Settings
knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE,
                      echo    = FALSE,
                      include = TRUE,
                      fig.pos = "H",
                      fig.align = "center",
                      out.width='80%')
```

\section{Introduction}

Pollution is one of the most debated social issues of our times. It has been attracting more and more attention given its serious implications on the health of both the Earth and human beings. The economic activity under the model of production and consumption that has been characterizing the last century is, with no doubt, at the very basis of such complex phenomenon. Only in the last fifteen years some consciousness about the importance of pollution in the long-run has evolved and serious interventions have been made. Moreover, among the numerous component which pollution can be split into, air pollution is certainly one of the most important and tightly linked to strict economic activity.

Therefore, the purpose of the present work is to model through common statistical techniques the evolution of air pollution during this last crucial fifteen years in a developped economy, i.e. the United States, trying to grasp some intuition about this fundamental phenomenon.

\section{Data}

The dataset \texttt{US\_pollution} contains monthly and daily data about air pollution for each state in the United States. It is provided by the United States Environmental Protection Agency (EPA). Specifically, four key indicators of air pollution are considered: 

\begin{itemize}
\item \textbf{Ground level ozone $\text{O}_{3}$} (\texttt{O3}): Bad ozone forms near the ground when pollutants (emitted by sources such as cars, power plants, industrial boilers, refineries, and chemical plants) react chemically in sunlight. Since it is more likely to form during warmer months, we expect to see a seasonal component. 
\item \textbf{Nitrogen dioxide $\text{NO}_{2}$} (\texttt{NO2}): It is produced as a result of road traffic and other fossil fuel combustion processes. Furthermore, its presence in air contributes to the formation and modification of other air pollutants, such as particle pollution.
\item \textbf{Carbon monoxide $\text{CO}$} (\texttt{CO}):  It forms from incomplete combustions mainly from vehicle exhausts (roughly 75\% of all carbon monoxide emissions nationwide and up to 95\% in cities), fuel combustion in industrial processes and natural sources. Cold temperatures make combustion less complete, therefore we expect also in this case a seasonal behavior. 
\item \textbf{Sulfur dioxide $\text{SO}_{2}$} (\texttt{SO2}): It is produced when sulfur-containing fuels are burned. It is common mainly in large industrial complexes. 
\end{itemize}
\medskip

Hence, the four variables of interest are interconnected and are the building blocks of a more general phenomenon, that is air pollution. Furthermore, they are related by a common latent process that can be summarized by several components such as industrialization, car traffic and regulation.

The preliminary analysis that led us from the raw data (with frequency of 4-intra-day observations) to the current dataset is made available in the GitHub repository at https://github.com/SimoneArrigoni/US_pollution. We relied on a moving average algorithm of order $120$ to smooth the data, assuming months of 30 days and a year of 360 days, with a minor approximation relative to the original data structure based on a actual 365 day-counts.

```{r echo=FALSE}
# Import the dataset
louisiana_df <- read.csv(url("https://raw.githubusercontent.com/SimoneArrigoni/US_pollution/master/Louisiana_pollution.csv"), sep = ";", header=T)
```

We will be focusing on measures for \textbf{Louisiana}, recorded monthly from January 2001 to June 2016, for a total of `r length(louisiana_df$n_months)` observations. The main advantage of studying the evolution of air pollution in the same country over the years is that this allows us to control for fixed effects and to better study the latent process of interest that relates the four pollution components.

\bigskip

We report in Table \ref{tab:louisiana_corr} pollutants'correlation coefficients.

Provided that the correlation between pollutants shows significant differences in regional patterns and that $\text{CO}$ is a good indicator for industrial and biomass burning pollution [@Logan1981], a positive and strong correlation with $\text{O}_{3}$ indicates that a region has experienced photochemical $\text{O}_{3}$ production from its precursors (including $\text{CO}$) and is aligned with previous studies in the field [@Voulgarakis2011]. $\text{CO}$ is also positively correlated with $\text{SO}_2$ which is justified by the fact that they share a main source of production, i.e. coal extraction and combustion.

Other studies report that, in the US, $\text{O}_3$ is on average positively correlated with $\text{NO}_2$, which is opposite to our empirical findings about Louisiana, where they are slightly negatively correlated. A possible explanation can be given by substitution effects: an increase of nitrogen dioxide levels can be associated to the use of more fossil fuels that produce $\text{NO}_2$ instead of others. This might be related to Louisiana's fixed effects, however further research would be beyond the purpose of our study.

```{r}
# Correlation plot
datacor = subset(louisiana_df, select=c(m_O3_mean, m_NO2_mean, m_CO_mean, m_SO2_mean))
cor.datacor = cor(datacor, use="complete.obs")
cormat = round(cor(datacor),2)
row.names(cormat) <- c("$\\text{O}_{3}$",
                       "$\\text{NO}_{2}$",
                       "$\\text{CO}$",
                       "$\\text{SO}_{2}$")

knitr::kable(cormat, "latex", booktabs=T, align="c", escape=F,
             caption="Correlation table \\label{tab:louisiana_corr}",
             col.names=c("$\\text{O}_{3}$",
                         "$\\text{NO}_{2}$",
                         "$\\text{CO}$",
                         "$\\text{SO}_{2}$")) %>%
  column_spec(1:4, width="5em") %>%
kable_styling(latex_options="hold_position")
```

We plot in Figure \ref{fig:louisiana_plots} the time series for the four pollutants under study to give a first qualitative description of the phenomenon.

```{r, fig.width=20, fig.height=6, out.width='100%', fig.cap="\\label{fig:louisiana_plots} Air pollution in the state of Louisiana"}

# Time series plot
O3 <- ggplot(louisiana_df, aes(x=n_months, y=m_O3_mean)) +
  geom_line(color="steelblue", size=0.7) +
  theme_classic(base_size=9) +
  labs(y=expression(O[3]~(ppm)), x="Year") +
  scale_x_continuous(breaks=seq(0,200,25),labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  theme(axis.text=element_text(size=19),
        axis.title.x=element_text(size=21,family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21,family="Times", margin=margin(r=5)))

NO2 <- ggplot(louisiana_df, aes(x=n_months, y=m_NO2_mean)) +
  geom_line(color="steelblue", size=0.7) +
  theme_classic(base_size=9) +
  labs(y=expression(NO[2]~(ppb)), x="Year") +
  scale_x_continuous(breaks=seq(0,200,25),labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  theme(axis.text=element_text(size=19),
        axis.title.x=element_text(size=21,family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21,family="Times", margin=margin(r=5)))

CO <- ggplot(louisiana_df, aes(x=n_months, y=m_CO_mean)) +
  geom_line(color="steelblue", size=0.7) +
  theme_classic(base_size=9) +
  labs(y=expression(CO~(ppm)), x="Year") +
  scale_x_continuous(breaks=seq(0,200,25),labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  theme(axis.text=element_text(size=19),
        axis.title.x=element_text(size=21,family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21,family="Times", margin=margin(r=5)))

SO2 <- ggplot(louisiana_df, aes(x=n_months, y=m_SO2_mean)) +
  geom_line(color="steelblue", size=0.7) +
  theme_classic(base_size=9) +
  labs(y=expression(SO[2]~(ppb)), x="Year") +
  scale_x_continuous(breaks=seq(0,200,25),labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  theme(axis.text=element_text(size=19),
        axis.title.x=element_text(size=21,family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21,family="Times", margin=margin(r=5)))

ggarrange(O3, NO2, CO, SO2,
          ncol=2, nrow=2)
```
The time series, each with its specific features, do not in general look stationary. Hence, they cannot be analized with the common models based on the assumption of covariance-stationarity, such as the well-known ARMA models.
\bigskip

\section{$\text{CO}$ series}

$\text{CO}$ is perhaps the pollutant more tightly linked with the economic and industrial network of a country. Consequently, we select this as the series of main interest for our analysis, being the purpose of this work tracking the evolution of air pollution relative to an underlying economic process.

As an exploratory step, we decompose the time series in its structural components using an additive model. Firstly, we confirm the presence of seasonality. Secondly, the series is characterized by a negative trend up to a clear breakpoint between 2007 and 2008 (around month 85). The effect of this break seems to be twofold: first, the trend, that was negative in the pre-break period, disappears; second, the variance reduces markedly in the post-break period.
These features suggest that the process may be described by a Homogeneous Hidden Markov Model with a trend component.

\section{Part 1. Univariate analysis}

\subsection{HMM for trend}

As outlined in the previous section, given the features of the $\text{CO}$ series, a suitable HMM with two states may be specified as follows:
\begin{align*}
\begin{cases}
  Y_{t}=\mu_{1} + \beta_{1}t + \varepsilon_{t}, \quad
  \varepsilon_{t}\overset{i.i.d.}{\sim}N(0,\sigma_{1}^2) \qquad \text{if} \; S_{t}=1 \\
  Y_{t}=\mu_{2}+ + \beta_{2}t + \varepsilon_{t}, \quad
  \varepsilon_{t}\overset{i.i.d.}{\sim}N(0,\sigma_{2}^2) \qquad \text{if} \; S_{t}=2 \\
\end{cases}
\end{align*}
where there are three state-dependent parameters, namely the level, $\mu_i$, the slope, $\beta_i$, and the variance, $\sigma_i$, for $i=1,2$. If our guess was correct, we would expect $\mu_1 > \mu_2$, $\beta_1<0$, $\beta_2=0$ and $\sigma_1^2>\sigma_2^2$ .

```{r echo=FALSE}
# Model specification
y <- as.numeric(louisiana_df$m_CO_mean)
nstates <- 2
set.seed(2)
HMM <- depmixS4::depmix(y ~ 1 + louisiana_df$n_months, data=data.frame(y), nstates=nstates)
```

```{r include=FALSE}
# Estimation of the unknown parameters
HMM_fit <- depmixS4::fit(HMM)
```

```{r}
# MLE
mu1hat <- paste0(round(HMM_fit@response[[1]][[1]]@parameters[["coefficients"]][["(Intercept)"]],3))
mu2hat <- paste0(round(HMM_fit@response[[2]][[1]]@parameters[["coefficients"]][["(Intercept)"]],3))
beta1hat <- paste0(round(HMM_fit@response[[1]][[1]]@parameters[["coefficients"]][["louisiana_df$n_months"]],3))
beta2hat <- paste0(round(HMM_fit@response[[2]][[1]]@parameters[["coefficients"]][["louisiana_df$n_months"]],3))
sd1hat <- paste0(round(HMM_fit@response[[1]][[1]]@parameters$sd,3))
sd2hat <- paste0(round(HMM_fit@response[[2]][[1]]@parameters$sd,3))

# se(MLE)
MLE_st.err=depmixS4::standardError(HMM_fit)
st.err_mu1hat <- paste0('(', round(MLE_st.err$se[7],3), ')')
st.err_mu2hat <- paste0('(', round(MLE_st.err$se[10],3), ')')
st.err_beta1hat <- paste0('(', round(MLE_st.err$se[8],3), ')')
st.err_beta2hat <- paste0('(', round(MLE_st.err$se[11],3), ')')
st.err_sd1hat <- paste0('(', round(MLE_st.err$se[9],3), ')')
st.err_sd2hat <- paste0('(', round(MLE_st.err$se[12],3), ')')
```

```{r}
# Build a summary table for estimates
MLEsum <- data.frame(state=c(rep("S=1",2), rep("S=2",2)),
                     mu=c(mu1hat, st.err_mu1hat, mu2hat, st.err_mu2hat),
                     beta=c(beta1hat, st.err_beta1hat,beta2hat,st.err_beta2hat),
                     sd=c(sd1hat, st.err_sd1hat, sd2hat, st.err_sd2hat))
knitr::kable(MLEsum, "latex", booktabs=T, align="c",
             col.names = c("State", "$\\hat{\\mu}_{MLE}$","$\\hat{\\beta}_{MLE}$","$\\hat{\\sigma}_{MLE}$"),
             escape=F, caption="Maximum Likelihood estimates of the state-dependent
             parameters \\label{tab:HMM_MLE}") %>%
  column_spec(1:4, width="5em") %>%
  collapse_rows(columns=1, latex_hline="none") %>%
  footnote(general="Standard errors in parentheses",
           general_title="Note:",
           footnote_as_chunk=T, escape=F) %>%
  kable_styling(latex_options="hold_position")
```

Table \ref{tab:HMM_MLE} reports, however, unexpected results. Indeed, the slope estimate in the first state, $\hat{\beta_1}_{MLE}$, is only slightly negative and very close to zero, whereas the estimate for the second state, $\hat{\beta_2}_{MLE}$ is null as we expected. Overall, the modell suggest that the trend in the two states is not different (i.e. it is equal to zero).
The change point, instead, significantly affects both the level and the variance. In fact, what seemed to be a change in the trend is, instead, a drop in the level, i.e. $\hat{\mu_1}_{MLE}>\hat{\mu_2}_{MLE}$, followed by a sharp decline in variance,i.e. $\hat{\sigma_1}_{MLE}>\hat{\sigma_2}_{MLE}$.

```{r}
# Decoding
# Get the estimated state for each timestep 
estStates <- posterior(HMM_fit)
```

The series with HMM estimated state-dependent means and state-dependent standard deviations are shown in Figure \ref{fig:HMM_results} (left panel) along with the Viterbi states obtained through \textit{global decoding} and with the posterior probabilities (right panel).

```{r, include=FALSE}
HMM_summary <- depmixS4::summary(HMM_fit)
```

```{r, fig.width=20, fig.height=6, out.width='90%', fig.cap="\\label{fig:HMM_results}State-dependent means and standard deviations"}

# Chart with state-dependent mean and standard deviations
HMM_main_CO <- data.frame(state=1:nstates,
                      mu=HMM_summary[,1],
                      sigma=HMM_summary[,3])
df_to_plot <- estStates %>%
  left_join(HMM_main_CO)
df_to_plot %<>%
  mutate(xtime=louisiana_df$n_months, yvalue=louisiana_df$m_CO_mean)

HMM_main <- ggplot(df_to_plot, aes(x=xtime, y=yvalue)) +
                geom_line(size=0.7) +
                geom_point(aes(x=xtime, y=mu), col="blue", size=.8) +
                geom_ribbon(aes(ymin=mu-2*sigma, ymax=mu+2*sigma), alpha=.1) +
                theme_classic(base_size=9) +
                theme(plot.title=element_text(hjust=0.5)) +
                labs(y=expression(CO~(ppm)), x="Year") +
                scale_x_continuous(breaks=seq(0,200,25),labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
                scale_y_continuous(breaks=seq(0,1.5,0.25)) +
                theme(plot.title=element_text(size=10, face="bold",
                                              margin=margin(0,0,10,0)),
                axis.text=element_text(size=19,face="bold"),
                axis.title.x=element_text(family="Times", margin=margin(t=5)),
                axis.title.y=element_text(size=21,family="Times", margin=margin(r=5)))

# Chart with Viterbi states and posterior probabilities
HMM_additional_CO <- data.frame(time_index=louisiana_df$n_months,
                           state=estStates[1],
                           S1=estStates[2]) %>% 
  gather("variable", "value", -time_index)

my_breaks <- function(x) { if (max(x)>1) seq(0,2,1) else seq(0,1,1) }
lab_names <- c('S1'="Posterior, S=1",'state'="Viterbi states")

HMM_additional <- ggplot(HMM_additional_CO, aes(time_index, value)) +
  geom_line(color="black", size=0.7) +
  facet_wrap(variable~., scales="free", ncol=1,
             strip.position="left",
             labeller = as_labeller(lab_names)) +
  labs(x="Year") +
  scale_y_continuous(breaks=my_breaks) +
  scale_x_continuous(breaks=seq(0,200,25),labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  theme_classic(base_size=9) +
  theme(axis.text=element_text(size=19),
        axis.title.x=element_text(size=21,family="Times", margin=margin(5,0,0,0)),
        axis.title.y=element_blank(),
        strip.text=element_text(size=21,family="Times",face="bold"),
        strip.background=element_rect(colour="black", fill=NA),
        strip.placement="outside")

# Option 1
## ggarrange(HMM_main, HMM_additional, ncol=1, widths = c(6,0.3), heights = c(1,0.8))

# Option 2
ggarrange(HMM_main, HMM_additional, widths = c(2,1))
```

Explanations of these results, which confirm a stable change in the evolution process of CO in Louisiana, are several. First, the decline of carbon particles has been widely attributed to a shift from the use of coal to natural gas in US electricity production which dates back exactly to 2007. This is also supported by a marked decline in US coal mines activity which started in that same year. Secondly, a high and positive correlation between economic growth is well documented, thus highlighting some relationship with the outbreak of the Great Recession. Moreover, 2007 was a crucial year when negotiations on greenhouse gas emission were made at international level (such as at the G8 summit and the Vienn agreement). 

A deeper anlysis on the factors explaining this shift can be found in [@feng2015drivers], in which they analyse the series of $\text{CO}_2$.

\subsection{Linear growth model}

The second univariate model that we consider to examine $\text{CO}$ is a linear growth model \footnote{Even if aware of the results obtained in the previous section, we decided to specify a dlm model considering both a level and a trend, thus using the same first guess as explained in the introduction. This allows a proper comparison of the results and it works also as a double check of the trend absence.}, defined as usual as following: 
\begin{eqnarray*} 
& Y_t = \begin{bmatrix}1 & 0 \end{bmatrix} \theta_t + v_t \quad            & v_t \overset{i.i.d.}\sim N(0, \sigma_{V}^{2}) \\
& \theta_t = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}\theta_{t-1} + w_t \quad   & w_t \overset{i.i.d.}\sim N_{2}(0, W = \begin{bmatrix}\sigma_{w_{1}}^{2} & 0 \\ 0 &  \sigma_{w_{1}}^{2} \end{bmatrix} )
\end{eqnarray*}
where $\theta_t = \begin{pmatrix} \mu_{t} \\ \beta_{t} \end{pmatrix}$, $\theta_{0} \sim N_{2}(m_{0} = \begin{pmatrix} \hat{\mu_{0}} \\ \hat{\beta_{0}} \end{pmatrix}, C_{0})$ and $\theta_{0} \indep (v_{t}) \indep (w_{t})$. 

```{r echo=FALSE}
# Build DLM with polynomial specification
build_LG <- function(param){dlmModPoly(order=2, dV=exp(param[1]), dW=exp(param[2:3]),m0=c(y[1],0))}

# Estimate MLE of parameters
fit_LG <- dlmMLE(y, rep(1,3), build_LG, hessian=TRUE)
unlist(build_LG(fit_LG$par)[c("V","W")]) # convergence achieved

# Calculate standard errors of the MLE using delta method
estVarLog_LG <- solve(fit_LG$hessian)
estVar_LG <- diag(exp(fit_LG$par)) %*% estVarLog_LG %*% + diag(exp(fit_LG$par))
SE_LG <- sqrt(diag(estVar_LG))
```

```{r echo=FALSE}
## Model 2
# Alternative (1): assume constant nominal speed in the dynamics
buildMod_2 <- function(param){dlmModPoly(order=2, dV=exp(param[1]), dW=c(exp(param[2]), 0),m0=c(y[1],0))}
fit_2 <- dlmMLE(y, rep(1,2), buildMod_2, hessian=TRUE) # convergence achieved
unlist(buildMod_2(fit_2$par)[c("V","W")])
# Calculate standard errors of the MLE using delta method
estVarLog <- solve(fit_2$hessian)
estVar <- diag(exp(fit_2$par)) %*% estVarLog %*% + diag(exp(fit_2$par))
Mod2_st.err <- sqrt(diag(estVar))
```

```{r echo=FALSE}
## Model 3
# Alternative (2): integrated random walk model
buildMod_3 <- function(param){dlmModPoly(order=2, dV=exp(param[1]), dW=c(0,exp(param[2])), m0=c(y[1],0))}
fit_3 <- dlmMLE(y, parm=rep(1,2), buildMod_2, hessian=TRUE) # convergence achieved
unlist(buildMod_3(fit_2$par)[c("V","W")])
# Calculate standard errors of the MLE using delta method
estVarLog <- solve(fit_3$hessian)
estVar <- diag(exp(fit_3$par)) %*% estVarLog %*% + diag(exp(fit_3$par))
Mod3_st.err <- sqrt(diag(estVar))
```

```{r}
# Put MLE in the model
LG <- build_LG(fit_LG$par)
# Kalman filter
LG_Filt <- dlmFilter(y, LG)
# Remove the first item (m_0) in the vector of filtered states
filt_est = dropFirst(LG_Filt$m)
# Compute the vector of state variances and standard deviations
list_c <- dlmSvd2var(LG_Filt$U.C, LG_Filt$D.C)
vol_C <- sqrt(unlist(list_c))
# One step ahead forecast
y_filt <- LG_Filt$f
```

\textcolor{red}{DATE SU ASSE X DIVERSE DA GRAFICO PRECEDENTE}

```{r, fig.width=7, fig.height=2, fig.cap="\\label{fig:filter_est} Filtering states estimates"}

n_dates <- seq(as.Date("2001/1/1"), as.Date("2016/6/1"), "month")

# Df
df_LG_filt <- data.frame(n_dates, y, y_filt)

# Plot
colors <- c("Observed data" = "black", "Filtering estimates" = "red")
ggplot(df_LG_filt, aes(x=n_dates)) +
  geom_line(aes(y=y, color="Observed data"), size=0.4) +
  geom_line(aes(y=y_filt, color = "Filtering estimates"), size=0.4) +
  geom_vline(xintercept = as.Date("2008/1/1"), color= "black", size=0.3) +
  theme_classic(base_size=9.5) +
  theme(panel.grid.minor = element_line(size=0.5)) +
  labs(x="Year", y=expression(CO~(ppm)), color = "") +
  scale_y_continuous(breaks=seq(0,1.5,0.25)) +
  scale_x_date(date_breaks="2 years", limits=as.Date(c('2001/01/01','2016/06/01')),
               labels=date_format("%Y")) +
  theme(axis.title.x=element_text(family="Times", margin=margin(t=5)),
        axis.title.y=element_text(family="Times", margin=margin(r=5))) +
        scale_color_manual(values = colors) +
        theme(legend.justification = c("left", "top"),
              legend.box.just = "left",legend.margin = margin(2, 2, 2, 2))
```

textcolor{red}{NEL CASO NON CI FOSSE ABBASTANZA SPAZIO POTREMMO TOGLIERE IL PLOT DEI RESIDUALS CHE DEI 3 MI SEMBRA IL MENO INFORMATIVO. (LUCA: in realta il primo grafico del linear che hai plottato è molto interessante perchè si vede chiaramente che la varianza degli errori non è time invariant ma ha un bel change dal 2008. possiamo dirlo e provare a fare le prossime specificazioni con la varianza che cambia dal 2008, probabilmente questo change è catturato meglio dagli errori dell'HMM e qui si può commentare la differenza tra i due. domani provo a capire come specificare un dlm con time variant variance qui su R.")}

```{r, fig.width=20, fig.height=12, fig.cap="\\label{fig:diagnostic} (a) Standardized one-step-ahead forecast errors; (b) ACF of one-step-ahead forecast errors; (c) Normal probability plot of standardized one-step-ahead forecast errors"}

# Model checking
res_LG <- residuals(LG_Filt, sd=FALSE)
res_LG = data.frame(res_LG, n_dates)
colnames(res_LG) = c("res", "time")

a <- ggplot(res_LG, aes(x=time)) +
  geom_line(aes(y=res), size=0.4) +
  theme_classic(base_size=9.5) +
  theme(panel.grid.minor = element_line(size=0.5)) +
  labs(x="Year", y="Residuals") +
  scale_x_date(date_breaks="2 years", limits=as.Date(c('2001/01/01','2017/06/01')),
               labels=date_format("%Y")) +
  scale_y_continuous(breaks=seq(-4.5,4.5,1), limits = c(-3,4.5)) +
  theme(axis.text=element_text(size=19),
        axis.title.x=element_text(size=21, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21, family="Times", margin=margin(r=5)))

ggplot.corr <- function(data, lag.max = 24, ci = 0.95, large.sample.size = TRUE, horizontal = TRUE,...)
{
  require(ggplot2)
  require(dplyr)
  require(cowplot)
  
  if(horizontal == TRUE) {numofrow <- 1} else {numofrow <- 2}
  
  list.acf <- acf(data, lag.max = lag.max, type = "correlation", plot = FALSE)
  N <- as.numeric(list.acf$n.used)
  df1 <- data.frame(lag = list.acf$lag, acf = list.acf$acf)
  df1$lag.acf <- dplyr::lag(df1$acf, default = 0)
  df1$lag.acf[2] <- 0
  df1$lag.acf.cumsum <- cumsum((df1$lag.acf)^2)
  df1$acfstd <- sqrt(1/N * (1 + 2 * df1$lag.acf.cumsum))
  df1$acfstd[1] <- 0
  df1 <- select(df1, lag, acf, acfstd)
  
  list.pacf <- acf(data, lag.max = lag.max, type = "partial", plot = FALSE)
  df2 <- data.frame(lag = list.pacf$lag,pacf = list.pacf$acf)
  df2$pacfstd <- sqrt(1/N)
  
  if(large.sample.size == TRUE)
  {
    plot.acf <- ggplot(data = df1, aes( x = lag, y = acf)) +
      geom_area(aes(x = lag, y = qnorm((1+ci)/2)*acfstd), fill = "#B9CFE7") +
      geom_area(aes(x = lag, y = -qnorm((1+ci)/2)*acfstd), fill = "#B9CFE7") +
      geom_col(fill = "#4373B6", width = 0.7) +
      theme_classic(base_size=9.5) +
      scale_x_continuous(breaks = seq(0,max(df1$lag),6)) +
      scale_y_continuous(name = element_blank(), 
                         limits = c(min(df1$acf,df2$pacf),1)) +
      theme(axis.text=element_text(size=19),
        axis.title.x=element_text(size=21,family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21,family="Times", margin=margin(r=5)),
        panel.grid.minor = element_line(size=0.5))
    
    plot.pacf <- ggplot(data = df2, aes(x = lag, y = pacf)) +
      geom_area(aes(x = lag, y = qnorm((1+ci)/2)*pacfstd), fill = "#B9CFE7") +
      geom_area(aes(x = lag, y = -qnorm((1+ci)/2)*pacfstd), fill = "#B9CFE7") +
      theme_classic(base_size=9.5)+
      geom_col(fill = "#4373B6", width = 0.7) +
      scale_x_continuous(breaks = seq(0,max(df2$lag, na.rm = TRUE),6)) +
      scale_y_continuous(name = element_blank(),
                         limits = c(min(df1$acf,df2$pacf),1)) +
      ggtitle("PACF") +
      theme(axis.text=element_text(size=19),
        axis.title.x=element_text(size=21,family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21,family="Times", margin=margin(r=5)),
        panel.grid.minor = element_line(size=0.5))
  }
  else
  {
    plot.acf <- ggplot(data = df1, aes( x = lag, y = acf)) +
      geom_col(fill = "#4373B6", width = 0.7) +
      geom_hline(yintercept = qnorm((1+ci)/2)/sqrt(N), 
                 colour = "sandybrown",
                 linetype = "dashed") + 
      geom_hline(yintercept = - qnorm((1+ci)/2)/sqrt(N), 
                 colour = "sandybrown",
                 linetype = "dashed") + 
      theme_classic(base_size=9.5)+
      scale_x_continuous(breaks = seq(0,max(df1$lag),6)) +
      scale_y_continuous(name = element_blank(), 
                         limits = c(min(df1$acf,df2$pacf),1)) +
       theme(axis.text=element_text(size=19),
        axis.title.x=element_text(size=21,family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21,family="Times", margin=margin(r=5)),
        panel.grid.minor = element_line(size=0.5))
    
    plot.pacf <- ggplot(data = df2, aes(x = lag, y = pacf)) +
      geom_col(fill = "#4373B6", width = 0.7) +
      geom_hline(yintercept = qnorm((1+ci)/2)/sqrt(N), 
                 colour = "sandybrown",
                 linetype = "dashed") + 
      geom_hline(yintercept = - qnorm((1+ci)/2)/sqrt(N), 
                 colour = "sandybrown",
                 linetype = "dashed") + 
      theme_classic(base_size=9.5)+
      scale_x_continuous(breaks = seq(0,max(df2$lag, na.rm=TRUE),6)) +
      scale_y_continuous(name = element_blank(),
                         limits = c(min(df1$acf,df2$pacf),1)) +
      ggtitle("PACF") +
       theme(axis.text=element_text(size=19),
        axis.title.x=element_text(size=21,family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21,family="Times", margin=margin(r=5)),
        panel.grid.minor=element_line(size=0.5))
  }
  cowplot::plot_grid(plot.acf)
}

b <- ggplot.corr(data=res_LG$res, lag.max=24, ci=0.95, large.sample.size=FALSE, horizontal=TRUE)

c <- ggplot(res_LG, aes(sample=res)) +
  stat_qq(col="blue") +
  stat_qq_line(col="red", lty=2, size=1) +
  scale_y_continuous(breaks=seq(-3, 3,1), limits=c(-3,3)) +
  scale_x_continuous(breaks=seq(-3, 3,1), limits=c(-3,3)) +
  theme_classic(base_size=9.5) +
  theme(panel.grid.minor=element_line(size=0.5)) +
  labs(x="Theoretical", y="Observed") +
  theme(axis.text=element_text(size=19),
        axis.title.x=element_text(size=21,family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21,family="Times", margin=margin(r=5)))

res_series <- plot_grid(a, ggarrange(b, c, ncol = 2, labels = c("(b)", "(c)")),
                        nrow = 2, labels = "(a)")
res_series
```

```{r, include=F}
# Measures of forecasting accuracy
MSE_LG <- round(mean((LG_Filt$f-y)^2),3) # mean square error (MSE)
MAE_LG <- round(mean(abs(LG_Filt$f-y)),3) # mean abs. error (MAE)
MAPE_LG <- round(mean(abs(LG_Filt$f-y)/y),3) # mean abs. percentage error (MAPE)
```

\subsection{Dynamic linear regression}

We take as explanatory variable $x_t$ Louisiana's coal consumption for electricity generation (in particular, for the totality of the electric power industry).

\textcolor{red}{DLM for Univariate regression potrebbe essere un'idea. per esempio come rgressore si potrebbe usare la quantità di carbone estratto negli USA, la quantità di gas naturale usata nella produzione di corrente. se trovare i dati fosse difficile potremmo semplicemente modellare il trend}

```{r}
# Import dataset
coal_cons <- read.csv(url("https://raw.githubusercontent.com/SimoneArrigoni/US_pollution/master/Louisiana_coal_consumption.csv"), sep=";", header=T, stringsAsFactors=F)
x <- as.numeric(coal_cons$tot_electric_power_industry)

# Build dynamic linear regression model
# initialize the algorithm using the initial level of y and the sample variance before the break
build_DR <- function(param){dlmModReg(x, addInt=TRUE, dV=param[1], dW=param[2:3], m0=c(y[1],0), C0=diag(rep(var(x[1:85])),2))}

# Estimate MLE of parameters
fit_DR <- dlmMLE(y, parm=rep(1,3), build_DR, lower=c(0.000001, 0, 0),hessian=TRUE)  # Convergence achieved
unlist(build_DR(fit_DR$par)[c("V","W")])

# Calculate standard errors based on numeriacally evaluated hessian matrix
estVar_DR <- solve(fit_DR$hessian)
SE_DR <- sqrt(diag(estVar_DR))

# Put MLE in the model
DR <- build_DR(fit_DR$par)

# Kalman filter
DR_Filt <- dlmFilter(y, DR)

# Remove the first item (m_0) in the vector of filtered states
filt_est = dropFirst(DR_Filt$m)

# Compute the vector of state variances and standard deviations
list_c <- dlmSvd2var(DR_Filt$U.C, DR_Filt$D.C)
vol_C <- sqrt(unlist(list_c))

# One step ahead forecast
y_filt_DR <- DR_Filt$f
```

```{r, fig.width=20, fig.height=12, fig.cap="\\label{fig:diagnostic} (a) Standardized one-step-ahead forecast errors; (b) ACF of one-step-ahead forecast errors; (c) Normal probability plot of standardized one-step-ahead forecast errors"}

# Model checking
res_DR <- residuals(DR_Filt, sd=FALSE)
res_DR = data.frame(res_DR, n_dates)
colnames(res_DR) = c("res", "time")

a <- ggplot(res_DR, aes(x=time)) +
  geom_line(aes(y=res), size=0.4) +
  theme_classic(base_size=9.5) +
  theme(panel.grid.minor = element_line(size=0.5)) +
  labs(x="Year", y="Residuals") +
  scale_x_date(date_breaks="4 years", limits=as.Date(c('2000/01/01','2017/01/01')),
               labels=date_format("%Y")) +
  scale_y_continuous(breaks=seq(-4.5,4.5,1), limits = c(-3,4.5)) +
  theme(axis.text=element_text(size=19),
        axis.title.x=element_text(size=21,family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21,family="Times", margin=margin(r=5)))

ggplot.corr <- function(data, lag.max = 24, ci = 0.95, large.sample.size = TRUE, horizontal = TRUE,...)
{
  require(ggplot2)
  require(dplyr)
  require(cowplot)
  
  if(horizontal == TRUE) {numofrow <- 1} else {numofrow <- 2}
  
  list.acf <- acf(data, lag.max = lag.max, type = "correlation", plot = FALSE)
  N <- as.numeric(list.acf$n.used)
  df1 <- data.frame(lag = list.acf$lag, acf = list.acf$acf)
  df1$lag.acf <- dplyr::lag(df1$acf, default = 0)
  df1$lag.acf[2] <- 0
  df1$lag.acf.cumsum <- cumsum((df1$lag.acf)^2)
  df1$acfstd <- sqrt(1/N * (1 + 2 * df1$lag.acf.cumsum))
  df1$acfstd[1] <- 0
  df1 <- select(df1, lag, acf, acfstd)
  
  list.pacf <- acf(data, lag.max = lag.max, type = "partial", plot = FALSE)
  df2 <- data.frame(lag = list.pacf$lag,pacf = list.pacf$acf)
  df2$pacfstd <- sqrt(1/N)
  
  if(large.sample.size == TRUE)
  {
    plot.acf <- ggplot(data = df1, aes( x = lag, y = acf)) +
      geom_area(aes(x = lag, y = qnorm((1+ci)/2)*acfstd), fill = "#B9CFE7") +
      geom_area(aes(x = lag, y = -qnorm((1+ci)/2)*acfstd), fill = "#B9CFE7") +
      geom_col(fill = "#4373B6", width = 0.7) +
      theme_classic(base_size=9.5) +
      scale_x_continuous(breaks = seq(0,max(df1$lag),6)) +
      scale_y_continuous(name = element_blank(), 
                         limits = c(min(df1$acf,df2$pacf),1)) +
      theme(axis.text=element_text(size=19),
        axis.title.x=element_text(size=21,family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21,family="Times", margin=margin(r=5)),
        panel.grid.minor = element_line(size=0.5))
    
    plot.pacf <- ggplot(data = df2, aes(x = lag, y = pacf)) +
      geom_area(aes(x = lag, y = qnorm((1+ci)/2)*pacfstd), fill = "#B9CFE7") +
      geom_area(aes(x = lag, y = -qnorm((1+ci)/2)*pacfstd), fill = "#B9CFE7") +
      theme_classic(base_size=9.5)+
      geom_col(fill = "#4373B6", width = 0.7) +
      scale_x_continuous(breaks = seq(0,max(df2$lag, na.rm = TRUE),6)) +
      scale_y_continuous(name = element_blank(),
                         limits = c(min(df1$acf,df2$pacf),1)) +
      ggtitle("PACF") +
      theme(axis.text=element_text(size=19),
        axis.title.x=element_text(size=21,family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21,family="Times", margin=margin(r=5)),
        panel.grid.minor = element_line(size=0.5))
  }
  else
  {
    plot.acf <- ggplot(data = df1, aes( x = lag, y = acf)) +
      geom_col(fill = "#4373B6", width = 0.7) +
      geom_hline(yintercept = qnorm((1+ci)/2)/sqrt(N), 
                 colour = "sandybrown",
                 linetype = "dashed") + 
      geom_hline(yintercept = - qnorm((1+ci)/2)/sqrt(N), 
                 colour = "sandybrown",
                 linetype = "dashed") + 
      theme_classic(base_size=9.5)+
      scale_x_continuous(breaks = seq(0,max(df1$lag),6)) +
      scale_y_continuous(name = element_blank(), 
                         limits = c(min(df1$acf,df2$pacf),1)) +
       theme(axis.text=element_text(size=19),
        axis.title.x=element_text(size=21,family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21,family="Times", margin=margin(r=5)),
        panel.grid.minor = element_line(size=0.5))
    
    plot.pacf <- ggplot(data = df2, aes(x = lag, y = pacf)) +
      geom_col(fill = "#4373B6", width = 0.7) +
      geom_hline(yintercept = qnorm((1+ci)/2)/sqrt(N), 
                 colour = "sandybrown",
                 linetype = "dashed") + 
      geom_hline(yintercept = - qnorm((1+ci)/2)/sqrt(N), 
                 colour = "sandybrown",
                 linetype = "dashed") + 
      theme_classic(base_size=9.5)+
      scale_x_continuous(breaks = seq(0,max(df2$lag, na.rm=TRUE),6)) +
      scale_y_continuous(name = element_blank(),
                         limits = c(min(df1$acf,df2$pacf),1)) +
      ggtitle("PACF") +
       theme(axis.text=element_text(size=19),
        axis.title.x=element_text(size=21,family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21,family="Times", margin=margin(r=5)),
        panel.grid.minor=element_line(size=0.5))
  }
  cowplot::plot_grid(plot.acf)
}

b <- ggplot.corr(data=res_DR$res, lag.max=24, ci=0.95, large.sample.size=FALSE, horizontal=TRUE)

c <- ggplot(res_DR, aes(sample=res)) +
  stat_qq(col="blue") +
  stat_qq_line(col="red", lty=2, size=1) +
  scale_y_continuous(breaks=seq(-3, 3,1), limits=c(-3,3)) +
  scale_x_continuous(breaks=seq(-3, 3,1), limits=c(-3,3)) +
  theme_classic(base_size=9.5) +
  theme(panel.grid.minor=element_line(size=0.5)) +
  labs(x="Theoretical", y="Observed") +
  theme(axis.text=element_text(size=19),
        axis.title.x=element_text(size=21,family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21,family="Times", margin=margin(r=5)))

res_series <- plot_grid(a, ggarrange(b, c, ncol = 2, labels = c("(b)", "(c)")),
                        nrow = 2, labels = "(a)")
res_series
```

```{r, include=F}
# Measures of forecasting accuracy
MSE_DR <- round(mean((DR_Filt$f-y)^2),3) # mean square error (MSE)
MAE_DR <- round(mean(abs(DR_Filt$f-y)),3) # mean abs. error (MAE)
MAPE_DR <- round(mean(abs(DR_Filt$f-y)/y),3) # mean abs. percentage error (MAPE)
```

\section{Part 2. Multivariate analysis}

\subsection{Seemingly unrelated regression (SUR)}

\textcolor{red}{Le serie SO2 e CO mostrano entrambe un breakpoint al month 100 e sono positivamente correlate. Inoltre entrambi i pollutants dipendono dalla combustione del carbone che potremmo usare come regressore. In questo caso avremmo un multivariate DLM con m=2}

```{r}
# Build SUR model
buildSUR <- function(param){
  Bmod=dlmModReg(x, addInt=TRUE)
  Bmod$FF=matrix(c(1,1), nrow=1, ncol=2) %x% diag(2)
  Bmod$JFF=matrix(c(0,0,0,0,1,0,0,2), nrow=2, ncol=4)
  Bmod$GG=diag(4)
  Bmod$V=diag(c(param[1], param[2]))
  Wa=matrix(c(param[3],param[4],param[5],param[6]),2,2)
  Wb=matrix(c(param[7], param[8], param[9], param[10]),2,2)
  W=bdiag(Wa, Wb) 
  Bmod$W=W  
  
  Bmod$m0=rep(0, 2*2)
  Bmod$C0=diag(1e7, nr = 2*2)
  return(Bmod)
}

# Estimate MLE of parameters
fit_SUR <- dlmMLE(Ym, parm=rep(1,6), buildSUR, hessian=TRUE)
unlist(build_SUR(fit_SUR$par)[c("V","W")])

# Calculate standard errors of the MLE using delta method
estVarLog_SUR <- solve(fit_SUR$hessian)
estVar_SUR <- diag(exp(fit_SUR$par)) %*% estVarLog_SUR %*% + diag(exp(fit_SUR$par))
SE_DR <- sqrt(diag(estVar_SUR))

# Put MLE in the model
SUR <- build_SUR(fit_SUR$par)

# Kalman filter
SUR_Filt <- dlmFilter(Ym, SUR)
```

```{r, include=F}
# Measures of forecasting accuracy
MSE_SUR <- round(mean((SUR_Filt$f-y)^2),3) # mean square error (MSE)
MAE_SUR <- round(mean(abs(SUR_Filt$f-y)),3) # mean abs. error (MAE)
MAPE_SUR <- round(mean(abs(SUR_Filt$f-y)/y),3) # mean abs. percentage error (MAPE)
```

\subsection{Seemingly unrelated time series equations (SUTSE)}

```{r}

```

\section{Conclusion}

```{r, include=T}
# Forecasting accuracy: Summary table
tabf <- matrix(nrow=3, ncol=5)
rownames(tabf) <- c('MSE','MAE','MAPE')
colnames(tabf) <- c('HMM', 'Linear growth model', 'Dynamic regression', 'SUR', 'SUTSE')
tabf[1,1] <- MSE_HMM
tabf[2,1] <- MAE_HMM
tabf[3,1] <- MAPE_HMM
tabf[1,2] <- MSE_LG
tabf[2,2] <- MAE_LG
tabf[3,2] <- MAPE_LG
tabf[1,3] <- MSE_DR
tabf[2,3] <- MAE_DR
tabf[3,3] <- MAPE_DR
tabf[1,4] <- MSE_SUR
tabf[2,4] <- MAE_SUR
tabf[3,4] <- MAPE_SUR
tabf[1,5] <- MSE_SUTSE
tabf[2,5] <- MAE_SUTSE
tabf[3,5] <- MAPE_SUTSE
kable(tabf, caption="Model checking")
```

\newpage
\section{References}