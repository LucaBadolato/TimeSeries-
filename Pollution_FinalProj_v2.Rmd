---
title: "20236 Time Series Analysis - Final project"
author:
- Simone Arrigoni (1794692)
- Luca Badolato (3086040)
- Simone Valle (3088281)
subtitle: "Bocconi University"
date: "5 June 2020"
output: pdf_document
geometry: margin=2cm
nocite: '@*'
bibliography: References.bib
header-includes:
  \usepackage[utf8]{inputenc}
  \usepackage{setspace}
  \usepackage{apacite}
  \usepackage{natbib}
  \usepackage{algpseudocode}
  \usepackage{algorithm}
  \usepackage{bm}
  \usepackage{amsmath}
  \usepackage{amssymb}
  \usepackage{graphicx}
  \usepackage{subfig}
  \usepackage{booktabs, caption}
  \usepackage{array}
  \usepackage{threeparttable}
  \usepackage{listings}
  \usepackage{physics}
  \usepackage{float}
  \floatplacement{figure}{H}
  \usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
  \definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
  \definecolor{mylilas}{RGB}{170,55,241}
  \definecolor{codegray}{RGB}{0.5,0.5,0.5}
  \definecolor{orange}{RGB}{255,165,0}
  \DeclareMathOperator*{\E}{\mathbb{E}}
  \DeclareMathOperator*{\Ec}{\mathbb{E}_t}
  \setlength\parindent{0pt}
  \newcommand{\indep}{\perp \!\!\! \perp}
---

```{r, include=F}
# Load useful packages
library(utf8)
library(labeling)
library(rmarkdown)
library(readxl)
library(httr)
library(knitr)
library(tseries)
library(tinytex)
library(scales)
library(dlm)
library(magrittr)
library(stringr)
library(depmixS4)
library(tidyverse)
library(ggthemes)
library(latex2exp)
library(kableExtra)
library(ggpubr)
library(reshape2)
library(dlm)

# Settings
knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE,
                      echo    = FALSE,
                      include = TRUE,
                      fig.pos = "H",
                      fig.align = "center",
                      out.width='80%')
```

\section{Introduction}

Pollution is one of the most debated social issues of our times. It has been attracting more and more attention given its serious implications on the health of both the Earth and human beings. The economic activity under the model of production and consumption that has been characterizing the last century is, with no doubt, at the very basis of such complex phenomenon. Only in the last fifteen years some consciousness about the importance of pollution in the long-run has evolved and serious interventions have been made. Moreover, among the numerous component which pollution can be split into, air pollution is certainly one of the most important and tightly linked to strict economic activity.

Therefore, the purpose of the present work is to model through common statistical techniques the evolution of air pollution during this last crucial fifteen years in a developed economy, i.e. the United States, trying to grasp some intuition about this fundamental phenomenon. Hence, after some preliminary descriptive steps, we present the models we think are more appropriate to analyze such complex phenomenon. Finally, we conclude with a comparison to underline pros and cons of each specification. 

\section{Data}

The dataset \texttt{US\_pollution} contains monthly and daily data about air pollution for each state in the United States. It is provided by the U.S. Environmental Protection Agency (EPA). Specifically, four key indicators of air pollution are considered: 

\begin{itemize}
\item \textbf{Ground level ozone $\text{O}_{3}$} (\texttt{O3}): Bad ozone forms near the ground when pollutants (emitted by sources such as cars, power plants, industrial boilers, refineries, and chemical plants) react chemically in sunlight. Since it is more likely to form during warmer months, we expect to see a seasonal component. 
\item \textbf{Nitrogen dioxide $\text{NO}_{2}$} (\texttt{NO2}): It is produced as a result of road traffic and other fossil fuel combustion processes. Furthermore, its presence in air contributes to the formation and modification of other air pollutants, such as particle pollution.
\item \textbf{Carbon monoxide $\text{CO}$} (\texttt{CO}):  It forms from incomplete combustions mainly from vehicle exhausts (roughly 75\% of all carbon monoxide emissions nationwide and up to 95\% in cities), fuel combustion in industrial processes and natural sources. Cold temperatures make combustion less complete, therefore we expect also in this case a seasonal behavior. 
\item \textbf{Sulfur dioxide $\text{SO}_{2}$} (\texttt{SO2}): It is produced when sulfur-containing fuels are burned. It is common mainly in large industrial complexes. 
\end{itemize}
\medskip

Very intuitively, the four variables of interest are interconnected and are the building blocks of a more general phenomenon, that is air pollution. Furthermore, they are related by a common latent process that can be summarized by several components such as industrialization, car traffic and regulation.

The preliminary analysis that led us from the raw data (with frequency of 4-intra-day observations) to the current dataset is made available in the GitHub repository at https://github.com/SimoneArrigoni/US_pollution. We relied on a moving average algorithm of order $120$ to smooth the data, assuming months of 30 days and a year of 360 days, with a minor approximation relative to the original data structure based on a actual 365 day-counts.

```{r echo=F}
# Import the dataset
louisiana_df <- read.csv(url("https://raw.githubusercontent.com/SimoneArrigoni/US_pollution/master/Louisiana_pollution.csv"), sep = ";", header=T)
```

We will be focusing on measures for \textbf{Louisiana}, recorded monthly from January 2001 to June 2016, for a total of `r length(louisiana_df$n_months)` observations. The main advantage of studying the evolution of air pollution in the same country over the years is that this allows us to control for fixed effects and to better study the latent process of interest that relates the four pollution components.
\bigskip

We report in Table \ref{tab:louisiana_corr} pollutants'correlation coefficients.

```{r}
# Correlation plot
datacor = subset(louisiana_df, select=c(m_O3_mean, m_NO2_mean, m_CO_mean, m_SO2_mean))
cor.datacor = cor(datacor, use="complete.obs")
cormat = round(cor(datacor),2)
row.names(cormat) <- c("$\\text{O}_{3}$",
                       "$\\text{NO}_{2}$",
                       "$\\text{CO}$",
                       "$\\text{SO}_{2}$")

knitr::kable(cormat, "latex", booktabs=T, align="c", escape=F,
             caption="Correlation table \\label{tab:louisiana_corr}",
             col.names=c("$\\text{O}_{3}$",
                         "$\\text{NO}_{2}$",
                         "$\\text{CO}$",
                         "$\\text{SO}_{2}$")) %>%
  column_spec(1:4, width="5em") %>%
kable_styling(latex_options="hold_position")
```

Provided that the correlation between pollutants shows significant differences in regional patterns and that $\text{CO}$ is a good indicator for industrial and biomass burning pollution [@Logan1981], a positive and strong correlation with $\text{O}_{3}$ indicates that a region has experienced photochemical $\text{O}_{3}$ production from its precursors (including $\text{CO}$) and is aligned with previous studies in the field [@Voulgarakis2011]. $\text{CO}$ is also positively correlated with $\text{SO}_2$ which is justified by the fact that they share a main source of production, i.e. coal extraction and combustion.

Other studies report that, in the US, $\text{O}_3$ is on average positively correlated with $\text{NO}_2$, which is opposite to our empirical findings about Louisiana, where they are slightly negatively correlated. A possible explanation can be given by substitution effects: an increase of nitrogen dioxide levels can be associated to the use of more fossil fuels that produce $\text{NO}_2$ instead of others. This might be related to Louisiana's fixed effects, however further research would be beyond the purpose of our study.

We plot in Figure \ref{fig:louisiana_plots} the time series for the four pollutants under study to give a first qualitative description of the phenomenon.

```{r, fig.width=20, fig.height=6, out.width='100%', fig.cap="\\label{fig:louisiana_plots}Air pollution in the state of Louisiana"}

# Time series plots
O3 <- ggplot(louisiana_df, aes(x=n_months, y=m_O3_mean)) +
  geom_line(color="steelblue", size=0.7) +
  theme_bw(base_size=9) +
  labs(y=expression(O[3]~(ppm)), x="Year") +
  scale_x_continuous(breaks=seq(0,200,25),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(size=19, family="Times", margin=margin(t=8)),
        axis.text.y=element_text(size=19, family="Times", margin=margin(r=8)),
        axis.title.x=element_text(size=21, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21, family="Times", margin=margin(r=5)))

NO2 <- ggplot(louisiana_df, aes(x=n_months, y=m_NO2_mean)) +
  geom_line(color="steelblue", size=0.7) +
  theme_bw(base_size=9) +
  labs(y=expression(NO[2]~(ppb)), x="Year") +
  scale_x_continuous(breaks=seq(0,200,25),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(size=19, family="Times", margin=margin(t=8)),
        axis.text.y=element_text(size=19, family="Times", margin=margin(r=8)),
        axis.title.x=element_text(size=21, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21, family="Times", margin=margin(r=5)))

CO <- ggplot(louisiana_df, aes(x=n_months, y=m_CO_mean)) +
  geom_line(color="steelblue", size=0.7) +
  theme_bw(base_size=9) +
  labs(y=expression(CO~(ppm)), x="Year") +
  scale_x_continuous(breaks=seq(0,200,25),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(size=19, family="Times", margin=margin(t=8)),
        axis.text.y=element_text(size=19, family="Times", margin=margin(r=8)),
        axis.title.x=element_text(size=21, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21, family="Times", margin=margin(r=5)))

SO2 <- ggplot(louisiana_df, aes(x=n_months, y=m_SO2_mean)) +
  geom_line(color="steelblue", size=0.7) +
  theme_bw(base_size=9) +
  labs(y=expression(SO[2]~(ppb)), x="Year") +
  scale_x_continuous(breaks=seq(0,200,25),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(size=19, family="Times", margin=margin(t=8)),
        axis.text.y=element_text(size=19, family="Times", margin=margin(r=8)),
        axis.title.x=element_text(size=21, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21, family="Times", margin=margin(r=5)))

ggarrange(O3, NO2, CO, SO2,
          ncol=2, nrow=2)
```

The time series, each with its specific features, do not in general look stationary. Hence, they cannot be analized with the common models based on the assumption of covariance-stationarity, such as the well-known ARMA models.
\bigskip

\section{$\text{CO}$ series}

$\text{CO}$ is perhaps, among the pollutants, the most tightly linked with the economic and industrial network of a country. Consequently, we select this as the series of main interest for our analysis, being the purpose of this work tracking the evolution of air pollution relative to an underlying economic process.

As an exploratory step, we decompose the time series in its structural components using an additive model \footnote{For sake of brevity we do not report the plots of the decomposed components.}. The decomposition confirms the presence of a quarterly seasonal component and shows a negative trend up to a clear breakpoint between 2007 and 2008. The effect of this break seems to be twofold: first, the trend, that was negative in the pre-break period, disappears; second, the variance reduces markedly in the post-break period.

\section{Section 1. HMM}

These features suggest that the process may be described by a homogeneous Hidden Markov Model with a trend component.

Therefore, we estimate a homogeneous HMM with two states, specified as:
\begin{align*}
\begin{cases}
  Y_{t}=\mu_{1} + \beta_{1}t + \varepsilon_{t}, &
  \varepsilon_{t}\overset{i.i.d.}{\sim}N(0,\sigma_{1}^2) \qquad \text{if} \; S_{t}=1 \\
  Y_{t}=\mu_{2}+ + \beta_{2}t + \varepsilon_{t}, &
  \varepsilon_{t}\overset{i.i.d.}{\sim}N(0,\sigma_{2}^2) \qquad \text{if} \; S_{t}=2 \\
\end{cases}
\end{align*}
where there are three state-dependent parameters, namely the level, $\mu_i$, the slope, $\beta_i$, and the variance, $\sigma_i$, for $i=1,2$. If our guess were correct, we would expect $\mu_1 > \mu_2$, $\beta_1<0$, $\beta_2=0$ and $\sigma_1^2>\sigma_2^2$.

```{r, include=F}
source("HMM_2states.R")
```

However, we noticed that two states are not sufficient to capture the actual behavior of the process\footnote{Again, results are not reported here but can be easily replicated using the provided R code.}. Indeed, a better fit can be obtained using a model with three hidden states that capture periods with negative trend and high volatility, periods with no trend and low volatility and periods with no trend and high volatility. The model is specified as above with an additional equation for $S=3$ and ideally $\mu_1 > \mu_2 > \mu_3$, $\beta_1<0$, $\beta_2=\beta_3=0$ and $\sigma_2^2<\sigma_3^2<\sigma_1^2$.

```{r echo=F, include=F}
# Model specification with 3 states
y <- as.numeric(louisiana_df$m_CO_mean)
nstates <- 3
set.seed(2)
HMM <- depmixS4::depmix(y ~ 1 + louisiana_df$n_months, data=data.frame(y), nstates=nstates)

# Estimation of the unknown parameters
HMM_fit <- depmixS4::fit(HMM)
```

```{r}
# MLE
mu1hat <- paste0(round(HMM_fit@response[[1]][[1]]@parameters[["coefficients"]][["(Intercept)"]],3))
mu2hat <- paste0(round(HMM_fit@response[[2]][[1]]@parameters[["coefficients"]][["(Intercept)"]],3))
mu3hat <- paste0(round(HMM_fit@response[[3]][[1]]@parameters[["coefficients"]][["(Intercept)"]],3))
beta1hat <- paste0(round(HMM_fit@response[[1]][[1]]@parameters[["coefficients"]][["louisiana_df$n_months"]],3))
beta2hat <- paste0(round(HMM_fit@response[[2]][[1]]@parameters[["coefficients"]][["louisiana_df$n_months"]],3))
beta3hat <- paste0(round(HMM_fit@response[[3]][[1]]@parameters[["coefficients"]][["louisiana_df$n_months"]],3))
sd1hat <- paste0(round(HMM_fit@response[[1]][[1]]@parameters$sd,3))
sd2hat <- paste0(round(HMM_fit@response[[2]][[1]]@parameters$sd,3))
sd3hat <- paste0(round(HMM_fit@response[[3]][[1]]@parameters$sd,3))

# se(MLE)
MLE_SE=depmixS4::standardError(HMM_fit)
SE_mu1hat <- paste0('(', round(MLE_SE$se[13],3), ')')
SE_mu2hat <- paste0('(', round(MLE_SE$se[16],3), ')')
SE_mu3hat <- paste0('(', round(MLE_SE$se[19],3), ')')
SE_beta1hat <- paste0('(', round(MLE_SE$se[14],3), ')')
SE_beta2hat <- paste0('(', round(MLE_SE$se[17],3), ')')
SE_beta3hat <- paste0('(', round(MLE_SE$se[20],3), ')')
SE_sd1hat <- paste0('(', round(MLE_SE$se[15],3), ')')
SE_sd2hat <- paste0('(', round(MLE_SE$se[18],3), ')')
SE_sd3hat <- paste0('(', round(MLE_SE$se[21],3), ')')

# Build a summary table for estimates
MLEsum <- data.frame(state=c(rep("S=1",2), rep("S=2",2), rep("S=3",2)),
                     mu=c(mu1hat, SE_mu1hat, mu2hat, SE_mu2hat,mu3hat, SE_mu3hat),
                     beta=c(beta1hat, SE_beta1hat, beta2hat, SE_beta2hat, beta3hat, SE_beta3hat),
                     sd=c(sd1hat, SE_sd1hat, sd2hat, SE_sd2hat, sd3hat, SE_sd3hat))
knitr::kable(MLEsum, "latex", booktabs=T, align="c",
             col.names=c("State",
                         "$\\hat{\\mu}_{MLE}$",
                         "$\\hat{\\beta}_{MLE}$",
                         "$\\hat{\\sigma}_{MLE}$"),
             escape=F, caption="Maximum Likelihood estimates of the state-dependent
             parameters \\label{tab:HMM_MLE}") %>%
  column_spec(1:4, width="5em") %>%
  collapse_rows(columns=1, latex_hline="none") %>%
  footnote(general="Standard errors in parentheses.",
           general_title="Note:",
           footnote_as_chunk=T, escape=F) %>%
  kable_styling(latex_options="hold_position")
```

Table \ref{tab:HMM_MLE} confirms our guess. Indeed, the slope estimate in the first state, $\hat{\beta_1}_{MLE}$, is negative and significant, whereas in the other states is null. Notice that, although apparently low, this coefficient implies a relatively high percentage change after comparing it with our data, which, roughly speaking, range in $(0,1)$. In fact, relating this estimate with a sample mean of $0.39$, we obtain an estimated monthly decline around $-0.015$ corresponding to around $18.3\%$ on an annual basis. Moreover, the model is able to capture shifts in the intecept as well as those in the variance as described before.

The series with HMM estimated state-dependent means and state-dependent standard deviations are shown in Figure \ref{fig:HMM_results} (left panel) along with the Viterbi states obtained through \textit{global decoding} and with the posterior probabilities (right panel).

```{r, include=F}
# Decoding
# Get the estimated state for each timestep 
estStates_HMM <- posterior(HMM_fit)

HMM_summary <- depmixS4::summary(HMM_fit)
```

```{r, fig.width=20, fig.height=6, out.width='100%', fig.cap="\\label{fig:HMM_results}State-dependent means and standard deviations"}

# Chart with state-dependent mean and standard deviations
HMM_main_CO <- data.frame(state=1:nstates,
                      mu=HMM_summary[,1],
                      beta=HMM_summary[,2],
                      sigma=HMM_summary[,3])
df_to_plot <- estStates_HMM %>%
  left_join(HMM_main_CO)
df_to_plot %<>%
  mutate(xtime=louisiana_df$n_months, yvalue=louisiana_df$m_CO_mean)

HMM_main <- ggplot(df_to_plot, aes(x=xtime, y=yvalue)) +
  geom_line(size=0.7) +
  geom_point(aes(x=xtime, y=mu+beta*xtime), col="blue", size=.8) +
  geom_ribbon(aes(ymin=mu+beta*xtime-2*sigma, ymax=mu+beta*xtime+2*sigma), alpha=.1) +
  theme_bw() +
  theme(plot.title=element_text(hjust=0.5)) +
  labs(y=expression(CO~(ppm)), x="Year") +
  scale_x_continuous(breaks=seq(0,200,25),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"),"2 years"), "%Y")) +
  scale_y_continuous(breaks=seq(0,1.5,0.25)) +
   theme(plot.margin=unit(c(0.5,1,0.5,0.5), "cm"),
         plot.title=element_text(size=10, margin=margin(0,0,10,0)),
         axis.text.x=element_text(size=20, family="Times", margin=margin(t=8)),
         axis.text.y=element_text(size=20, family="Times", margin=margin(r=8)),
         axis.title.x=element_text(size=25, family="Times", margin=margin(t=8)),
         axis.title.y=element_text(size=25, family="Times", margin=margin(r=8)))

# Chart with Viterbi states and posterior probabilities
HMM_additional_CO <- data.frame(time_index=louisiana_df$n_months,
                           state=estStates_HMM[1],
                           S1=estStates_HMM[2],
                           S2=estStates_HMM[3]) %>% 
  gather("variable", "value", -time_index)

my_breaks <- function(x) { if (max(x)<4) seq(0,3,1) else seq(0,1,1) }
lab_names <- c('S1'="Post. S=1",'S2'="Post. S=2",'state'="Vit. states")

HMM_additional <- ggplot(HMM_additional_CO, aes(time_index, value)) +
  geom_line(color="black", size=0.7) +
  facet_wrap(variable~., scales="free", ncol=1,
             strip.position="left",
             labeller = as_labeller(lab_names)) +
  labs(x="Year") +
  scale_y_continuous(breaks=my_breaks) +
  scale_x_continuous(breaks=seq(0,200,25),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  theme_bw() +
  theme(panel.spacing=unit(1.5, "lines"),
        axis.text.x=element_text(size=20, family="Times", margin=margin(t=8)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=8, l=8)),
        axis.title.x=element_text(size=25, family="Times", margin=margin(t=8)),
        axis.title.y=element_blank(),
        strip.text=element_text(size=17, family="Times",face="bold"),
        strip.background=element_rect(colour="black", fill=NA),
        strip.placement="outside")

# Option 1
## ggarrange(HMM_main, HMM_additional, ncol=1, widths = c(6,0.3), heights = c(1,0.8))

# Option 2
ggarrange(HMM_main, HMM_additional, widths=c(4,2))
```

Explanations of these results, which confirm a stable change in the evolution process of $\text{CO}$ in Louisiana, are several. First, the decline of carbon particles has been widely attributed to a shift from the use of coal to natural gas in US electricity production which dates back exactly to 2007. This is also supported by a marked decline in US coal mines activity which started in that same year. Second, a high and positive correlation between economic growth is well documented, thus highlighting some relationship with the outbreak of the Great Recession. Moreover, 2007 was a crucial year when negotiations on greenhouse gas emission were made at international level (such as at the G8 summit and the Wien agreement)\footnote{A deeper analysis on the factors explaining this shift can be found in @Feng2015, in which they analyse the series of $\text{CO}_2$.}. 

\textcolor{red}{REMEMBER MODEL CHECKING FOR HMM}

```{r warning=FALSE}
source("HMM_model_checking.R")
```

\section{Section 2. Univariate DLMs}
We now try to model the series of interest through Dynamic Linear Model specifications. Indeed, DLMs are very attractive, since they allow a natural interpretation of the process as the combination of several components, such as trend, seasonal or regressive ones. As a consequence, we consider different specifications which, simply combined, are able to explain all the features previously described\footnote{Therefore we are aware (and we expect) that by construction each model will miss some process characteristics. For example, residuals checking for a linear growth model may shows a seasonal behaviour but this is fully in conformity with the additive structure we consider.}. 

\subsection{2.1. Linear growth model}

The first DLM specification we consider to examine $\text{CO}$ is a time invariant linear growth model, which is suitable to capture both a latent level, $\mu_{t}$, and a time varying splope in its dynamics, $\beta_{t}$. As usual, it is defined as following: 
\begin{align*} 
\begin{cases}
   Y_t = \begin{bmatrix}1 & 0 \end{bmatrix} \theta_t + v_t \quad & v_t \overset{iid}\sim
  \mathcal{N}(0, \sigma_{V}^{2}) 
  \\
  \\
   \theta_t = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}\theta_{t-1} + w_t \quad & w_t
  \overset{iid}\sim \mathcal{N}_{2}(0, W = \begin{bmatrix}\sigma_{w_{1}}^{2} & 0 \\ 0 &  \sigma_{w_{1}}^{2} \end{bmatrix})
  \\
  \end{cases}
\end{align*}
where $\theta_t = \begin{pmatrix} \mu_{t} \\ \beta_{t} \end{pmatrix}$, $\theta_{0} \sim N_{2}(m_{0} = \begin{pmatrix} \hat{\mu_{0}} \\ \hat{\beta_{0}} \end{pmatrix}, C_{0})$ and $\theta_{0} \indep (v_{t}) \indep (w_{t})$. STARTING PRIORS TO BE COMMENTED.

```{r echo=FALSE, warning=FALSE}
#Time invariant linear growth model:
build_LG <- function(param) {dlm( m0=c(y[1],0), C0 = 0.001*diag(2), FF= matrix(c(1,0), nr=1), V = exp(param[1]), GG = matrix(c(1,0,1,1), nr=2), W = diag(c(exp(param[2]), exp(param[3]))) )}

# Estimate MLE of parameters
fit_LG <- dlmMLE(y, rep(0.5,3), build_LG, hessian=TRUE)

# Calculate standard errors of the MLE using delta method
estVarLog_LG <- solve(fit_LG$hessian)
estVar_LG <- diag(exp(fit_LG$par)) %*% estVarLog_LG %*% + diag(exp(fit_LG$par))
SE_LG <- sqrt(diag(estVar_LG))

# Alternative models
source("LG_alternative_models.R")

```

In Table \ref{tab:MLE_lgm} are presented the MLEs of the unknown coefficients and the associated standard errors\footnote{We treat $\psi = (\sigma_{V}^{2}, \sigma_{W_{1}}^{2} \text{and} \ \sigma_{W_{2}}^{2})$ as unknown parameters and we estimate them by maximum likelihood. Then, we perform the analysis of interest plugging them in the model as if they were known costants. We follow this customary way to proceed also in the following sections.}.


```{r}
# MLE
MLE_sigmaV  <- paste0(round(exp(fit_LG$par[1]),5))
MLE_sigmaW1 <- paste0(round(exp(fit_LG$par[2]),9))
MLE_sigmaW2 <- paste0(round(exp(fit_LG$par[3]),9))

# se(MLE)
se_sigmaV  <- paste0('(', round(SE_LG[1],5), ')')
se_sigmaW1 <- paste0('(', round(SE_LG[2],9), ')')
se_sigmaW2 <- paste0('(', round(SE_LG[3],9), ')')

# Summary table
MLEsum <- data.frame(sigmaV=c(MLE_sigmaV, se_sigmaV),
                     sigmaW1=c(MLE_sigmaW1, se_sigmaW1),
                     sigmaW2=c(MLE_sigmaW2, se_sigmaW2))
knitr::kable(MLEsum, "latex", booktabs=T, align="c",
             col.names = c("$\\hat{\\sigma}^2_{v}$",
                           "$\\hat{\\sigma}^2_{w_1}$",
                           "$\\hat{\\sigma}^2_{w_2}$"),
             escape=F, caption="Maximum Likelihood estimates of the unknown
             parameters\\label{tab:MLE_lgm}") %>%
  column_spec(1:3, width="5em") %>%
  collapse_rows(columns=1, latex_hline="none") %>%
  footnote(general="Standard errors in parentheses.",
           general_title="Note:",
           footnote_as_chunk=T, escape=F) %>%
  kable_styling(latex_options="hold_position")
```

```{r}
# Put MLE in the model
LG <- build_LG(fit_LG$par)

# Kalman filter
LG_Filt <- dlmFilter(y, LG)

# Remove the first item (m_0) in the vector of filtered states
filt_est = dropFirst(LG_Filt$m)

# Compute the vector of state variances and standard deviations
list_c <- dlmSvd2var(LG_Filt$U.C, LG_Filt$D.C)
vol_C <- sqrt(unlist(list_c))

# One step ahead forecast
y_filt <- LG_Filt$f

# Signal-to-noise ratio
S_to_N <- matrix(NA,2,1)
S_to_N[1] <- exp(fit_LG$par[2]) * exp(-fit_LG$par[1])
S_to_N[2] <- exp(fit_LG$par[3]) * exp(-fit_LG$par[1])
```

\textcolor{red}{ COMMENTO:SIGNAL-TO-NOISE RATIO MOLTO BASSO DA METTERE IN FONDO...}

As expected, a first graphical inspection of the standardized one-step-ahead forecast error series (reported in Figure) shows both a seasonal behavior and a clear change in variance around 2008. We can take into account this second evidence allowing $\sigma_{V}^{2}$ to be time dependent.

For this purpose, we estimated a linear growth model specified as above but considering  $\sigma_{Vt}^{2} = \sigma_{V1}^{2}$ for observations before 2008 and  $\sigma_{Vt}^{2} = \sigma_{V2}^{2}$ for observations after 2008. The MLE estimators are reported in Table \ref{tab:MLE_lgm_v}.

```{r echo=FALSE, warning=FALSE}
#Time variant linear growth model:
build_LG_v <- function(param) {dlm( m0=c(y[1],0), C0 = 0.001*diag(2), FF= matrix(c(1,0), nr=1), V = 1, GG = matrix(c(1,0,1,1), nr=2), W = diag(c(exp(param[3]), exp(param[4]))), JV = 1, X = rep( c(exp(param[1]), exp(param[2])), c(84, 102)) )}

fit_LG_v <- dlmMLE(y, rep(0.5, 4), build_LG_v, hessian=TRUE)

# Calculate standard errors of the MLE using delta method
estVarLog_LG_v <- solve(fit_LG_v$hessian)
estVar_LG_v <- diag(exp(fit_LG_v$par)) %*% estVarLog_LG_v %*% + diag(exp(fit_LG_v$par))
SE_LG_v <- sqrt(diag(estVar_LG_v))
```

```{r}
# MLE
MLE_sigmaV1_v  <- paste0(round(exp(fit_LG_v$par[1]),5))
MLE_sigmaV2_v  <- paste0(round(exp(fit_LG_v$par[2]),5))
MLE_sigmaW1_v  <- paste0(round(exp(fit_LG_v$par[3]),9))
MLE_sigmaW2_v  <- paste0(round(exp(fit_LG_v$par[4]),12))

# se(MLE)
se_sigmaV1_v  <- paste0('(', round(SE_LG_v[1],5), ')')
se_sigmaV2_v  <- paste0('(', round(SE_LG_v[2],5), ')')
se_sigmaW1_v  <- paste0('(', round(SE_LG_v[3],9), ')')
se_sigmaW2_v  <- paste0('(', round(SE_LG_v[4],9), ')')

# Summary table
MLEsum <- data.frame(sigmaV1=c(MLE_sigmaV1_v, se_sigmaV1_v),
                     sigmaV2=c(MLE_sigmaV2_v, se_sigmaV2_v),
                     sigmaW1=c(MLE_sigmaW1_v, se_sigmaW1_v),
                     sigmaW2=c(MLE_sigmaW2_v, se_sigmaW2_v))

knitr::kable(MLEsum, "latex", booktabs=T, align="c",
             col.names = c("$\\hat{\\sigma}^2_{v1}$",
                           "$\\hat{\\sigma}^2_{v2}$",
                           "$\\hat{\\sigma}^2_{w_1}$",
                           "$\\hat{\\sigma}^2_{w_2}$"),
             escape=F, caption="Maximum Likelihood estimates of the unknown
             parameters\\label{tab:MLE_lgm_v}") %>%
  column_spec(1:4, width="5em") %>%
  collapse_rows(columns=1, latex_hline="none") %>%
  footnote(general="Standard errors in parentheses.",
           general_title="Note:",
           footnote_as_chunk=T, escape=F) %>%
  kable_styling(latex_options="hold_position")
```

```{r}
# Put MLE in the model
LG_v <- build_LG_v(fit_LG_v$par)

# Kalman filter
LG_Filt_v <- dlmFilter(y, LG_v)

# Remove the first item (m_0) in the vector of filtered states
filt_est_v = dropFirst(LG_Filt_v$m)

# Compute the vector of state variances and standard deviations
list_c_v <- dlmSvd2var(LG_Filt_v$U.C, LG_Filt_v$D.C)
vol_C_v <- sqrt(unlist(list_c_v))

# One step ahead forecast
y_filt_v <- LG_Filt_v$f
```

In Figure \ref{Fig:innovation process plot} we compare the standardized innovation process of the two specifications.The second specification reports an improvement, as the innovation process does not report anymore a clear change in variance and looks more close to a random walk\footnote{Note that there is still a not captured seasonal behaviour and some other noises that we will try to capture through a regression component}. 

```{r, fig.width=20, fig.height=3.5, out.width='100%', fig.cap="\\label{Fig:innovation process plot}Standardized one-step-ahead forecast errors. (a) Time invariant model. (b) Time variant model."}
res_LG <- residuals(LG_Filt, sd=FALSE)
res_LG = data.frame(res_LG,louisiana_df$n_months)
colnames(res_LG) = c("res", "time")

A <- ggplot(res_LG, aes(x=time)) +
  geom_line(aes(y=res), size=0.4) +
  theme_classic(base_size=9.5) +
  theme(panel.grid.minor = element_line(size=0.5)) +
  labs(x="Year", y="Residuals") +
  scale_x_continuous(breaks=seq(0,200,25),labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  scale_y_continuous(breaks=seq(-4.5,4.5,1), limits = c(-3,4.5)) +
  theme(axis.text=element_text(size=19),
        axis.title.x=element_text(size=21, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21, family="Times", margin=margin(r=5)))

res_LG_v <- residuals(LG_Filt_v, sd=FALSE)
res_LG_v = data.frame(res_LG_v,louisiana_df$n_months)
colnames(res_LG_v) = c("res", "time")

B <- ggplot(res_LG_v, aes(x=time)) +
  geom_line(aes(y=res), size=0.4) +
  theme_classic(base_size=9.5) +
  theme(panel.grid.minor = element_line(size=0.5)) +
  labs(x="Year", y="Residuals") +
  scale_x_continuous(breaks=seq(0,200,25),labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  scale_y_continuous(breaks=seq(-4.5,4.5,1), limits = c(-3,4.5)) +
  theme(axis.text=element_text(size=19),
        axis.title.x=element_text(size=21, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21, family="Times", margin=margin(r=5)))

ggarrange(A, B,
          ncol=2, nrow=1, labels = c("(a)", "(b)"))
```

\subsection{2.2. Seasonal Dynamic Linear Model}
We now fit a seasonal dlm, suitable to capture the seasonal behavior of the process. As explained above, there are evidences that the frequency is 12 months. Indeed, monthly-specific factors influence the series, such as temperature, precipitations and economic activity. Hence, we consider 12 seasonal factors $\alpha_{1} \dots \alpha_{12}$ imposing the usual restriction $\sum_{i=1}^{12} \alpha_{i} = 0$\footnote{This linear constraint implies that there are effectively only 11 free seasonal factors. Therefore, in our representation we use an 11-dimensional state vector}. As usual, the model is specified as: 
\begin{align*} 
\begin{cases}
   Y_t = \begin{bmatrix}1 & 0 & \cdots & 0 \end{bmatrix} \theta_t + v_t \quad & v_t \overset{iid}\sim
  \mathcal{N}(0, \sigma_{V}^{2}) 
  \\
  \\
   \theta_t = \begin{bmatrix} -1 &  & \cdots &  & -1 \\ 1 & 0  & 0 & \cdots & 0 \\ 0 & 1 & 0 & \cdots & 0 \\ \vdots &  & \ddots &  & \vdots \\ 0 & \cdots & 0& 1 & 0 \end{bmatrix} \theta_{t-1} + w_t \quad & w_t
  \overset{iid}\sim \mathcal{N}_{2}(0, W = \begin{bmatrix}\sigma_{w}^{2} & 0 & \cdots  & 0 \\ 0 & 0 & \cdots  & 0 \\  \vdots &  & \ddots  & \vdots \\  0 & 0 & 0  & \cdots  \end{bmatrix})
  \\
  \end{cases}
\end{align*}
\

\textcolor{red}{ WHICH ASSUMPTIONS?} Notice that the state vector $\theta_{t}$ contains 11 seasona factors properly permutated through the matrix $G$. Once again, we estimate the unkown variances by MLE, obtaining in Table \ref{tab:MLE_seasonal}: 
```{r}
#Seasonal DLM 
build_seasonal  <- function(param){dlmModSeas(frequency=12, dV = exp(param[1]), dW=c(exp(param[2]),0,0,0,0,0,0,0,0,0,0))}

# Estimate MLE of parameters
fit_seasonal <- dlmMLE(y, rep(0.5,2), build_seasonal, hessian=TRUE)
#fit_stagional$convergence
# Calculate standard errors of the MLE using delta method
estVarLog_seasonal <- solve(fit_seasonal$hessian)
estVar_seasonal <- diag(exp(fit_seasonal$par)) %*% estVarLog_seasonal %*% + diag(exp(fit_seasonal$par))
SE_seasonal <- sqrt(diag(estVar_seasonal))

```

```{r}
# MLE
MLE_sigmaV  <- paste0(round(exp(fit_seasonal$par[1]),5))
MLE_sigmaW <- paste0(round(exp(fit_seasonal$par[2]),9))

# se(MLE)
se_sigmaV  <- paste0('(', round(SE_seasonal[1],5), ')')
se_sigmaW <- paste0('(', round(SE_seasonal[2],9), ')')

# Summary table
MLEsum <- data.frame(sigmaV=c(MLE_sigmaV, se_sigmaV),
                     sigmaW=c(MLE_sigmaW, se_sigmaW))
knitr::kable(MLEsum, "latex", booktabs=T, align="c",
             col.names = c("$\\hat{\\sigma}^2_{v}$",
                           "$\\hat{\\sigma}^2_{w}$"),
             escape=F, caption="Maximum Likelihood estimates of the unknown
             parameters\\label{tab:MLE_seasonal}") %>%
  column_spec(1:2, width="5em") %>%
  collapse_rows(columns=1, latex_hline="none") %>%
  footnote(general="Standard errors in parentheses.",
           general_title="Note:",
           footnote_as_chunk=T, escape=F) %>%
  kable_styling(latex_options="hold_position")
```

\subsection{2.3. Dynamic linear regression}

We take as explanatory variable $x_t$ Louisiana's coal consumption for electricity generation (in particular, for the totality of the electric power industry). The model we consider is the following:
\begin{eqnarray*} 
y_t &=& \alpha_t + \beta_t x_t + v_t , \quad v_t \overset{i.i.d.}\sim  N(0, \sigma_{V}^{2})\\
\theta_t &=& G \theta_{t-1} + w_t , \qquad w_t \overset{i.i.d.}\sim N(0, W)
\end{eqnarray*}
where
\begin{eqnarray*}
\theta_t=[\alpha_t, \beta_t]', \qquad  \theta_0 \sim N_2(m_0, C_0), \qquad \theta_0 \indep (v_t) \indep (w_t)
\end{eqnarray*}
and $G=I_2$ and $W=diag(\sigma_{W_{1}}^{2}, \sigma_{W_{2}}^{2})$ i.e the two time variyng coefficients $\alpha_t$ and $\beta_t$ are modeled as independent random walks. 

```{r}
# Import dataset
coal_cons <- read.csv(url("https://raw.githubusercontent.com/SimoneArrigoni/US_pollution/master/Louisiana_coal_consumption.csv"), sep=";", header=T, stringsAsFactors=F)
x <- as.numeric(coal_cons$tot_electric_power_industry)

# Build dynamic linear regression model
# note: initialize the algorithm using the initial level of y and the sample variance before the break
build_DR <- function(param){dlmModReg(x, dV=exp(param[1]), dW=c(0, param[2]), m0=c(y[1],0))}

# Estimate MLE of parameters
fit_DR <- dlmMLE(y, c(0.5, 0.5), build_DR, hessian=TRUE) 
#fit_DR$convergence
parameters = unlist(build_DR(fit_DR$par)[c("V","W")])

#fit_DR$hessian 

# Calculate standard errors based on numeriacally evaluated hessian matrix
estVarLog_DR <- solve(fit_DR$hessian)
estVar_DR <- diag(exp(fit_DR$par)) %*% estVarLog_DR %*% + diag(exp(fit_DR$par))
SE_DR <- sqrt(diag(estVar_DR))

# Put MLE in the model
DR <- build_DR(fit_DR$par)

# Kalman filter
DR_Filt <- dlmFilter(y, DR)

# Remove the first item (m_0) in the vector of filtered states
filt_est = dropFirst(DR_Filt$m)

# Compute the vector of state variances and standard deviations
list_c <- dlmSvd2var(DR_Filt$U.C, DR_Filt$D.C)
vol_C <- sqrt(unlist(list_c))

# One step ahead forecast
y_filt_DR <- DR_Filt$f
# We observed an outlier at prediction 128, that has been removed. The data point has been filled by linear interpolation
y_filt_DR[128] <- (y_filt_DR[127]+y_filt_DR[129])/2
```

In Table \ref{tab:MLE_dr} are presented the MLEs of the unknown coefficients and the associated standard errors.

```{r}
# MLE
MLE_sigmaV  <- paste0(round(parameters[1],5))
MLE_sigmaW1 <- paste0(round(parameters[2],5))
MLE_sigmaW2 <- paste0(round(parameters[5],9))

# se(MLE)
se_sigmaV  <- paste0('(', round(SE_DR[1],5), ')')
se_sigmaW1 <- paste0('(', round(SE_DR[2],5), ')')
se_sigmaW2 <- paste0('(', round(SE_DR[3],9), ')')

# Summary table
MLEsum <- data.frame(sigmaV=c(MLE_sigmaV, se_sigmaV),
                     sigmaW1=c(MLE_sigmaW1, se_sigmaW1),
                     sigmaW2=c(MLE_sigmaW2, se_sigmaW2))
knitr::kable(MLEsum, "latex", booktabs=T, align="c",
             col.names = c("$\\hat{\\sigma}^2_{v}$",
                           "$\\hat{\\sigma}^2_{w_1}$",
                           "$\\hat{\\sigma}^2_{w_2}$"),
             escape=F, caption="Maximum Likelihood estimates of the unknown
             parameters\\label{tab:MLE_dr}") %>%
  column_spec(1:3, width="5em") %>%
  collapse_rows(columns=1, latex_hline="none") %>%
  footnote(general="Standard errors in parentheses",
           general_title="Note:",
           footnote_as_chunk=T, escape=F) %>%
  kable_styling(latex_options="hold_position")
```

Let's now compute the smoothing estimates of $\theta_t=(\alpha_t, \beta_t)'$, for $t=0, \ldots, T$. The smoothing estimates of $\beta_t$ are plotted over time together with their $90\%$ credible intervals in Figure \ref{fig:smooth_est}.

```{r, fig.width=7, fig.height=3, fig.cap="\\label{fig:smooth_est} Smoothing estimates of beta with credible intervals"}

# Smoothing estimates

cO_Smooth = dlmSmooth(y, DR)
smooth_state_est <- dropFirst(cO_Smooth$s[,2])

# Smoothing variances
smooth_var <- dlmSvd2var(cO_Smooth$U.S, cO_Smooth$D.S)
smooth_sd <- (unlist(smooth_var))^.5
beta_sd <- vector(mode = "numeric", length = length(smooth_var))
for (i in 1:length(smooth_var)){
  beta_sd[i] <- smooth_sd[i*4]
}
smooth_sd_est <- dropFirst(beta_sd)

# C.I. upper and lower bounds
upper_smooth <- smooth_state_est + 1.64*smooth_sd_est # upper bound of 90% C.I.
lower_smooth <- smooth_state_est - 1.64*smooth_sd_est # lower bound of 90% C.I.

# Create df
df_co_ci <- data.frame(louisiana_df$n_months, smooth_state_est, upper_smooth, lower_smooth)
colnames(df_co_ci) = c("n_months", "smooth_state_est", "upper_smooth","lower_smooth")
# Plot the smoothing estimates with their credible intervals
ggplot(df_co_ci, aes(x=n_months)) +
  geom_line(aes(y=smooth_state_est), size=0.5) +
  geom_line(aes(y=upper_smooth), color = "red", linetype = "dashed", size=0.2) +
  geom_line(aes(y=lower_smooth), color = "red", linetype = "dashed", size=0.2) +
  geom_ribbon(aes(ymin=lower_smooth, ymax=upper_smooth), alpha=.1, fill="red") +
  theme_classic(base_size=9.5) +
  theme(panel.grid.minor = element_line(size=0.5)) +
  labs(x="Year", y="Beta", color = "black") +
  scale_x_continuous(breaks=seq(0,200,25),labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  scale_y_continuous() +
  theme(axis.title.x=element_text(family="Times", margin=margin(t=5)),
        axis.title.y=element_text(family="Times", margin=margin(r=5)))
```

```{r, fig.width=7, fig.height=2, fig.cap="\\label{fig:filter_est} Filtering states estimates"}
# Build df
df_filt <- data.frame(louisiana_df$n_months, y, y_filt, struct_fit, y_filt_DR)
colnames(df_filt) = c("n_months", "y", "y_filt","struct_fit", "y_filt_DR")

# Plot
colors <- c("Observed data" = "black", "Linear Growth" = "springgreen4", "Basic structural model"="blue", "Dynamic regression"="red")
ggplot(df_filt, aes(x=n_months)) +
  geom_line(aes(y=y, color="Observed data"), size=0.4) +
  geom_line(aes(y=y_filt, color = "Linear Growth"), size=0.4) +
  geom_line(aes(y=struct_fit, color="Basic structural model"), size=0.4) +
  geom_line(aes(y=y_filt_DR, color="Dynamic regression"), size=0.4) +
  #geom_vline(xintercept = as.Date("2008/1/1"), color= "black", size=0.3) +
  theme_classic(base_size=9.5) +
  theme(panel.grid.minor = element_line(size=0.5)) +
  labs(x="Year", y=expression(CO~(ppm)), color = "") +
  scale_y_continuous(breaks=seq(0,1.5,0.25)) +
  scale_x_continuous(breaks=seq(0,200,25),labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  theme(axis.title.x=element_text(family="Times", margin=margin(t=5)),
        axis.title.y=element_text(family="Times", margin=margin(r=5))) +
        scale_color_manual(values = colors) +
        theme(legend.justification = c("left", "top"),
              legend.box.just = "left",legend.margin = margin(2, 2, 2, 2))
```

```{r}
source("DR_model_checking.R")
```

\section{Section 3. Multivariate DLM}

\subsection{3.1. Seemingly unrelated regression (SUR)}

As a last step, we build a multivariate dynamic regression model. In addition to the $\text{CO}$ series, we consider now also the $\text{SO}_2$ monthly data. Assuming that the intercepts and the slopes are correlated across the two pollutants, we can define a \textit{seemingly unrelated regression model} (SUR) as follows:
\begin{align*}
  & Y_t = (F_t \otimes I_2) \theta_t + v_t, & v_t \overset{iid}\sim \mathcal{N}_2(0,V) \\
  & \theta_t = (G \otimes I_2) \theta_{t-1} + w_t, & w_t \overset{iid}\sim
  \mathcal{N}_4(0,W)
\end{align*}
with
\begin{equation*}
  \underset{2 \times 1}{Y_t} = \begin{bmatrix} Y_{CO,t} \\ Y_{SO_2,t} \end{bmatrix}, \quad
  \underset{4 \times 1}{\theta_t} = \begin{bmatrix} \\ \alpha_{CO,t} \\ \alpha_{SO_2,t}, \\ \beta_{CO,t} \\
  \beta_{SO_2,t} \end{bmatrix}, \quad
  \underset{1 \times 2}{F_t} = \begin{bmatrix} 1 & x_t \end{bmatrix}, \quad
  \underset{2 \times 2}{G} = I_2 \quad \text{ and } \quad
  \underset{4 \times 4}{W} = blockdiag(W_{\alpha}, W_{\beta}).
\end{equation*}
Note that, since both pollutants are by-products of incomplete fuel combustion, we consider $x_{i,t}=x_t$, that is we take the same explanatory variable for the two series.

```{r}
# Build df
y_m <- data.frame(louisiana_df$m_CO_mean, louisiana_df$m_SO2_mean)
y_m <- rename(y_m, c("CO"="louisiana_df.m_CO_mean", "SO2"="louisiana_df.m_SO2_mean"))
y_m <- data.matrix(y_m)
m <- ncol(y_m)

# Build SUR model
build_SUR <- function(param){
  mod <- dlmModReg(x)
  mod$FF <- mod$FF %x% diag(m)
  mod$JFF <- mod$JFF %x% diag(m)
  mod$GG <- mod$GG %x% diag(m)
  mod$V <- mod$V %x% matrix(0, m, m)
  mod$V <- c(exp(param[1]), 0,
             0, exp(param[2]))
  ## Old version:
  #mod$V <- diag(c(exp(param[1]), exp(param[2])))
  mod$W <- mod$W %x% matrix(0, m, m)
    Wa <- matrix(c(exp(param[3]), 0, 0, exp(param[4])), 2, 2)
    Wb <- matrix(c(exp(param[5]), exp(param[6]), exp(param[7]), exp(param[8])), 2, 2)
    W <- bdiag(Wa, Wb)
  mod$W <- W
  ## Alternative version (why doesn't it work?):
  #mod$W <- c(exp(param[3]), 0, 0, 0,
  #           0, exp(param[4]), 0, 0,
  #           0, 0, exp(param[5]), exp(param[6]),
  #           0, 0, exp(param[7]), exp(param[8]))
  mod$m0 <- c(y_m[1,1], y_m[1,2], 0, 0)
  return(mod)
}

# Estimate MLE of parameters
fit_SUR <- dlmMLE(y_m, parm=rep(0.5, 8), build_SUR, hessian=TRUE)
unlist(build_SUR(fit_SUR$par)[c("V","W")])

# Calculate SE of the MLE using delta method
estVarLog_SUR <- solve(fit_SUR$hessian)
estVar_SUR <- diag(exp(fit_SUR$par)) %*% estVarLog_SUR %*% + diag(exp(fit_SUR$par))
SE_SUR <- sqrt(diag(estVar_SUR))

# Put MLE in the model
SUR <- build_SUR(fit_SUR$par)

# Kalman filter
SUR_Filt <- dlmFilter(y_m, SUR)
SUR_sd <- residuals(SUR_Filt)$sd

# Kalman smoother
SUR_Smooth <- dlmSmooth(y_m, SUR)
```

\subsection{(ADDITIONAL) Seemingly unrelated time series equations (SUTSE)}

Let us still consider the time series for $\text{CO}$ and $\text{SO}_2$. From visual inspection of Figure \ref{fig:louisiana_plots} it appears that the two series display a similar type of qualitative behavior, that can be modeled by a linear growth DLM. To set up a multivariate model for the two series, we combine the two linear growth models in a comprehensive \textit{seemingly unrelated time series equations model} (SUTSE). This is written as:
\begin{align*}
  & Y_t = (F \otimes I_2) \theta_t + v_t, & v_t \sim \mathcal{N}_2(0,V) \\
  & \theta_t = (G \otimes I_2) \theta_{t-1} + w_t, & w_t \sim \mathcal{N}_4(0,W)
\end{align*}
with
\begin{equation*}
  \underset{2 \times 1}{Y_t} = \begin{bmatrix} Y_{CO,t} \\ Y_{SO_2,t} \end{bmatrix}, \quad
  \underset{4 \times 1}{\theta_t} = \begin{bmatrix} \\ \mu_{CO,t} \\ \mu_{SO_2,t}, \\ \beta_{CO,t} \\
  \beta_{SO_2,t} \end{bmatrix}, \quad
  \underset{1 \times 2}{F} = \begin{bmatrix} 1 & 0 \end{bmatrix}, \quad
  \underset{2 \times 2}{G} = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \quad \text{ and } \quad \underset{4
  \times 4}{W} = blockdiag(W_{\mu}, W_{\beta}).
\end{equation*}
We assume that growth $\beta_{i,t}$ is correlated among pollutants. Moreover, to simplify the model in order to reduce the overall number of parameters, we also assume that the two linear growth models are integrated random walks, that is $W_{\mu}=0$.

```{r}
# Build SUTSE model
build_SUTSE <- function(param){
  mod <- dlmModPoly(2)
  mod$FF <- mod$FF %x% diag(m)
  mod$GG <- mod$GG %x% diag(m)
  mod$V <- mod$V %x% matrix(0, m, m)
  mod$V <- c(exp(param[1]), exp(param[2]),
             exp(param[3]), exp(param[4]))
  ## Old version:
  #mod$V <- matrix(c(exp(param[1]), param[2], exp(param[3]), param[4]), m, m)
  mod$W <- mod$W %x% matrix(0, m, m)
  mod$W <- c(0, 0, 0, 0,
             0, 0, 0, 0,
             0, 0, exp(param[5]), exp(param[6]),
             0, 0, exp(param[7]), exp(param[8]))  
  ## Old version:  
    #Wu <- matrix(0, m, m)
    #Wb <- matrix(c(exp(param[5]), param[5], exp(param[7]), param[8]), m, m)
    #W <- bdiag(Wu, Wb)
  #mod$W <- W
  mod$m0 <- c(y_m[1,1], y_m[1,2], 0, 0)
  return(mod)
}
  
# Estimate MLE of parameters
fit_SUTSE <- dlmMLE(y_m, parm=rep(0.5, 8), build_SUTSE, hessian=TRUE)
unlist(build_SUTSE(fit_SUTSE$par)[c("V","W")])

# Calculate SE of the MLE using delta method
estVarLog_SUTSE <- solve(fit_SUTSE$hessian)
estVar_SUTSE <- diag(exp(fit_SUTSE$par)) %*% estVarLog_SUTSE %*% + diag(exp(fit_SUTSE$par))
SE_SUTSE <- sqrt(diag(estVar_SUTSE))

# Put MLE in the model
SUTSE <- build_SUTSE(fit_SUTSE$par)

# Kalman filter
SUTSE_Filt <- dlmFilter(y_m, SUTSE)
SUTSE_sd <- residuals(SUTSE_Filt)$sd
```

\section{Conclusion}

In conclusion, \dots

```{r, include=F}
source("forecasting_accuracy.R")
```

```{r, include=T}
# Forecasting accuracy: Summary table
tabf <- matrix(nrow=3, ncol=8)
rownames(tabf) <- c('MSE','MAE','MAPE')
colnames(tabf) <- c('HMM', 'Linear growth', 'Constant speed', 'Integrated random walk', 'BSM','Dynamic regression', 'SUR', 'SUTSE')

#tabf[1,1] <- MSE_HMM
#tabf[2,1] <- MAE_HMM
#tabf[3,1] <- MAPE_HMM

tabf[1,2] <- MSE_LG
tabf[2,2] <- MAE_LG
tabf[3,2] <- MAPE_LG

tabf[1,3] <- MSE_LG_2
tabf[2,3] <- MAE_LG_2
tabf[3,3] <- MAPE_LG_2

tabf[1,4] <- MSE_LG_3
tabf[2,4] <- MAE_LG_3
tabf[3,4] <- MAPE_LG_3

tabf[1,5] <- MSE_BSM
tabf[2,5] <- MAE_BSM
tabf[3,5] <- MAPE_BSM

tabf[1,6] <- MSE_DR
tabf[2,6] <- MAE_DR
tabf[3,6] <- MAPE_DR

tabf[1,7] <- MSE_SUR
tabf[2,7] <- MAE_SUR
tabf[3,7] <- MAPE_SUR

tabf[1,8] <- MSE_SUTSE
tabf[2,8] <- MAE_SUTSE
tabf[3,8] <- MAPE_SUTSE

kable(tabf, caption="Measures of predictive accuracy")
```

\newpage
\section{References}