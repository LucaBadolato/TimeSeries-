---
title: "20236 Time Series Analysis - Final project"
author:
- Simone Arrigoni (1794692)
- Luca Badolato (3086040)
- Simone Valle (3088281)
subtitle: "Group: Arrigoni_Badolato_Valle"
date: "5 June 2020"
output: pdf_document
geometry: margin=2cm
nocite: '@*'
bibliography: References.bib
header-includes:
  \usepackage[utf8]{inputenc}
  \usepackage[bottom]{footmisc}
  \usepackage{setspace}
  \usepackage{apacite}
  \usepackage{natbib}
  \usepackage{algpseudocode}
  \usepackage{algorithm}
  \usepackage{bm}
  \usepackage{amsmath}
  \usepackage{amssymb}
  \usepackage{graphicx}
  \usepackage{subfig}
  \usepackage{booktabs, caption}
  \usepackage{array}
  \usepackage{threeparttable}
  \usepackage{listings}
  \usepackage{physics}
  \usepackage{float}
  \floatplacement{figure}{H}
  \usepackage{color} % red, green, blue, yellow, cyan, magenta, black, white
  \definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
  \definecolor{mylilas}{RGB}{170,55,241}
  \definecolor{codegray}{RGB}{0.5,0.5,0.5}
  \definecolor{orange}{RGB}{255,165,0}
  \DeclareMathOperator*{\E}{\mathbb{E}}
  \DeclareMathOperator*{\Ec}{\mathbb{E}_t}
  \setlength\parindent{0pt}
  \newcommand{\separate}{\vspace{+1.5mm}}
  \newcommand{\smallsqueeze}{\vspace{-1mm}}
  \newcommand{\squeeze}{\vspace{-2.5mm}}
  \newcommand{\bigsqueeze}{\vspace{-3mm}}
  \newcommand{\indep}{\perp \!\!\! \perp}
---

```{r, include=F}
# Load useful packages
library(utf8)
library(labeling)
library(rmarkdown)
library(readxl)
library(httr)
library(knitr)
library(tseries)
library(tinytex)
library(scales)
library(dlm)
library(magrittr)
library(stringr)
library(depmixS4)
library(tidyverse)
library(ggthemes)
library(latex2exp)
library(kableExtra)
library(ggpubr)
library(reshape2)
library(cowplot)

# Settings
knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE,
                      echo    = FALSE,
                      include = TRUE,
                      fig.pos = "H",
                      fig.align = "center",
                      out.width='80%')
```
\squeeze

\subsection{Introduction}
\squeeze
Pollution is one of the most debated social issues of our times. It has been attracting more and more attention given its serious implications on the health of both the Earth and human beings. The economic activity under the model of production and consumption that has been characterizing the last century is, with no doubt, at the very basis of such complex phenomenon. Only in the last fifteen years some consciousness about the importance of pollution in the long-run has evolved and serious interventions have been made. Moreover, among the numerous component which pollution can be split into, air pollution is certainly one of the most important and tightly linked to strict economic activity.\
Therefore, the purpose of the present work is to model through common statistical techniques the evolution of air pollution during this last crucial fifteen years in a developed economy, i.e. the United States, trying to grasp some intuition about this fundamental phenomenon. Hence, after some preliminary descriptive steps, we present the models we think are more appropriate to analyze such complex phenomenon. Finally, we conclude with a comparison of the predictive performances of each specification. 
\squeeze

\subsection{Data}
\squeeze
The dataset \texttt{US\_pollution} contains monthly and daily data about air pollution for each state in the United States\footnote{The preliminary analysis that led us from the raw data (with frequency of 4-intra-day observations) to the current dataset is made available in the GitHub repository at \url{https://github.com/SimoneArrigoni/US_pollution}. We relied on a moving average algorithm of order $120$ to smooth the data, assuming months of 30 days and a year of 360 days.}. It is provided by the U.S. Environmental Protection Agency (EPA). Specifically, four key indicators of air pollution are considered: carbon monoxide ($\text{CO}$), sulfur dioxide ($\text{SO}_{2}$), ground level ozone ($\text{O}_{3}$) and nitrogen dioxide ($\text{NO}_{2}$). Particularly suitable for our analysis:
\begin{itemize}
\item \textbf{Carbon monoxide ($\text{CO}$)}:  It forms from incomplete combustions mainly from vehicle exhausts (roughly 75\% of all carbon monoxide emissions nationwide and up to 95\% in cities), fuel combustion in industrial processes and natural sources. Cold temperatures make combustion less complete, therefore we expect also in this case a seasonal behavior. 
\item \textbf{Sulfur dioxide ($\text{SO}_{2}$)}: It is produced when sulfur-containing fuels are burned. It is common mainly in large industrial complexes. 
\end{itemize}

```{r, echo=F}
# Import the dataset
louisiana_df <- read.csv(url("https://raw.githubusercontent.com/SimoneArrigoni/US_pollution/master/Louisiana_pollution.csv"), sep = ";", header=T)

# Correlation matrix
datacor <- subset(louisiana_df, select=c(m_O3_mean, m_NO2_mean, m_CO_mean, m_SO2_mean))
cor.datacor <- cor(datacor, use="complete.obs")
cor_mat <- round(cor(datacor),2)
row.names(cor_mat) <- c("$\\text{O}_{3}$", "$\\text{NO}_{2}$", "$\\text{CO}$", "$\\text{SO}_{2}$")

# Correlation table
#knitr::kable(cor_mat, "latex", booktabs=T, align="c", escape=F,
#             caption="Correlation table \\label{tab:louisiana_corr}",
#             col.names=c("$\\text{O}_{3}$", "$\\text{NO}_{2}$", "$\\text{CO}$", "$\\text{SO}_{2}$")) %>%
#  column_spec(1:4, width="5em") %>%
#kable_styling(latex_options="hold_position")
```

Very intuitively, the two variables of interest are interconnected and are the building blocks of a more general phenomenon, that is air pollution. Furthermore, they are related by a common latent process that can be summarized by several components such as industrialization, car traffic and regulation.\
We will be focusing on measures for Louisiana, recorded monthly from January 2001 to June 2016, for a total of `r length(louisiana_df$n_months)` observations. The main advantage of studying the evolution of air pollution in the same country over the years is that this allows us to control for fixed effects and to better study the latent process of interest that relates the four pollution components.\
The data show a positive correlation between $\text{CO}$ and $\text{SO}_2$ (`r cor_mat[4,3]`). Provided that the correlation between pollutants shows significant differences in regional patterns and that $\text{CO}$ is a good indicator for industrial and biomass burning pollution [@Logan1981], a positive and strong correlation with $\text{O}_{3}$ indicates that a region has experienced photochemical $\text{O}_{3}$ production from its precursors (including $\text{CO}$) and is aligned with previous studies in the field [@Voulgarakis2011]. $\text{CO}$ is also positively correlated with $\text{SO}_2$ which is justified by the fact that they share a main source of production, i.e. coal extraction and combustion\footnote{Other studies report that, in the US, $\text{O}_3$ is on average positively correlated with $\text{NO}_2$, which is opposite to our empirical findings about Louisiana, where they are slightly negatively correlated. A possible explanation can be given by substitution effects: an increase of nitrogen dioxide levels can be associated to the use of more fossil fuels that produce $\text{NO}_2$ instead of others. This might be related to Louisiana's fixed effects, however further research would be beyond the purpose of our study.}.\
We plot in Figure \ref{fig:louisiana_plots} the time series of the two pollutants we will consider in the following analysis. One can clearly see that the time series, each with its specific features, do not in general look stationary. Therefore, an analysis which employs HMM and DLMs seems appropriate. \separate

```{r, fig.width=18, fig.height=4, out.width='100%', fig.cap="\\label{fig:louisiana_plots}Air pollution in the state of Louisiana"}

# Time series plots
O3 <- ggplot(louisiana_df, aes(x=n_months, y=m_O3_mean)) +
  geom_line(color="black", size=0.5) +
  theme_bw(base_size=9) +
  labs(y=expression(O[3]~(ppm)), x="Year") +
  scale_x_continuous(breaks=seq(0,186,24),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0,0.5,0.5,0), "cm"))

NO2 <- ggplot(louisiana_df, aes(x=n_months, y=m_NO2_mean)) +
  geom_line(color="black", size=0.5) +
  theme_bw(base_size=9) +
  labs(y=expression(NO[2]~(ppb)), x="Year") +
  scale_x_continuous(breaks=seq(0,186,24),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0,0,0.5,0.5), "cm"))

CO <- ggplot(louisiana_df, aes(x=n_months, y=m_CO_mean)) +
  geom_line(color="black", size=0.5) +
  theme_bw(base_size=9) +
  labs(y=expression(CO~(ppm)), x="Year") +
  scale_x_continuous(breaks=seq(0,186,24),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0.5,0.5,0,0), "cm"))

SO2 <- ggplot(louisiana_df, aes(x=n_months, y=m_SO2_mean)) +
  geom_line(color="black", size=0.5) +
  theme_bw(base_size=9) +
  labs(y=expression(SO[2]~(ppb)), x="Year") +
  scale_x_continuous(breaks=seq(0,186,24),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),        
        plot.margin=unit(c(0.5,0,0,0.5), "cm"))

ggarrange(CO, SO2, ncol=2, nrow=1, align="h")
```
\bigsqueeze

In particular, among the pollutants, $\text{CO}$ is probably the most tightly linked with the economic and industrial network of a country. Therefore, we select this as the series of main interest for our analysis, the purpose of our work being to track the evolution of air pollution relative to an underlying economic process.\
As an exploratory step, we decompose the time series in its structural components using an additive model\footnote{For sake of brevity we do not report the plots of the decomposed components.}. The decomposition confirms the presence of a quarterly seasonal component and shows a negative trend up to a clear breakpoint around 2008. The effect of this break seems to be twofold: first, the trend, that was negative in the pre-break period, disappears; second, the variance reduces markedly in the post-break period.
\squeeze

\section{Section 1. HMM}
\squeeze
The features we just identified suggest that the process may be well described by a homogeneous \textit{hidden Markov model} (HMM) with a trend component. Therefore, we estimate a homogeneous HMM with two states, specified as:
\begin{align*}
\begin{cases}
  Y_{t}=\mu_{1} + \beta_{1}t + \varepsilon_{t}, &
  \varepsilon_{t}\overset{iid}{\sim}N(0,\sigma_{1}^2) \qquad \text{if} \; S_{t}=1 \\
  Y_{t}=\mu_{2} + \beta_{2}t + \varepsilon_{t}, &
  \varepsilon_{t}\overset{iid}{\sim}N(0,\sigma_{2}^2) \qquad \text{if} \; S_{t}=2 \\
\end{cases}
\end{align*}
where there are three state-dependent parameters, namely the level $\mu_i$, the slope $\beta_i$, and the variance $\sigma_i$, for $i=1,2$. If our initial guess were correct, we would expect $\mu_1 > \mu_2$, $\beta_1<0$, $\beta_2=0$ and $\sigma_1^2>\sigma_2^2$.\
However, a homogeneous HMM with two states proves unable to capture the actual behavior of the process\footnote{Again, results are not reported here but can be easily replicated using the provided R code.}. A better fit can be obtained using a model with three hidden states that capture periods with negative trend and high volatility, periods with no trend and low volatility and periods with no trend and high volatility. The model is specified as above with an additional equation for $S=3$ and ideally $\mu_1 > \mu_2 \approx \mu_3$, $\beta_1<0$, $\beta_2=\beta_3=0$ and $\sigma_2^2<\sigma_3^2<\sigma_1^2$.\
Table \ref{tab:HMM_MLE} confirms our guess. Indeed, the slope estimate in the first state, $\hat{\beta_1}_{MLE}$, is negative and significant, whereas in the other states is null. Notice that, although apparently low, this coefficient implies a relatively high percentage change after comparing it with our data, which, roughly speaking, range in $(0,1)$. In fact, relating this estimate with a sample mean of $0.39$, we obtain an estimated monthly decline around $-0.015$ corresponding to around $18.3\%$ on an annual basis. Moreover, the model is able to capture shifts in the intecept as well as those in the variance as described before.\
The series with HMM estimated state-dependent means and state-dependent standard deviations are shown in Figure \ref{fig:HMM_results} (left panel) along with the Viterbi states obtained through \textit{global decoding} and with the posterior probabilities (right panel).

```{r echo=F, include=F}
# Model specification with 2 states
source("HMM_2states.R")

# Model specification with 3 states
y <- as.numeric(louisiana_df$m_CO_mean)
nstates <- 3
set.seed(2)
HMM <- depmixS4::depmix(y ~ 1 + louisiana_df$n_months, data=data.frame(y), nstates=nstates)

# Estimation of the unknown parameters
HMM_fit <- depmixS4::fit(HMM)
```

```{r}
# MLE
mu1hat <- paste0(round(HMM_fit@response[[1]][[1]]@parameters[["coefficients"]][["(Intercept)"]],3))
mu2hat <- paste0(round(HMM_fit@response[[2]][[1]]@parameters[["coefficients"]][["(Intercept)"]],3))
mu3hat <- paste0(round(HMM_fit@response[[3]][[1]]@parameters[["coefficients"]][["(Intercept)"]],3))
beta1hat <- paste0(round(HMM_fit@response[[1]][[1]]@parameters[["coefficients"]][["louisiana_df$n_months"]],3))
beta2hat <- paste0(round(HMM_fit@response[[2]][[1]]@parameters[["coefficients"]][["louisiana_df$n_months"]],3))
beta3hat <- paste0(round(HMM_fit@response[[3]][[1]]@parameters[["coefficients"]][["louisiana_df$n_months"]],3)) 

sd1hat_n <- round(HMM_fit@response[[1]][[1]]@parameters$sd,3)
sd1hat <- paste0(sd1hat_n)
sd2hat_n <- round(HMM_fit@response[[2]][[1]]@parameters$sd,3)
sd2hat <- paste0(sd2hat_n)
sd3hat_n <- round(HMM_fit@response[[3]][[1]]@parameters$sd,3)
sd3hat <- paste0(sd3hat_n)

# se(MLE)
MLE_SE=depmixS4::standardError(HMM_fit)

SE_mu1hat_n <- round(MLE_SE$se[13],3)
SE_mu1hat <- paste0('(', SE_mu1hat_n, ')')
SE_mu2hat_n <- round(MLE_SE$se[16],3)
SE_mu2hat <- paste0('(', SE_mu2hat_n, ')')
SE_mu3hat_n <- round(MLE_SE$se[19],3)
SE_mu3hat <- paste0('(', SE_mu3hat_n, ')')

SE_beta1hat_n <- round(MLE_SE$se[14],3)
SE_beta1hat <- paste0('(', SE_beta1hat_n, ')')
SE_beta2hat_n <- round(MLE_SE$se[17],3)
SE_beta2hat <- paste0('(', SE_beta2hat_n , ')')
SE_beta3hat_n <- round(MLE_SE$se[20],3)
SE_beta3hat <- paste0('(', round(MLE_SE$se[20],3), ')')
SE_sd1hat <- paste0('(', round(MLE_SE$se[15],3), ')')
SE_sd2hat <- paste0('(', round(MLE_SE$se[18],3), ')')
SE_sd3hat <- paste0('(', round(MLE_SE$se[21],3), ')')

# Build a summary table for estimates
MLEsum <- data.frame(state=c(rep("S=1",2), rep("S=2",2), rep("S=3",2)),
                     mu=c(mu1hat, SE_mu1hat, mu2hat, SE_mu2hat,mu3hat, SE_mu3hat),
                     beta=c(beta1hat, SE_beta1hat, beta2hat, SE_beta2hat, beta3hat, SE_beta3hat),
                     sd=c(sd1hat, SE_sd1hat, sd2hat, SE_sd2hat, sd3hat, SE_sd3hat))

knitr::kable(MLEsum, "latex", booktabs=T, align="c",
             col.names=c("State",
                         "$\\hat{\\mu}_{MLE}$",
                         "$\\hat{\\beta}_{MLE}$",
                         "$\\hat{\\sigma}_{MLE}$"),
             escape=F, caption="Maximum Likelihood estimates of the state-dependent
             parameters \\label{tab:HMM_MLE}") %>%
  column_spec(1:4, width="5em") %>%
  collapse_rows(columns=1, latex_hline="none") %>%
  footnote(general="Standard errors in parentheses.",
           general_title="Note:",
           footnote_as_chunk=T, escape=F) %>%
  kable_styling(latex_options="hold_position")
```
\squeeze
```{r, include=F}
# Decoding
# Get the estimated state for each timestep 
estStates_HMM <- posterior(HMM_fit)
HMM_summary <- depmixS4::summary(HMM_fit)
```
\squeeze
```{r, fig.width=18, fig.height=5.5, out.width='100%', fig.cap="\\label{fig:HMM_results}State-dependent means and standard deviations"}
# Chart with state-dependent mean and standard deviations
HMM_main_CO <- data.frame(state=1:nstates,
                      mu=HMM_summary[,1],
                      beta=HMM_summary[,2],
                      sigma=HMM_summary[,3])
df_to_plot <- estStates_HMM %>%
  left_join(HMM_main_CO)
df_to_plot %<>%
  mutate(xtime=louisiana_df$n_months, yvalue=louisiana_df$m_CO_mean)

HMM_main <- ggplot(df_to_plot, aes(x=xtime, y=yvalue)) +
  geom_line(size=0.5) +
  geom_point(aes(x=xtime, y=mu+beta*xtime), col="blue", size=.8) +
  geom_ribbon(aes(ymin=mu+beta*xtime-2*sigma, ymax=mu+beta*xtime+2*sigma), alpha=.1) +
  theme_bw() +
  labs(y=expression(CO~(ppm)), x="Year") +
  scale_x_continuous(breaks=seq(0,186,24),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "2 years"), "%Y")) +
  scale_y_continuous(breaks=seq(0,1.5,0.25)) +
   theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=8)),
         axis.text.y=element_text(size=20, family="Times", margin=margin(r=8)),
         axis.title.x=element_text(size=20, family="Times", margin=margin(t=8)),
         axis.title.y=element_text(size=20, family="Times", margin=margin(r=8)),
         panel.grid.major=element_line(size=0.5),
         panel.grid.minor=element_blank(),
         plot.margin=unit(c(0.5,1,0.5,0.5), "cm"))

# Chart with Viterbi states and posterior probabilities
HMM_additional_CO <- data.frame(time_index=louisiana_df$n_months,
                           state=estStates_HMM[1],
                           S1=estStates_HMM[2],
                           S2=estStates_HMM[3]) %>% 
  gather("variable", "value", -time_index)

my_breaks <- function(x) { if (max(x)<4) seq(0,3,1) else seq(0,1,1) }
lab_names <- c('S1'="Post. S=1",'S2'="Post. S=2",'state'="Vit. states")

HMM_additional <- ggplot(HMM_additional_CO, aes(time_index, value)) +
  geom_line(color="black", size=0.5) +
  facet_wrap(variable~., scales="free_y", ncol=1,
             strip.position="left",
             labeller = as_labeller(lab_names)) +
  labs(x="Year") +
  scale_y_continuous(breaks=my_breaks) +
  scale_x_continuous(breaks=seq(0,186,36),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "3 years"), "%Y")) +
  theme_bw() +
  theme(panel.spacing=unit(1.5, "lines"),
        axis.text.x=element_text(size=20, family="Times", margin=margin(t=8)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=8, l=8)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=8)),
        axis.title.y=element_blank(),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        strip.text=element_text(size=18, family="Times"),
        strip.background=element_rect(colour="black", fill=NA),
        strip.placement="outside")

# Option 1
## ggarrange(HMM_main, HMM_additional, ncol=1, widths = c(6,0.3), heights = c(1,0.8))

# Option 2
ggarrange(HMM_main, HMM_additional, widths=c(4,2), align="h")
```
\bigsqueeze

Explanations of these results, which confirm a stable change in the evolution process of $\text{CO}$ in Louisiana, are several. First, the decline of carbon particles has been widely attributed to a shift from the use of coal to natural gas in U.S. electricity production which dates back exactly to 2007. This is also supported by a marked decline in US coal mines activity which started in that same year. Second, a high and positive correlation between economic growth is well documented, thus highlighting some relationship with the outbreak of the Great Recession. Moreover, 2007 was a crucial year when negotiations on greenhouse gas emission were made at international level (such as at the G8 summit and the Wien agreement)\footnote{A deeper analysis on the factors explaining this shift can be found in Feng et al. (2015), in which they analyse the series of $\text{CO}_2$.}.\
In addition to explaining such a shifts, we may want predict them using this homogeneous HMM. For this reason, we produce the one-step-ahead forecast treating the data as streaming ones. Further details on this are reported in the \textbf{Performance evaluation} section below.

```{r}
# One-step-ahead forecast
trans_mat <- HMM_fit@trDens[1,,] %>% t()
trans_mat <- matrix(t(trans_mat), ncol = 3)
HMM_forecast <- matrix(NA, nrow=length(y), ncol=1)
HMM_forecast_sd <- matrix(NA, nrow=length(y), ncol=1)
mu_var <- c((SE_mu1hat_n)^2, (SE_mu2hat_n)^2, (SE_mu3hat_n)^2 )
beta_var <- c((SE_beta1hat_n)^2, (SE_beta2hat_n)^2, (SE_beta3hat_n)^2 )
sigma_2 <- c((sd1hat_n)^2, (sd2hat_n)^2, (sd3hat_n)^2 )
for (t in 1:length(y)){
  v <- estStates_HMM[t,1]  
  HMM_forecast[t] <- (trans_mat[v,]%*%HMM_summary[,1])+(trans_mat[v,]%*%HMM_summary[,2])*(t)
  HMM_forecast_sd[t] <- sqrt((trans_mat[v,]%*%mu_var)+(trans_mat[v,]%*%beta_var)*(t^2)+(trans_mat[v,]%*%sigma_2))
}
HMM_forecast_lb <- HMM_forecast - 1.64 * HMM_forecast_sd
HMM_forecast_ub <- HMM_forecast + 1.64 * HMM_forecast_sd
```
\squeeze

\section{Section 2. Univariate DLMs}
\squeeze
We try to model the series of interest through \textit{dynamic linear model} (DLM) specifications. Indeed, DLMs are very attractive, since they allow a natural interpretation of the process as the combination of several components, such as trend, seasonal and regressive ones. Therefore, we consider different specifications which, simply combined, are able to explain all the features previously described\footnote{Therefore we are aware (and we expect) that by construction each model will miss some features of the process. For example, residuals for a linear growth model may shows a seasonal behavior but this is fully consistent with the additive structure we consider.}. 
\squeeze

\subsection{Linear growth model}
\squeeze
The first DLM specification we consider to examine $\text{CO}$ is a time-invariant linear growth model, which is suitable to capture both a latent level $\mu_{t}$ and a time-varying slope in its dynamics $\beta_{t}$. As usual, it is defined as follows:
\begin{eqnarray*}
  & Y_t = F \theta_t + v_t, \quad & v_t \overset{iid}\sim \mathcal{N}(0, \sigma_v^2) \\
  & \theta_t =  G \theta_{t-1} + w_t, \quad & w_t \overset{iid}\sim \mathcal{N}_{2}(0, W)
\end{eqnarray*}
where
\begin{equation*}
  \theta_t = \begin{pmatrix} \mu_{t} \\ \beta_{t} \end{pmatrix}, \quad
  F = \begin{bmatrix} 1 & 0 \end{bmatrix}, \quad
  G = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}, \quad
  W = \begin{bmatrix} \sigma_{w_1}^{2} & 0 \\ 0 & \sigma_{w_2}^{2} \end{bmatrix}, \quad
  \theta_0 \sim \mathcal{N}_{2}(m_0, C_0) \quad \text{ and } \quad \theta_{0} \indep (v_t) \indep (w_t).
\end{equation*}
In particular, we impose as starting condition $m_0=[ y_{1} \;\; 0 ]'$ and $C_0=diag(0.001,0.001)$ -- that is a relatively large variance, which reflects our uncertainty. The MLEs of the unknown coefficients are reported on the left in Table \ref{tab:MLE_LG}\footnote{We treated $\psi = (\sigma_{v}^{2}, \; \sigma_{w_1}^{2}, \;\sigma_{w_2}^{2})$ as unknown parameters and estimated them by ML. Then, we performed the analysis of interest plugging them in the model as if they were known constants. We proceeded in this way also in the following sections.}.\
A first visual inspection of the series of standardized one-step-ahead forecast errors (Figure \ref{fig:innovation_process}(a)), however, reveals the presence of both a seasonal behavior and a clear change in variance around 2008. We can take into account this second evidence allowing $\sigma_{v}^{2}$ to be time dependent.\
To this purpose, we estimate a linear growth model specified as above but considering $\sigma_{v_t}^{2} = \sigma_{v_1}^{2}$ for observations before 2008 and $\sigma_{v_t}^{2} = \sigma_{v_2}^{2}$ for observations after 2008. The MLEs of the time-varying model are reported alongside those of the time-invariant one in Table \ref{tab:MLE_LG}. As expected, the MLE of  $\sigma_{v_t}^{2}$ decreases after the breakpoint. \
The standardized innovation process of the time-varying specification are plotted in Figure \ref{fig:innovation_process}(b). The improvement with respect to the time-invariant model is clear, as the change in the variance is milder and the process looks closer to a random noise. Still, some \textit{structure} --more precisely, a seasonal behavior-- can be detected in standardized residuals. Hence, we will try to capture it through a seasonal model.

```{r, echo=F, warning=F}
# Time invariant linear growth model
build_LG <- function(param){dlm(m0 = c(y[1],0), C0 = 0.001*diag(2), FF = matrix(c(1,0), nr=1), V = exp(param[1]), GG = matrix(c(1,0,1,1), nr=2), W = diag(c(exp(param[2]), exp(param[3]))))}

# Estimate MLE of parameters
fit_LG <- dlmMLE(y, rep(0.5,3), build_LG, hessian=TRUE)

# Calculate SE of the MLE using delta method
estVarLog_LG <- solve(fit_LG$hessian)
estVar_LG <- diag(exp(fit_LG$par)) %*% estVarLog_LG %*%  diag(exp(fit_LG$par))
SE_LG <- sqrt(diag(estVar_LG))

# Alternative time-invariant models
source("LG_alternative_models.R")

# Time variant linear growth model
build_LG_v <- function(param){dlm(m0 = c(y[1],0), C0 = 0.001*diag(2), FF = matrix(c(1,0), nr=1), V = 1, GG = matrix(c(1,0,1,1), nr=2), W = diag(c(exp(param[3]), exp(param[4]))), JV = 1, X = rep(c(exp(param[1]), exp(param[2])), c(84, 102)))}

# Estimate MLE of parameters
fit_LG_v <- dlmMLE(y, rep(0.5, 4), build_LG_v, hessian=TRUE)

# Calculate SE of the MLE using delta method
estVarLog_LG_v <- solve(fit_LG_v$hessian)
estVar_LG_v <- diag(exp(fit_LG_v$par)) %*% estVarLog_LG_v %*%  diag(exp(fit_LG_v$par))
SE_LG_v <- sqrt(diag(estVar_LG_v))
```

```{r}
# MLE time-invariant model
MLE_sigmaV  <- paste0(round(exp(fit_LG$par[1]),4))
MLE_sigmaW1 <- paste0(round(exp(fit_LG$par[2]),9))
MLE_sigmaW2 <- paste0(round(exp(fit_LG$par[3]),7))

# se(MLE) time-invariant model
se_sigmaV  <- paste0('(', round(SE_LG[1],4), ')')
se_sigmaW1 <- paste0('(', round(SE_LG[2],6), ')')
se_sigmaW2 <- paste0('(', round(SE_LG[3],7), ')')

# MLE time-varynig model
MLE_sigmaV1_v  <- paste0(round(exp(fit_LG_v$par[1]),4))
MLE_sigmaV2_v  <- paste0(round(exp(fit_LG_v$par[2]),4))
MLE_sigmaW1_v  <- paste0(round(exp(fit_LG_v$par[3]),4))
MLE_sigmaW2_v  <- paste0(round(exp(fit_LG_v$par[4]),12))

# se(MLE) time-varying model
se_sigmaV1_v  <- paste0('(', round(SE_LG_v[1],4), ')')
se_sigmaV2_v  <- paste0('(', round(SE_LG_v[2],4), ')')
se_sigmaW1_v  <- paste0('(', round(SE_LG_v[3],4), ')')
se_sigmaW2_v  <- paste0('(', round(SE_LG_v[4],9), ')')

# Summary table
MLEsum_lg <- data.frame(sigmaV=c(MLE_sigmaV, se_sigmaV),
                     sigmaW1=c(MLE_sigmaW1, se_sigmaW1),
                     sigmaW2=c(MLE_sigmaW2, se_sigmaW2))
MLEsum_lgv <- data.frame(sigmaV1=c(MLE_sigmaV1_v, se_sigmaV1_v),
                     sigmaV2=c(MLE_sigmaV2_v, se_sigmaV2_v),
                     sigmaW1=c(MLE_sigmaW1_v, se_sigmaW1_v),
                     sigmaW2=c(MLE_sigmaW2_v, se_sigmaW2_v))
MLEsum <- data.frame(MLEsum_lg, MLEsum_lgv)

MLEsum_table <- knitr::kable(MLEsum, "latex", booktabs=T, align="c",
                             col.names = c("$\\hat{\\sigma}^2_{v}$",
                                           "$\\hat{\\sigma}^2_{w_1}$",
                                           "$\\hat{\\sigma}^2_{w_2}$",
                                           "$\\hat{\\sigma}^2_{v1}$",
                                           "$\\hat{\\sigma}^2_{v2}$",
                                           "$\\hat{\\sigma}^2_{w_1}$",
                                           "$\\hat{\\sigma}^2_{w_2}$"),
                             escape=F, caption="Maximum Likelihood estimates of the unknown
             parameters\\label{tab:MLE_LG}") %>%
  column_spec(1:7, width="5em") %>%
  collapse_rows(columns=1, latex_hline="none") %>%
  footnote(general="Standard errors in parentheses.",
           general_title="Note:",
           footnote_as_chunk=T, escape=F) %>%
  kable_styling(latex_options="hold_position")
add_header_above(MLEsum_table, c("Time-invariant LG" = 3, "Time-varying LG" = 4))
```

```{r}
### Time-invariant LG
# Put MLE in the model
LG <- build_LG(fit_LG$par)

# Kalman filter
LG_Filt <- dlmFilter(y, LG)

# Remove the first item (m_0) in the vector of filtered states
filt_est_LG = dropFirst(LG_Filt$m)

# Compute the vector of state variances and standard deviations
list_c_LG <- dlmSvd2var(LG_Filt$U.C, LG_Filt$D.C)
vol_C_LG <- sqrt(unlist(list_c_LG))

# One-step-ahead forecast
y_filt_LG <- LG_Filt$f

# Credible intervals of the one-step-ahead forecast
y_filt_LG_sd <- residuals(LG_Filt)$sd
y_filt_LG_lb <- y_filt_LG - 1.64 * y_filt_LG_sd
y_filt_LG_ub <- y_filt_LG + 1.64 * y_filt_LG_sd

# Signal-to-noise ratio
S_to_N <- matrix(NA,2,1)
S_to_N[1] <- exp(fit_LG$par[2]) * exp(-fit_LG$par[1])
S_to_N[2] <- exp(fit_LG$par[3]) * exp(-fit_LG$par[1])

### Time-varynig LG
# Put MLE in the model
LG_v <- build_LG_v(fit_LG_v$par)

# Kalman filter
LG_v_Filt <- dlmFilter(y, LG_v)

# Remove the first item (m_0) in the vector of filtered states
filt_est_LG_v = dropFirst(LG_v_Filt$m)

# Compute the vector of state variances and standard deviations
list_c_LG_v <- dlmSvd2var(LG_v_Filt$U.C, LG_v_Filt$D.C)
vol_C_LG_v <- sqrt(unlist(list_c_LG_v))

# One-step-ahead forecast
y_filt_LG_v <- LG_v_Filt$f

# Credible intervals of the one-step-ahead forecast
y_filt_LG_v_sd <- residuals(LG_v_Filt)$sd
y_filt_LG_v_lb <- y_filt_LG_v - 1.64 * y_filt_LG_v_sd
y_filt_LG_v_ub <- y_filt_LG_v + 1.64 * y_filt_LG_v_sd
```

```{r, fig.width=18, fig.height=3.5, out.width='100%',fig.cap="\\label{fig:innovation_process}Standardized one-step-ahead forecast errors: (a) Time-invariant LG model; (b) Time-varying LG model."}

res_LG <- residuals(LG_Filt, sd=FALSE)
res_LG <- data.frame(res_LG,louisiana_df$n_months)
colnames(res_LG) <- c("res", "time")

res_LG_v <- residuals(LG_v_Filt, sd=FALSE)
res_LG_v = data.frame(res_LG_v,louisiana_df$n_months)
colnames(res_LG_v) = c("res", "time")

# Innovation process plot
A <- ggplot(res_LG, aes(x=time)) +
  geom_line(aes(y=res), size=0.5) +
  theme_bw(base_size=9) +
  labs(x="Year", y="Residuals") +
  scale_x_continuous(breaks=seq(0,186,24),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "2 years"), "%Y")) +
  scale_y_continuous(breaks=seq(-5,5,1), limits = c(-3,4)) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0,0.5,0,0.8), "cm"))

B <- ggplot(res_LG_v, aes(x=time)) +
  geom_line(aes(y=res), size=0.5) +
  theme_bw(base_size=9) +
  labs(x="Year", y="Residuals") +
  scale_x_continuous(breaks=seq(0,186,24),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "2 years"), "%Y")) +
  scale_y_continuous(breaks=seq(-5,5,1), limits = c(-3,4)) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0,0,0,0.8), "cm"))

ggarrange(A, B, ncol=2, nrow=1, align="h",
          labels=c("(a)","(b)"), font.label=list(size=20, family="Times"))
```
\bigsqueeze \bigsqueeze

\subsection{Seasonal dynamic linear model}
\squeeze
In this section we fit a seasonal DLM given the seasonal behavior documented above. In particular, we have a seasonal factor with monthly periodicity. In fact, monthly-specific factors influence the series, such as temperature, precipitations and economic activity. Hence, we consider 12 seasonal factors $\alpha_{1},\dots,\alpha_{12}$ imposing the usual restriction $\sum_{i=1}^{12} \alpha_{i} = 0$\footnote{This linear constraint implies that there are effectively only 11 free seasonal factors free to vary.}. The model is specified in a standard way, with $W = diag(\sigma_{w}^{2}, 0, \ldots, 0)$. Importantly, we model the seasonal effect as a time-varying one letting the variance of the seasonal fatctor being different from zero such that the seasonal component is also liable to be affected by the structural break.\
Once again, we estimate the unknown variances by MLE, which are reported in Table \ref{tab:MLE_seasonal}.

```{r}
# Seasonal DLM 
build_seas <- function(param){dlmModSeas(frequency = 12, dV = exp(param[1]), dW = c(exp(param[2]),0,0,0,0,0,0,0,0,0,0))}

# Estimate MLE of parameters
fit_seas <- dlmMLE(y, rep(1,2), build_seas, hessian=TRUE)
#fit_stagional$convergence

# Calculate SE of the MLE using delta method
estVarLog_seas <- solve(fit_seas$hessian)
estVar_seas <- diag(exp(fit_seas$par)) %*% estVarLog_seas %*% diag(exp(fit_seas$par))
SE_seas <- sqrt(diag(estVar_seas))

# MLE
MLE_sigmaV  <- paste0(round(exp(fit_seas$par[1]),4))
MLE_sigmaW <- paste0(round(exp(fit_seas$par[2]),9))

# se(MLE)
se_sigmaV  <- paste0('(', round(SE_seas[1],4), ')')
se_sigmaW <- paste0('(', round(SE_seas[2],6), ')')

# Summary table
MLEsum <- data.frame(sigmaV=c(MLE_sigmaV, se_sigmaV),
                     sigmaW=c(MLE_sigmaW, se_sigmaW))

knitr::kable(MLEsum, "latex", booktabs=T, align="c",
             col.names = c("$\\hat{\\sigma}^2_{v}$",
                           "$\\hat{\\sigma}^2_{w}$"),
             escape=F, caption="Maximum Likelihood estimates of the unknown
             parameters\\label{tab:MLE_seasonal}") %>%
  column_spec(1:2, width="8em") %>%
  collapse_rows(columns=1, latex_hline="none") %>%
  footnote(general="Standard errors in parentheses.",
           general_title="Note:",
           footnote_as_chunk=T, escape=F) %>%
  kable_styling(latex_options="HOLD_position")
```

```{r, warning=F}
# Put MLE in the model + Add DLMs
Seas <- build_seas(fit_seas$par)
LG_v_seas <- LG_v + Seas

# Kalman filter
LG_v_seas_Filt <- dlmFilter(y, LG_v_seas)

# Remove the first item (m_0) in the vector of filtered states
filt_est_LG_v_seas = dropFirst(LG_v_seas_Filt$m)

# Compute the vector of state variances and standard deviations
list_c_LG_v_seas <- dlmSvd2var(LG_v_seas_Filt$U.C, LG_v_seas_Filt$D.C)
my_C <- matrix(NA, nrow=length(y)+1, ncol=13)
for (t in 1:length(y)+1) {
  my_mat <- data.frame(list_c_LG_v_seas[t])
  my_mat <- as.matrix(my_mat)
  my_C[t,] <- diag(my_mat)}
vol_C_LG_v_seas <- sqrt(my_C)

# One-step-ahead forecast
y_filt_LG_v_seas <- LG_v_seas_Filt$f

# Credible intervals of the one-step-ahead forecast
y_filt_LG_v_seas_sd <- residuals(LG_v_seas_Filt)$sd
y_filt_LG_v_seas_lb <- y_filt_LG_v_seas - 1.64 * y_filt_LG_v_seas_sd
y_filt_LG_v_seas_ub <- y_filt_LG_v_seas + 1.64 * y_filt_LG_v_seas_sd
```
\bigsqueeze

In order to evaluate the improvement that a seasonal component represents in our analysis, we combine it with the time-varying linear growth model previously specified. Figure \ref{fig:innovation_process_combined} plots the series of innovation process obtained from such a specification. Comparing this plot with the ones reported in Figure \ref{fig:innovation_process}, it is easy to see that the seasonal component is no more present in standardazed residuals. In fact, the series looks like a true noise with almost no structure.

```{r, fig.width=9, fig.height=3.5, out.width='50%', fig.cap="\\label{fig:innovation_process_combined} Standardized one-step-ahead forecast errors: Time-varying seasonal LG model."}

res_LG_v_seas <- residuals(LG_v_seas_Filt, sd=FALSE)
res_LG_v_seas = data.frame(res_LG_v_seas,louisiana_df$n_months)
colnames(res_LG_v_seas) = c("res", "time")

# Innovation process plot
ggplot(res_LG_v_seas, aes(x=time)) +
  geom_line(aes(y=res), size=0.5) +
  theme_bw(base_size=9) +
  labs(x="Year", y="Residuals") +
  scale_x_continuous(breaks=seq(0,186,24),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "2 years"), "%Y")) +
  scale_y_continuous(breaks=seq(-5,5,1), limits = c(-4,3)) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0,0,0,0.8), "cm"))
```
\bigsqueeze

In the next section we will follow a different approach to study the behavior of $\text{CO}$, i.e. we will exploit external information instead of modelling its structural components.

\squeeze

\subsection{Dynamic linear regression}
\squeeze
In light of what we said in the introduction, we may want to consider $\text{SO}_2$ as an explanatory variable for $\text{CO}$. A \textit{dynamic linear regression} model is specified as follows:
\begin{eqnarray*} 
  & y_t = \alpha_t + \beta_t x_t + v_t, & v_t \overset{iid}\sim \mathcal{N}(0, \sigma_{v}^{2}) \\
  & \theta_t = G \theta_{t-1} + w_t, & w_t \overset{iid}\sim \mathcal{N}(0, W)
\end{eqnarray*}
\smallsqueeze
where
\smallsqueeze
\begin{equation*}
  \theta_t = \begin{bmatrix} \alpha_t & \beta_t \end{bmatrix}', \quad 
  \theta_0 \sim \mathcal{N}_2(m_0, C_0), \quad
  \theta_0 \indep (v_t) \indep (w_t), \quad
  G=I_2 \quad \text{ and } \quad W=\text{diag}(\sigma_{w_1}^{2}, \sigma_{w_2}^{2})
\end{equation*}
that is, the two time-varying coefficients $\alpha_t$ and $\beta_t$ are modeled as independent random walks, with unknown system variances.
The MLEs of the unknown coefficients and the associated standard errors are presented in Table \ref{tab:MLE_dr}.

```{r}
# Define regressor
x <- louisiana_df$m_SO2_mean

# Build dynamic linear regression model
# note: initialize the algorithm using the initial level of y and the sample variance before the break
build_DR <- function(param){dlmModReg(x, dV=exp(param[1]), dW=c(exp(param[2]), exp(param[3])), m0=c(y[1],0), C0 = 0.001*diag(2))}

# Estimate MLE of parameters
fit_DR <- dlmMLE(y, c(0.5, 0.5, 0.5), build_DR, hessian=TRUE) 
#fit_DR$convergence
param_DR = unlist(build_DR(fit_DR$par)[c("V","W")])
#fit_DR$hessian 

# Calculate standard errors based on numeriacally evaluated hessian matrix
estVarLog_DR <- solve(fit_DR$hessian)
estVar_DR <- diag(exp(fit_DR$par)) %*% estVarLog_DR %*% diag(exp(fit_DR$par))
SE_DR <- sqrt(diag(estVar_DR))

# Put MLE in the model
DR <- build_DR(fit_DR$par)
```

```{r}
# MLE
MLE_sigmaV  <- paste0(round(param_DR[1],4))
MLE_sigmaW1 <- paste0(round(param_DR[2],4))
MLE_sigmaW2 <- paste0(round(param_DR[5],4))

# se(MLE)
se_sigmaV  <- paste0('(', round(SE_DR[1],4), ')')
se_sigmaW1 <- paste0('(', round(SE_DR[2],4), ')')
se_sigmaW2 <- paste0('(', round(SE_DR[3],4), ')')

# Summary table
MLEsum <- data.frame(sigmaV=c(MLE_sigmaV, se_sigmaV),
                     sigmaW1=c(MLE_sigmaW1, se_sigmaW1),
                     sigmaW2=c(MLE_sigmaW2, se_sigmaW2))

knitr::kable(MLEsum, "latex", booktabs=T, align="c",
             col.names = c("$\\hat{\\sigma}^2_{v}$",
                           "$\\hat{\\sigma}^2_{w_1}$",
                           "$\\hat{\\sigma}^2_{w_2}$"),
             escape=F, caption="Maximum Likelihood estimates of the unknown
             parameters\\label{tab:MLE_dr}") %>%
  column_spec(1:3, width="5em") %>%
  collapse_rows(columns=1, latex_hline="none") %>%
  footnote(general="Standard errors in parentheses.",
           general_title="Note:",
           footnote_as_chunk=T, escape=F) %>%
  kable_styling(latex_options="hold_position")
```
\smallsqueeze

The filtering estimates of $\theta_t=[\alpha_t \; \beta_t]'$, for $t=0,\ldots,T$, are also computed. The filtering estimates of the state variables are plotted over time together with their $90\%$ credible intervals in Figure \ref{fig:filt_est}.\
These graphs show the relevance of the break point also in the relationship between $\text{CO}$ and the covariate $\text{SO}_2$. On the one hand, the $\alpha$'s obtained from the filtering procedure, confirming the findings of the HMM section, are decreasing though becoming roughtly constant exactly around the break, namely around 2008. On the other hand, the interpretation of the filtered $\beta$'s in light of the break point is more difficult. However, we observe a mild and unstable relationship between the two variables in the pre-break period (i.e. the coefficient oscillate around zero). It becomes stronger and negative immediatly before the break, whereas becoming positive and increasing during the post-break period. Explanation of this behavior are of a technical kind, being related to the chemical relation between the two pollutants, hence we are not going to dive deeper into a causal explanation. It is enough for us to show that the model is able to track this dynamic relationship even in presence of structural instability.

```{r}
# Kalman filter
DR_Filt <- dlmFilter(y, DR)

# Remove the first item (m_0) in the vector of filtered states
filt_est_DR = dropFirst(DR_Filt$m)

# Compute the vector of state variances and standard deviations
list_c_DR <- dlmSvd2var(DR_Filt$U.C, DR_Filt$D.C)

my_C <- matrix(NA, nrow=length(y)+1, ncol = 2)

for (t in 1:length(y)+1) {
  my_mat <- data.frame(list_c_DR[t])
  my_mat <- as.matrix(my_mat)
  my_C[t,] <- diag(my_mat)}
vol_C_DR <- sqrt(my_C)

# One step ahead forecast
y_filt_DR <- DR_Filt$f

# Credible intrvals of the one-step ahead forecast
y_filt_DR_sd <- residuals(DR_Filt)$sd
y_filt_DR_lb <- y_filt_DR - 1.64 * y_filt_DR_sd
y_filt_DR_ub <- y_filt_DR + 1.64 * y_filt_DR_sd

# Estimated coefficients
filt_state_est_alpha <- filt_est_DR[,1]
filt_state_est_beta  <- filt_est_DR[,2]

filt_sd_est_alpha <- dropFirst(vol_C_DR[,1])
filt_sd_est_beta <- dropFirst(vol_C_DR[,2])

# C.I. upper and lower bounds
upper_filt_alpha <- filt_state_est_alpha + 1.64*filt_sd_est_alpha # upper bound of 90% C.I.
lower_filt_alpha <- filt_state_est_alpha - 1.64*filt_sd_est_alpha # lower bound of 90% C.I.
upper_filt_beta  <- filt_state_est_beta + 1.64*filt_sd_est_beta # upper bound of 90% C.I.
lower_filt_beta  <- filt_state_est_beta - 1.64*filt_sd_est_beta # lower bound of 90% C.I.
```

```{r, fig.width=18, fig.height=4, fig.cap="\\label{fig:filt_est} Filtering estimates with 90\\% credible intervals: (a) $\\alpha$; (b) $\\beta$."}

# Create df
df_co_ci <- data.frame(louisiana_df$n_months, filt_state_est_alpha, filt_state_est_beta, 
                       upper_filt_alpha, lower_filt_alpha, upper_filt_beta, lower_filt_beta)
colnames(df_co_ci) = c("n_months", "filt_state_est_alpha", "filt_state_est_beta",
                       "upper_filt_alpha", "lower_filt_alpha", "upper_filt_beta", "lower_filt_beta")

# Plot the filtering estimates with their credible intervals
filt_alpha <- ggplot(df_co_ci, aes(x=n_months)) +
  geom_line(aes(y=filt_state_est_alpha), size=0.4) +
  geom_line(aes(y=upper_filt_alpha), color="red", linetype="dashed", size=0.2) +
  geom_line(aes(y=lower_filt_alpha), color="red", linetype="dashed", size=0.2) +
  geom_ribbon(aes(ymin=lower_filt_alpha, ymax=upper_filt_alpha), alpha=.1, fill="red") +
  theme_bw(base_size=9) +
  labs(x="Year", y=expression(alpha), color="black") +
  scale_y_continuous() +
  scale_x_continuous(breaks=seq(0,186,24),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0,0,0,0.8), "cm"))

filt_beta <- ggplot(df_co_ci, aes(x=n_months)) +
  geom_line(aes(y=filt_state_est_beta), size=0.4) +
  geom_line(aes(y=upper_filt_beta), color="red", linetype="dashed", size=0.2) +
  geom_line(aes(y=lower_filt_beta), color="red", linetype="dashed", size=0.2) +
  geom_ribbon(aes(ymin=lower_filt_beta, ymax=upper_filt_beta), alpha=.1, fill="red") +
  theme_bw(base_size=9) +
  labs(x="Year", y=expression(beta), color="black") +
  scale_y_continuous() +
  scale_x_continuous(breaks=seq(0,186,24),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0,0,0,0.8), "cm"))

ggarrange(filt_alpha, filt_beta, ncol=2, nrow=1, labels=c("(a)","(b)"),
          font.label=list(size=20, family="Times"))
```
\bigsqueeze \bigsqueeze

\subsection{Model checking}
\squeeze
In order to check the assumptions made in our model specifications, we test in this section the properties of the innovation process. In particular, we report in Figure \ref{fig: summary} the correlogram (left panels) and the QQ-plot of the standardazed residuals (right panels).

```{r, fig.width=10, fig.height=9.2, out.width='70%', fig.cap="\\label{fig: summary}ACF of one-step-ahead forecast errors and normal probability plot of standardized one-step-ahead forecast errors: (a) Time-invariant LG model; (b) Time-varying LG model; (c) Time-varying seasonal LG model."}

source("LG_model_checking.R")
mod1 <- ggarrange(b,c, ncol=2, labels = c("(a)"),font.label=list(size=15, family="Times"))

source("LG_V_model_checking.R")
mod2 <- ggarrange(b,c, ncol=2, labels = c("(b)"),font.label=list(size=15, family="Times"))

source("LG_V_seas_mod_checking.R")
mod3 <- ggarrange(b,c, ncol=2, labels = c("(c)"),font.label=list(size=15, family="Times"))

ggarrange(mod1,mod2,mod3, ncol=1, nrow=3, align="hv")
```
\bigsqueeze

On the one side, the correlograms provide evidence of a departure from independence. Indeed, in all the three plots the autocorrelation function is significantly different from zero at some lags. This proves the presence of persistency (especially in the short run), which is not fully captured by our relatively simple models.\
On the other side, the QQ-plots clearly show a departure from normality. While residuals for the time-invariant linear growth model show the presence of fat tails in the empirical distribution, residuals of the time-varying model seem to be closer to a Gaussian distribution in terms of kurtosis. Still, fat tails are present and some positive skewness appears. Very similar results are also obtained for the seasonal model.\footnote{For sake of brevity we do not report model checking for dynamic regression. It would not be of significant added value being the results similar to the ones of the time-varying seasonal LG model. However, this exercise can be replicated using the provided R code.} These considerations suggest that a better specification should rely on leptokurtic distributions, such as the \textit{Student's t} or the \textit{Generalized Pareto Distribution}. However, we do not conduct a deeper analysis, being it beyond the purpose of the present work.
\squeeze

\subsection{Performance evaluation}
\squeeze
We treat the data under study as streaming data, periodically updated by EPA. Since our final objective is forecasting, we are going to check the out-of-sample performances of the different specifications considered so far. The one-step-ahead forecasts obtained with the models specified in the previous sections are plotted in Figure \ref{fig:one-step-forecast_est}\footnote{For sake of clarity we decided to report only the temporal window around the breakpoint of 2008.}.

```{r, fig.width=18, fig.height=7, out.width='100%', fig.cap="\\label{fig:one-step-forecast_est}Observed data (in black) and one-step-ahead forecast estimates (colored) with 90\\% credible intervals: (a) Homogeneous HMM; (b) Time-varying LG; (c) Time-varying seasonal LG; (d) Dynamic regression."}

# Build df
df_forecast <- data.frame(louisiana_df$n_months, y,
                          HMM_forecast, HMM_forecast_lb, HMM_forecast_ub,
                          y_filt_LG, y_filt_LG_lb, y_filt_LG_ub,
                          y_filt_LG_v, y_filt_LG_v_lb, y_filt_LG_v_ub,
                          y_filt_LG_v_seas, y_filt_LG_v_seas_lb, y_filt_LG_v_seas_ub,
                          y_filt_DR, y_filt_DR_lb, y_filt_DR_ub) 
colnames(df_forecast) <- c("n_months", "y",
                           "HMM_forecast", "HMM_forecast_lb", "HMM_forecast_ub",
                           "y_filt_LG", "y_filt_LG_lb", "y_filt_LG_ub",
                           "y_filt_LG_v", "y_filt_LG_v_lb", "y_filt_LG_v_ub",
                           "y_filt_LG_v_seas", "y_filt_LG_v_seas_lb", "y_filt_LG_v_seas_ub",
                           "y_filt_DR", "y_filt_DR_lb", "y_filt_DR_ub") 

# Plot
colors <- c("Observed data" = "black", "Homogeneous HMM" = "red", "Time-invar LG" = "firebrick", 
            "Time-var LG"="blue", "Time-var seas LG"="springgreen4",
            "Dynamic regr"="darkorange")

plot_HMM <- ggplot(df_forecast[61:109,], aes(x=n_months)) +
  geom_line(aes(y=y, color="Observed data")) +
  geom_line(aes(y=HMM_forecast, color = "Homogeneous HMM")) +
  geom_line(aes(y=HMM_forecast_lb), color="red", linetype="dashed", size=0.2) +
  geom_line(aes(y=HMM_forecast_ub), color="red", linetype="dashed", size=0.2) +
  geom_ribbon(aes(ymin=HMM_forecast_lb, ymax=HMM_forecast_ub), alpha=.1, fill="red") +
  geom_vline(xintercept = 85, color= "black", size=0.3) +
  theme_bw(base_size=9) +
  labs(y=expression(CO~(ppm)), color = "") +
  scale_y_continuous(breaks=seq(0,1,0.2)) +
  scale_x_continuous(breaks=seq(61,109,12),
                     labels=format(seq(as.Date("2006/1/1"),as.Date("2010/12/31"), "1 years"), "%Y")) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_blank(),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0,0.5,0,0.5), "cm"),
        legend.position="none") +
  scale_color_manual(values=colors)

plot_LG <- ggplot(df_forecast[61:109,], aes(x=n_months)) +
  geom_line(aes(y=y, color="Observed data")) +
  geom_line(aes(y=y_filt_LG, color = "Time-invar LG")) +
  geom_line(aes(y=y_filt_LG_lb), color="firebrick", linetype="dashed", size=0.2) +
  geom_line(aes(y=y_filt_LG_ub), color="firebrick", linetype="dashed", size=0.2) +
  geom_ribbon(aes(ymin=y_filt_LG_lb, ymax=y_filt_LG_ub), alpha=.1, fill="firebrick") +
  geom_vline(xintercept = 85, color= "black", size=0.3) +
  theme_bw(base_size=9) +
  labs(x="Year", y=expression(CO~(ppm)), color = "") +
  scale_y_continuous(breaks=seq(0,1,0.2)) +
  scale_x_continuous(breaks=seq(61,109,12),
                     labels=format(seq(as.Date("2006/1/1"),as.Date("2010/12/31"), "1 years"), "%Y")) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0,0,0,0), "cm"),
        legend.position="none") +
  scale_color_manual(values=colors)

plot_LG_V <- ggplot(df_forecast[61:109,], aes(x=n_months)) +
  geom_line(aes(y=y, color="Observed data")) +
  geom_line(aes(y=y_filt_LG_v, color = "Time-var LG")) +
  geom_line(aes(y=y_filt_LG_v_lb), color="blue", linetype="dashed", size=0.2) +
  geom_line(aes(y=y_filt_LG_v_ub), color="blue", linetype="dashed", size=0.2) +
  geom_ribbon(aes(ymin=y_filt_LG_v_lb, ymax=y_filt_LG_v_ub), alpha=.1, fill="blue") +
  geom_vline(xintercept = 85, color= "black", size=0.3) +
  theme_bw(base_size=9) +
  labs(y=expression(CO~(ppm)), color = "") +
  scale_y_continuous(breaks=seq(0,1,0.2)) +
  scale_x_continuous(breaks=seq(61,109,12),
                     labels=format(seq(as.Date("2006/1/1"),as.Date("2010/12/31"), "1 years"), "%Y")) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_blank(),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0,0,0,0.5), "cm"),
        legend.position="none") +
  scale_color_manual(values=colors)

plot_LG_V_seas <- ggplot(df_forecast[61:109,], aes(x=n_months)) +
  geom_line(aes(y=y, color="Observed data")) +
  geom_line(aes(y=y_filt_LG_v_seas, color = "Time-var seas LG")) +
  geom_line(aes(y=y_filt_LG_v_seas_lb), color="springgreen4", linetype="dashed", size=0.2) +
  geom_line(aes(y=y_filt_LG_v_seas_ub), color="springgreen4", linetype="dashed", size=0.2) +
  geom_ribbon(aes(ymin=y_filt_LG_v_seas_lb, ymax=y_filt_LG_v_seas_ub), alpha=.1, fill="springgreen4") +
  geom_vline(xintercept = 85, color= "black", size=0.3) +
  theme_bw(base_size=9) +
  labs(x="Year", y=expression(CO~(ppm)), color = "") +
  scale_y_continuous(breaks=seq(0,1,0.2)) +
  scale_x_continuous(breaks=seq(61,109,12),
                     labels=format(seq(as.Date("2006/1/1"),as.Date("2010/12/31"), "1 years"), "%Y")) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0,0.5,0,0.5), "cm"),
        legend.position="none") +
  scale_color_manual(values=colors)

plot_DR <- ggplot(df_forecast[61:109,], aes(x=n_months)) +
  geom_line(aes(y=y, color="Observed data")) +  
  geom_line(aes(y=y_filt_DR, color="Dynamic regr")) +
  geom_line(aes(y=y_filt_DR_lb), color="darkorange", linetype="dashed", size=0.2) +
  geom_line(aes(y=y_filt_DR_ub), color="darkorange", linetype="dashed", size=0.2) +
  geom_ribbon(aes(ymin=y_filt_DR_lb, ymax=y_filt_DR_ub), alpha=.1, fill="darkorange") +
  geom_vline(xintercept = 85, color= "black", size=0.3) +
  theme_bw(base_size=9) +
  labs(x="Year", y=expression(CO~(ppm)), color = "") +
  scale_y_continuous(breaks=seq(0,1,0.2)) +
  scale_x_continuous(breaks=seq(61,109,12),
                     labels=format(seq(as.Date("2006/1/1"),as.Date("2010/12/31"), "1 years"), "%Y")) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0,0,0,0.5), "cm"),
        legend.position="none") +
  scale_color_manual(values=colors)

ggarrange(plot_HMM, plot_LG_V, plot_LG_V_seas, plot_DR, ncol=2, nrow=2, align="hv",
          labels=c("(a)","(b)","(c)","(d)"), font.label=list(size=20, family="Times"))
```
\bigsqueeze

From these plots, we can see that all models are able to some extent to predict the structural breaks, although being able to reproduce the dynamics with different accuracy.
A further evidence that the time-varying linear growth model well describes structural instability can be found in the \textit{signal-to-noise ratio}. Indeed, it is possible to see the effect that it has on the one-step-ahead forecast: after the breakpoint the magnitude of the evolution error variance $W$ relative to the observation equation variance $\sigma_{v_t}^{2}$ increases and, as a consequence, the one-step-ahead forecasts follow more closely the data.
\squeeze

\section{Section 3. Multivariate DLMs}
\smallsqueeze
\subsection{Seemingly unrelated time series equations (SUTSE)}
\squeeze
Let us still consider the time series for $\text{CO}$ and $\text{SO}_2$. From visual inspection of Figure \ref{fig:louisiana_plots} it appears that the two series display a similar type of qualitative behavior, that, as we have previously seen, can be modeled by a time-varying linear growth DLM. To set up a multivariate model for the two series, we combine two linear growth models in a comprehensive \textit{seemingly unrelated time series equations model} (SUTSE). This is written as:
\begin{eqnarray*}
  & Y_t = (F \otimes I_2) \; \theta_t + v_t, & v_t \sim \mathcal{N}_{2}(0,V_{t}) \\
  & \theta_t = (G \otimes I_2) \; \theta_{t-1} + w_t, & w_t \sim \mathcal{N}_{4}(0,W)
\end{eqnarray*}
\smallsqueeze
with
\smallsqueeze
\begin{equation*}
  \underset{2 \times 1}{Y_t} = \begin{bmatrix} Y_{CO,t} \\ Y_{SO_2,t} \end{bmatrix}, \quad
  \underset{4 \times 1}{\theta_t} = [ \mu_{CO,t} \; \mu_{SO_2,t} \; \beta_{CO,t} \; \beta_{SO_2,t} ]', \quad
  \underset{1 \times 2}{F} = \begin{bmatrix} 1 & 0 \end{bmatrix}, \quad
  \underset{2 \times 2}{G} = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \quad
  \underset{4 \times 4}{W} = \text{blockdiag}(W_{\mu}, W_{\beta}).
\end{equation*}
We also assume $V_{t}$ to be a time-varying diagonal matrix, similarly to what we did above, considering January 2008 as change point. Moreover, in the attempt to minimize \textit{saturation} issues we reduce the number of parameters, assuming that only the level $\mu_{i,t}$ is correlated among pollutants, which seems a reasonable assumption in this context.
In Table \ref{tab:MLE_SUTSE} are presented the MLEs of the unknown coefficients and the associated standard errors. The one-step-ahead forecasts with credible intervals for both pollutants are plotted in Figure \ref{fig:SUTSE_forecast}.

```{r, include=F, warning=F}
# Build df multivariate
y_m <- data.frame(louisiana_df$m_CO_mean, louisiana_df$m_SO2_mean)
y_m <- rename(y_m, c("CO"="louisiana_df.m_CO_mean", "SO2"="louisiana_df.m_SO2_mean"))
y_m <- data.matrix(y_m)
m <- ncol(y_m)

# Build SUTSE model
build_SUTSE <- function(param){
  mod <- dlmModPoly(2)
  mod$FF <- mod$FF %x% diag(m)
  mod$GG <- mod$GG %x% diag(m)
  mod$V <- diag(c(1,1),2)
  mod$JV <- diag(c(1,2),2)
  mod$X <-  matrix(c(rep(c(exp(param[1]), exp(param[2]), exp(param[3]), exp(param[4])), c(84, 102, 84, 102))), ncol=2)
  W_mu <- matrix(c(exp(param[8]), exp(param[6]), exp(param[6]), exp(param[9])), m, m)
  W_beta <- matrix(c(exp(param[5]), 0, 0, exp(param[7])), m, m)
  mod$W <- bdiag(W_mu, W_beta)
  mod$m0 <- c(y_m[1,1], y_m[1,2], 0, 0)
  mod$C0 <- diag(4)*0.001
  return(mod)
}

# Estimate MLE of parameters
fit_SUTSE <- dlmMLE(y_m, parm=c(0.5, 0.5, 0.5, -0.5, 0.5, 0.5, 0.5, 0.5, 0.5), build_SUTSE, hessian=TRUE)
param_SUTSE <- unlist(build_SUTSE(fit_SUTSE$par)[c("V","W")])
fit_SUTSE$convergence

estVarLog_SUTSE <- solve(fit_SUTSE$hessian)
estVar_SUTSE <- diag(exp(fit_SUTSE$par)) %*% estVarLog_SUTSE %*%  diag(exp(fit_SUTSE$par))
SE_SUTSE <- sqrt(diag(estVar_SUTSE))

# Put MLE in the model
SUTSE <- build_SUTSE(fit_SUTSE$par)

# Kalman filter
SUTSE_Filt <- dlmFilter(y_m, SUTSE)

#one step ahead forecast
y_filt_SUTSE <- SUTSE_Filt$f
```

```{r, include=F, warning=F}
## Credible intrvals of the one-step ahead forecast
SUTSE_sd <- residuals(SUTSE_Filt)$sd
SUTSE_sd[1,] <- c(SUTSE_sd[2,1],SUTSE_sd[2,2]) 
SUTSE_lb_CO <- SUTSE_Filt$f[,1] - 1.64 * SUTSE_sd[,1]
SUTSE_ub_CO <- SUTSE_Filt$f[,1] + 1.64 * SUTSE_sd[,1]
SUTSE_lb_SO2 <- SUTSE_Filt$f[,2] - 1.64 * SUTSE_sd[,2]
SUTSE_ub_SO2 <- SUTSE_Filt$f[,2] + 1.64 * SUTSE_sd[,2]
```

```{r}
# MLE
MLE_sigmaV11_1  <- paste0(round(exp(fit_SUTSE$par[1]),4))
MLE_sigmaV22_1  <- paste0(round(exp(fit_SUTSE$par[3]),4))
MLE_sigmaV11_2  <- paste0(round(exp(fit_SUTSE$par[2]),4))
MLE_sigmaV22_2  <- paste0(round(exp(fit_SUTSE$par[4]),4))
MLE_sigmaWmu11  <- paste0(round(exp(fit_SUTSE$par[8]),4))
MLE_sigmaWmu12  <- paste0(round(exp(fit_SUTSE$par[6]),4))
MLE_sigmaWmu22  <- paste0(round(exp(fit_SUTSE$par[9]),4))
MLE_sigmaWbeta11  <- paste0(round(exp(fit_SUTSE$par[5]),12))
MLE_sigmaWbeta22  <- paste0(round(exp(fit_SUTSE$par[7]),9))

# se(MLE)
se_sigmaV11_1  <- paste0('(', round(SE_SUTSE[1],4), ')')
se_sigmaV22_1  <- paste0('(', round(SE_SUTSE[3],4), ')')
se_sigmaV11_2  <- paste0('(', round(SE_SUTSE[2],4), ')')
se_sigmaV22_2  <- paste0('(', round(SE_SUTSE[4],4), ')')
se_sigmaWmu11  <- paste0('(', round(SE_SUTSE[8],4), ')')
se_sigmaWmu12  <- paste0('(', round(SE_SUTSE[6],4), ')')
se_sigmaWmu22  <- paste0('(', round(SE_SUTSE[9],4), ')')
se_sigmaWbeta11  <- paste0('(', round(SE_SUTSE[5],9), ')')
se_sigmaWbeta22  <- paste0('(', round(SE_SUTSE[7],7), ')')

# Summary table
MLEsum <- data.frame(sigmaV11_1=c(MLE_sigmaV11_1, se_sigmaV11_1),
                     sigmaV22_1=c(MLE_sigmaV22_1, se_sigmaV22_1),
                     sigmaV11_2=c(MLE_sigmaV11_2, se_sigmaV11_2),
                     sigmaV22_2=c(MLE_sigmaV22_2, se_sigmaV22_2),
                     sigmaWmu11=c(MLE_sigmaWmu11, se_sigmaWmu11),
                     sigmaWmu12=c(MLE_sigmaWmu12, se_sigmaWmu12),
                     sigmaWmu22=c(MLE_sigmaWmu22, se_sigmaWmu22),
                     sigmaWbeta11=c(MLE_sigmaWbeta11, se_sigmaWbeta11),
                     sigmaWbeta22=c(MLE_sigmaWbeta22, se_sigmaWbeta22))

MLEsum_table_2 <- knitr::kable(MLEsum, "latex", booktabs=T, align="c",
             col.names = c("$\\hat{V}_{11}$",
                           "$\\hat{V}_{22}$",
                           "$\\hat{V}_{11}$",
                           "$\\hat{V}_{22}$",
                           "$\\hat{W}_{\\mu11}$",
                           "$\\hat{W}_{\\mu12}$",
                           "$\\hat{W}_{\\mu22}$",
                           "$\\hat{W}_{\\beta11}$",
                           "$\\hat{W}_{\\beta22}$"),
             escape=F, caption="Maximum Likelihood estimates of the unknown
             parameters\\label{tab:MLE_SUTSE}") %>%
  column_spec(1:9, width="4em") %>%
  collapse_rows(columns=1, latex_hline="none") %>%
  footnote(general="Standard errors in parentheses.",
           general_title="Note:",
           footnote_as_chunk=T, escape=F) %>%
  kable_styling(latex_options="hold_position")
add_header_above(MLEsum_table_2, c("Jan. 2001 to Dec. 2007"=2, "Jan. 2008 to Jun. 2016"=2, " "=5))
```

```{r, fig.width=18, fig.height=3.5, out.width='100%', fig.cap="\\label{fig:SUTSE_forecast}Observed data and one-step-ahead forecast estimates with 90\\% credible intervals: (a) CO; (b) $\\text{SO}_2$."}

df_SUTSE <- data.frame(y_m, louisiana_df$n_months,
                       y_filt_SUTSE, SUTSE_lb_CO, SUTSE_lb_SO2, SUTSE_ub_CO, SUTSE_ub_SO2)
colnames(df_SUTSE)[3] <- "n_months" 

CO_forecast <- ggplot(df_SUTSE[61:109,], aes(x=n_months)) +
  geom_line(aes(y=CO), size=0.4) +
  geom_line(aes(y=CO.1), size=0.8, color="red") +
  geom_line(aes(y=SUTSE_ub_CO), color="red", linetype="dotted", size=0.2) +
  geom_line(aes(y=SUTSE_lb_CO), color="red", linetype="dotted", size=0.2) +
  geom_ribbon(aes(ymin=SUTSE_lb_CO, ymax=SUTSE_ub_CO), alpha=.1, fill="red") +
  geom_vline(xintercept = 85, color= "black", size=0.3) +
  theme_bw(base_size=9) +
  labs(x="Year", y=expression(CO~(ppm)), color="black") +
  scale_y_continuous() +
  scale_x_continuous(breaks=seq(61,109,12),
                     labels=format(seq(as.Date("2006/1/1"),as.Date("2010/12/31"), "1 years"), "%Y")) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0,0.5,0,0.5), "cm"))

SO2_forecast <- ggplot(df_SUTSE[61:109,], aes(x=n_months)) +
  geom_line(aes(y=SO2), size=0.4) +
  geom_line(aes(y=SO2.1), size=0.8, color="red") +
  geom_line(aes(y=SUTSE_ub_SO2), color="red", linetype="dotted", size=0.2) +
  geom_line(aes(y=SUTSE_lb_SO2), color="red", linetype="dotted", size=0.2) +
  geom_ribbon(aes(ymin=SUTSE_lb_SO2, ymax=SUTSE_ub_SO2), alpha=.1, fill="red") +
  geom_vline(xintercept = 85, color= "black", size=0.3) +
  theme_bw(base_size=9) +
  labs(x="Year", y=expression(SO[2]~(ppb)), color="black") +
  scale_y_continuous() +
  scale_x_continuous(breaks=seq(61,109,12),
                     labels=format(seq(as.Date("2006/1/1"),as.Date("2010/12/31"), "1 years"), "%Y")) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0,0,0,0.5), "cm"))

ggarrange(CO_forecast, SO2_forecast, ncol=2, nrow=1, labels=c("(a)","(b)"),
          font.label=list(size=20, family="Times"))
```
\bigsqueeze \bigsqueeze

\section{Conclusion}

Finally, in Table \ref{tab:for_accuracy} we report the measures of forecasting accuracy for all the models considered. Interestingly, results show that, in this case, introducing dependence across time series of different pollutants does not lead to sizable improvements due to the so called \textit{borrowing of strenght}. Indeed, the SUTSE is the best model only according to the MAE, while the MSE and MAPE select less sophisticated specifications.
 
```{r, include=F}
source("forecasting_accuracy.R")
```

```{r, include=T}
# Forecasting accuracy: Summary table (v1)
for_accuracy <- data.frame(measure   = c("MSE",          "MAE",          "MAPE"),
                           HMM       = c(MSE_HMM,        MAE_HMM,        MAPE_HMM),
                           LG        = c(MSE_LG,         MAE_LG,         MAPE_LG),
                           #LG_2     = c(MSE_LG_2,       MAE_LG_2,       MAPE_LG_2),
                           #LG_3     = c(MSE_LG_3,       MAE_LG_3,       MAPE_LG_3),
                           LG_v      = c(MSE_LG_v,       MAE_LG_v,       MAPE_LG_v),
                           LG_v_seas = c(MSE_LG_v_seas,  MAE_LG_v_seas,  MAPE_LG_v_seas),
                           DR        = c(MSE_DR,         MAE_DR,         MAPE_DR),
                           SUTSE     = c(MSE_SUTSE,      MAE_SUTSE,      MAPE_SUTSE))

knitr::kable(for_accuracy, "latex", booktabs=T, align="c",
             col.names=c('', 'HMM', 'Time-invar LG', 'Time-var LG', 'Time-var seas LG', 'DR', 'SUTSE'),
             escape=F, caption="Measures of predictive accuracy\\label{tab:for_accuracy}") %>%
  column_spec(c(1,2,6,7), width="4em") %>%
  column_spec(c(3,4,5), width="8em") %>%
  kable_styling(latex_options="hold_position")
```

To conclude, we have presented several models that can be used to examine the series CO, pointing out for each of them advantages and disadvantages. Even if, given the complexity of the phenomenon, our specifications may be too basic to have accurate predictions, we can be satisfied with the results we obtained. we studied a debated topic and identified \textit{empirically} the consequences of that \textit{green} culture that lead only in recent years to the effective implementation of economic and regulatory interventions.
\squeeze

\section{References}
\squeeze