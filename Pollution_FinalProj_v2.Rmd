---
title: "20236 Time Series Analysis - Final project"
author:
- Simone Arrigoni (1794692)
- Luca Badolato (3086040)
- Simone Valle (3088281)
subtitle: "Bocconi University"
date: "5 June 2020"
output: pdf_document
geometry: margin=2cm
nocite: '@*'
bibliography: References.bib
header-includes:
  \usepackage[utf8]{inputenc}
  \usepackage{setspace}
  \usepackage{apacite}
  \usepackage{natbib}
  \usepackage{algpseudocode}
  \usepackage{algorithm}
  \usepackage{bm}
  \usepackage{amsmath}
  \usepackage{amssymb}
  \usepackage{graphicx}
  \usepackage{subfig}
  \usepackage{booktabs, caption}
  \usepackage{array}
  \usepackage{threeparttable}
  \usepackage{listings}
  \usepackage{physics}
  \usepackage{float}
  \floatplacement{figure}{H}
  \usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
  \definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
  \definecolor{mylilas}{RGB}{170,55,241}
  \definecolor{codegray}{RGB}{0.5,0.5,0.5}
  \definecolor{orange}{RGB}{255,165,0}
  \DeclareMathOperator*{\E}{\mathbb{E}}
  \DeclareMathOperator*{\Ec}{\mathbb{E}_t}
  \setlength\parindent{0pt}
  \newcommand{\indep}{\perp \!\!\! \perp}
---

```{r, include=F}
# Load useful packages
library(utf8)
library(labeling)
library(rmarkdown)
library(readxl)
library(httr)
library(knitr)
library(tseries)
library(tinytex)
library(scales)
library(dlm)
library(magrittr)
library(stringr)
library(depmixS4)
library(tidyverse)
library(ggthemes)
library(latex2exp)
library(kableExtra)
library(ggpubr)
library(reshape2)
library(dlm)

# Settings
knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE,
                      echo    = FALSE,
                      include = TRUE,
                      fig.pos = "H",
                      fig.align = "center",
                      out.width='80%')
```

\section{Introduction}

Pollution is one of the most debated social issues of our times. It has been attracting more and more attention given its serious implications on the health of both the Earth and human beings. The economic activity under the model of production and consumption that has been characterizing the last century is, with no doubt, at the very basis of such complex phenomenon. Only in the last fifteen years some consciousness about the importance of pollution in the long-run has evolved and serious interventions have been made. Moreover, among the numerous component which pollution can be split into, air pollution is certainly one of the most important and tightly linked to strict economic activity.

Therefore, the purpose of the present work is to model through common statistical techniques the evolution of air pollution during this last crucial fifteen years in a developed economy, i.e. the United States, trying to grasp some intuition about this fundamental phenomenon.

\section{Data}

The dataset \texttt{US\_pollution} contains monthly and daily data about air pollution for each state in the United States. It is provided by the U.S. Environmental Protection Agency (EPA). Specifically, four key indicators of air pollution are considered: 

\begin{itemize}
\item \textbf{Ground level ozone $\text{O}_{3}$} (\texttt{O3}): Bad ozone forms near the ground when pollutants (emitted by sources such as cars, power plants, industrial boilers, refineries, and chemical plants) react chemically in sunlight. Since it is more likely to form during warmer months, we expect to see a seasonal component. 
\item \textbf{Nitrogen dioxide $\text{NO}_{2}$} (\texttt{NO2}): It is produced as a result of road traffic and other fossil fuel combustion processes. Furthermore, its presence in air contributes to the formation and modification of other air pollutants, such as particle pollution.
\item \textbf{Carbon monoxide $\text{CO}$} (\texttt{CO}):  It forms from incomplete combustions mainly from vehicle exhausts (roughly 75\% of all carbon monoxide emissions nationwide and up to 95\% in cities), fuel combustion in industrial processes and natural sources. Cold temperatures make combustion less complete, therefore we expect also in this case a seasonal behavior. 
\item \textbf{Sulfur dioxide $\text{SO}_{2}$} (\texttt{SO2}): It is produced when sulfur-containing fuels are burned. It is common mainly in large industrial complexes. 
\end{itemize}
\medskip

Hence, the four variables of interest are interconnected and are the building blocks of a more general phenomenon, that is air pollution. Furthermore, they are related by a common latent process that can be summarized by several components such as industrialization, car traffic and regulation.

The preliminary analysis that led us from the raw data (with frequency of 4-intra-day observations) to the current dataset is made available in the GitHub repository at https://github.com/SimoneArrigoni/US_pollution. We relied on a moving average algorithm of order $120$ to smooth the data, assuming months of 30 days and a year of 360 days, with a minor approximation relative to the original data structure based on a actual 365 day-counts.

```{r echo=F}
# Import the dataset
louisiana_df <- read.csv(url("https://raw.githubusercontent.com/SimoneArrigoni/US_pollution/master/Louisiana_pollution.csv"), sep = ";", header=T)
```

We will be focusing on measures for \textbf{Louisiana}, recorded monthly from January 2001 to June 2016, for a total of `r length(louisiana_df$n_months)` observations. The main advantage of studying the evolution of air pollution in the same country over the years is that this allows us to control for fixed effects and to better study the latent process of interest that relates the four pollution components.
\bigskip

We report in Table \ref{tab:louisiana_corr} pollutants'correlation coefficients.

```{r}
# Correlation plot
datacor = subset(louisiana_df, select=c(m_O3_mean, m_NO2_mean, m_CO_mean, m_SO2_mean))
cor.datacor = cor(datacor, use="complete.obs")
cormat = round(cor(datacor),2)
row.names(cormat) <- c("$\\text{O}_{3}$",
                       "$\\text{NO}_{2}$",
                       "$\\text{CO}$",
                       "$\\text{SO}_{2}$")

knitr::kable(cormat, "latex", booktabs=T, align="c", escape=F,
             caption="Correlation table \\label{tab:louisiana_corr}",
             col.names=c("$\\text{O}_{3}$",
                         "$\\text{NO}_{2}$",
                         "$\\text{CO}$",
                         "$\\text{SO}_{2}$")) %>%
  column_spec(1:4, width="5em") %>%
kable_styling(latex_options="hold_position")
```

Provided that the correlation between pollutants shows significant differences in regional patterns and that $\text{CO}$ is a good indicator for industrial and biomass burning pollution [@Logan1981], a positive and strong correlation with $\text{O}_{3}$ indicates that a region has experienced photochemical $\text{O}_{3}$ production from its precursors (including $\text{CO}$) and is aligned with previous studies in the field [@Voulgarakis2011]. $\text{CO}$ is also positively correlated with $\text{SO}_2$ which is justified by the fact that they share a main source of production, i.e. coal extraction and combustion.

Other studies report that, in the US, $\text{O}_3$ is on average positively correlated with $\text{NO}_2$, which is opposite to our empirical findings about Louisiana, where they are slightly negatively correlated. A possible explanation can be given by substitution effects: an increase of nitrogen dioxide levels can be associated to the use of more fossil fuels that produce $\text{NO}_2$ instead of others. This might be related to Louisiana's fixed effects, however further research would be beyond the purpose of our study.

We plot in Figure \ref{fig:louisiana_plots} the time series for the four pollutants under study to give a first qualitative description of the phenomenon.

```{r, fig.width=20, fig.height=6, out.width='100%', fig.cap="\\label{fig:louisiana_plots}Air pollution in the state of Louisiana"}

# Time series plots
O3 <- ggplot(louisiana_df, aes(x=n_months, y=m_O3_mean)) +
  geom_line(color="steelblue", size=0.7) +
  theme_bw(base_size=9) +
  labs(y=expression(O[3]~(ppm)), x="Year") +
  scale_x_continuous(breaks=seq(0,200,25),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(size=19, family="Times", margin=margin(t=8)),
        axis.text.y=element_text(size=19, family="Times", margin=margin(r=8)),
        axis.title.x=element_text(size=21, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21, family="Times", margin=margin(r=5)))

NO2 <- ggplot(louisiana_df, aes(x=n_months, y=m_NO2_mean)) +
  geom_line(color="steelblue", size=0.7) +
  theme_bw(base_size=9) +
  labs(y=expression(NO[2]~(ppb)), x="Year") +
  scale_x_continuous(breaks=seq(0,200,25),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(size=19, family="Times", margin=margin(t=8)),
        axis.text.y=element_text(size=19, family="Times", margin=margin(r=8)),
        axis.title.x=element_text(size=21, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21, family="Times", margin=margin(r=5)))

CO <- ggplot(louisiana_df, aes(x=n_months, y=m_CO_mean)) +
  geom_line(color="steelblue", size=0.7) +
  theme_bw(base_size=9) +
  labs(y=expression(CO~(ppm)), x="Year") +
  scale_x_continuous(breaks=seq(0,200,25),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(size=19, family="Times", margin=margin(t=8)),
        axis.text.y=element_text(size=19, family="Times", margin=margin(r=8)),
        axis.title.x=element_text(size=21, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21, family="Times", margin=margin(r=5)))

SO2 <- ggplot(louisiana_df, aes(x=n_months, y=m_SO2_mean)) +
  geom_line(color="steelblue", size=0.7) +
  theme_bw(base_size=9) +
  labs(y=expression(SO[2]~(ppb)), x="Year") +
  scale_x_continuous(breaks=seq(0,200,25),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(size=19, family="Times", margin=margin(t=8)),
        axis.text.y=element_text(size=19, family="Times", margin=margin(r=8)),
        axis.title.x=element_text(size=21, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=21, family="Times", margin=margin(r=5)))

ggarrange(O3, NO2, CO, SO2,
          ncol=2, nrow=2)
```

The time series, each with its specific features, do not in general look stationary. Hence, they cannot be analized with the common models based on the assumption of covariance-stationarity, such as the well-known ARMA models.
\bigskip

\section{$\text{CO}$ series}

$\text{CO}$ is perhaps, among the pollutants, the most tightly linked with the economic and industrial network of a country. Consequently, we select this as the series of main interest for our analysis, being the purpose of this work tracking the evolution of air pollution relative to an underlying economic process.

As an exploratory step, we decompose the time series in its structural components using an additive model. First, we confirm the presence of seasonality. Second, the series is characterized by a negative trend up to a clear breakpoint between 2007 and 2008. The effect of this break seems to be twofold: first, the trend, that was negative in the pre-break period, disappears; second, the variance reduces markedly in the post-break period.
These features suggest that the process may be described by a homogeneous Hidden Markov Model with a trend component.

\section{Section 1. HMM}

After estimating a homogeneous HMM with two states specified as:
\begin{align*}
\begin{cases}
  Y_{t}=\mu_{1} + \beta_{1}t + \varepsilon_{t}, &
  \varepsilon_{t}\overset{i.i.d.}{\sim}N(0,\sigma_{1}^2) \qquad \text{if} \; S_{t}=1 \\
  Y_{t}=\mu_{2}+ + \beta_{2}t + \varepsilon_{t}, &
  \varepsilon_{t}\overset{i.i.d.}{\sim}N(0,\sigma_{2}^2) \qquad \text{if} \; S_{t}=2 \\
\end{cases}
\end{align*}
where there are three state-dependent parameters, namely the level, $\mu_i$, the slope, $\beta_i$, and the variance, $\sigma_i$, for $i=1,2$. If our guess were correct, we would expect $\mu_1 > \mu_2$, $\beta_1<0$, $\beta_2=0$ and $\sigma_1^2>\sigma_2^2$.

```{r, include=F}
source("HMM_2states.R")
```

We noticed that two states are not sufficient to capture the actual behavior of the process (results are not reported for sake of brevity). Indeed, a better fit can be obtained using a model with three states that capture periods with negative trend and high volatility, periods with no trend and low volatility and periods with no trend and high volatility. The model is specified similarly as above with an additional equation for $S=3$ and ideally $\mu_1 > \mu_2 > \mu_3$, $\beta_1<0$, $\beta_2=\beta_3=0$ and $\sigma_2^2<\sigma_3^2<\sigma_1^2$.

```{r echo=F, include=F}
# Model specification with 3 states
y <- as.numeric(louisiana_df$m_CO_mean)
nstates <- 3
set.seed(2)
HMM <- depmixS4::depmix(y ~ 1 + louisiana_df$n_months, data=data.frame(y), nstates=nstates)

# Estimation of the unknown parameters
HMM_fit <- depmixS4::fit(HMM)
```

```{r}
# MLE
mu1hat <- paste0(round(HMM_fit@response[[1]][[1]]@parameters[["coefficients"]][["(Intercept)"]],3))
mu2hat <- paste0(round(HMM_fit@response[[2]][[1]]@parameters[["coefficients"]][["(Intercept)"]],3))
mu3hat <- paste0(round(HMM_fit@response[[3]][[1]]@parameters[["coefficients"]][["(Intercept)"]],3))
beta1hat <- paste0(round(HMM_fit@response[[1]][[1]]@parameters[["coefficients"]][["louisiana_df$n_months"]],3))
beta2hat <- paste0(round(HMM_fit@response[[2]][[1]]@parameters[["coefficients"]][["louisiana_df$n_months"]],3))
beta3hat <- paste0(round(HMM_fit@response[[3]][[1]]@parameters[["coefficients"]][["louisiana_df$n_months"]],3))
sd1hat <- paste0(round(HMM_fit@response[[1]][[1]]@parameters$sd,3))
sd2hat <- paste0(round(HMM_fit@response[[2]][[1]]@parameters$sd,3))
sd3hat <- paste0(round(HMM_fit@response[[3]][[1]]@parameters$sd,3))

# se(MLE)
MLE_SE=depmixS4::standardError(HMM_fit)
SE_mu1hat <- paste0('(', round(MLE_SE$se[13],3), ')')
SE_mu2hat <- paste0('(', round(MLE_SE$se[16],3), ')')
SE_mu3hat <- paste0('(', round(MLE_SE$se[19],3), ')')
SE_beta1hat <- paste0('(', round(MLE_SE$se[14],3), ')')
SE_beta2hat <- paste0('(', round(MLE_SE$se[17],3), ')')
SE_beta3hat <- paste0('(', round(MLE_SE$se[20],3), ')')
SE_sd1hat <- paste0('(', round(MLE_SE$se[15],3), ')')
SE_sd2hat <- paste0('(', round(MLE_SE$se[18],3), ')')
SE_sd3hat <- paste0('(', round(MLE_SE$se[21],3), ')')

# Build a summary table for estimates
MLEsum <- data.frame(state=c(rep("S=1",2), rep("S=2",2), rep("S=3",2)),
                     mu=c(mu1hat, SE_mu1hat, mu2hat, SE_mu2hat,mu3hat, SE_mu3hat),
                     beta=c(beta1hat, SE_beta1hat, beta2hat, SE_beta2hat, beta3hat, SE_beta3hat),
                     sd=c(sd1hat, SE_sd1hat, sd2hat, SE_sd2hat, sd3hat, SE_sd3hat))
knitr::kable(MLEsum, "latex", booktabs=T, align="c",
             col.names=c("State",
                         "$\\hat{\\mu}_{MLE}$",
                         "$\\hat{\\beta}_{MLE}$",
                         "$\\hat{\\sigma}_{MLE}$"),
             escape=F, caption="Maximum Likelihood estimates of the state-dependent
             parameters \\label{tab:HMM_MLE}") %>%
  column_spec(1:4, width="5em") %>%
  collapse_rows(columns=1, latex_hline="none") %>%
  footnote(general="Standard errors in parentheses.",
           general_title="Note:",
           footnote_as_chunk=T, escape=F) %>%
  kable_styling(latex_options="hold_position")
```

Table \ref{tab:HMM_MLE} confirms our guess. Indeed, the slope estimate in the first state, $\hat{\beta_1}_{MLE}$, is negative and significant, whereas in the other states is null. Notice that, although apparently low, this coefficient implies a relatively high percentage change after comparing it with our data, which, roughly speaking, range in $(0,1)$. In fact, relating this estimate with a sample mean of $0.39$, we obtain an estimated monthly decline around $-0.015$ corresponding to around $18.3\%$ on an annual basis. Moreover, the model is able to capture shifts in the intecept as well as those in the variance as described before.

The series with HMM estimated state-dependent means and state-dependent standard deviations are shown in Figure \ref{fig:HMM_results} (left panel) along with the Viterbi states obtained through \textit{global decoding} and with the posterior probabilities (right panel).

```{r, include=F}
# Decoding
# Get the estimated state for each timestep 
estStates_HMM <- posterior(HMM_fit)

HMM_summary <- depmixS4::summary(HMM_fit)
```

```{r, fig.width=20, fig.height=6, out.width='100%', fig.cap="\\label{fig:HMM_results}State-dependent means and standard deviations"}

# Chart with state-dependent mean and standard deviations
HMM_main_CO <- data.frame(state=1:nstates,
                      mu=HMM_summary[,1],
                      beta=HMM_summary[,2],
                      sigma=HMM_summary[,3])
df_to_plot <- estStates_HMM %>%
  left_join(HMM_main_CO)
df_to_plot %<>%
  mutate(xtime=louisiana_df$n_months, yvalue=louisiana_df$m_CO_mean)

HMM_main <- ggplot(df_to_plot, aes(x=xtime, y=yvalue)) +
  geom_line(size=0.7) +
  geom_point(aes(x=xtime, y=mu+beta*xtime), col="blue", size=.8) +
  geom_ribbon(aes(ymin=mu+beta*xtime-2*sigma, ymax=mu+beta*xtime+2*sigma), alpha=.1) +
  theme_bw() +
  theme(plot.title=element_text(hjust=0.5)) +
  labs(y=expression(CO~(ppm)), x="Year") +
  scale_x_continuous(breaks=seq(0,200,25),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"),"2 years"), "%Y")) +
  scale_y_continuous(breaks=seq(0,1.5,0.25)) +
   theme(plot.margin=unit(c(0.5,1,0.5,0.5), "cm"),
         plot.title=element_text(size=10, margin=margin(0,0,10,0)),
         axis.text.x=element_text(size=20, family="Times", margin=margin(t=8)),
         axis.text.y=element_text(size=20, family="Times", margin=margin(r=8)),
         axis.title.x=element_text(size=25, family="Times", margin=margin(t=8)),
         axis.title.y=element_text(size=25, family="Times", margin=margin(r=8)))

# Chart with Viterbi states and posterior probabilities
HMM_additional_CO <- data.frame(time_index=louisiana_df$n_months,
                           state=estStates_HMM[1],
                           S1=estStates_HMM[2],
                           S2=estStates_HMM[3]) %>% 
  gather("variable", "value", -time_index)

my_breaks <- function(x) { if (max(x)<4) seq(0,3,1) else seq(0,1,1) }
lab_names <- c('S1'="Post. S=1",'S2'="Post. S=2",'state'="Vit. states")

HMM_additional <- ggplot(HMM_additional_CO, aes(time_index, value)) +
  geom_line(color="black", size=0.7) +
  facet_wrap(variable~., scales="free", ncol=1,
             strip.position="left",
             labeller = as_labeller(lab_names)) +
  labs(x="Year") +
  scale_y_continuous(breaks=my_breaks) +
  scale_x_continuous(breaks=seq(0,200,25),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  theme_bw() +
  theme(panel.spacing=unit(1.5, "lines"),
        axis.text.x=element_text(size=20, family="Times", margin=margin(t=8)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=8, l=8)),
        axis.title.x=element_text(size=25, family="Times", margin=margin(t=8)),
        axis.title.y=element_blank(),
        strip.text=element_text(size=17, family="Times",face="bold"),
        strip.background=element_rect(colour="black", fill=NA),
        strip.placement="outside")

# Option 1
## ggarrange(HMM_main, HMM_additional, ncol=1, widths = c(6,0.3), heights = c(1,0.8))

# Option 2
ggarrange(HMM_main, HMM_additional, widths=c(4,2))
```

Explanations of these results, which confirm a stable change in the evolution process of $\text{CO}$ in Louisiana, are several. First, the decline of carbon particles has been widely attributed to a shift from the use of coal to natural gas in US electricity production which dates back exactly to 2007. This is also supported by a marked decline in US coal mines activity which started in that same year. Second, a high and positive correlation between economic growth is well documented, thus highlighting some relationship with the outbreak of the Great Recession. Moreover, 2007 was a crucial year when negotiations on greenhouse gas emission were made at international level (such as at the G8 summit and the Wien agreement). 

A deeper analysis on the factors explaining this shift can be found in @Feng2015, in which they analyse the series of $\text{CO}_2$.

\textcolor{red}{MODEL CHECKING FOR HMM: DECIDERE L'ORIDINE. LASCIARLO QUI O PRESENTARE ENTRAMBI I MODEL CHECK PER HMM A LGM INSIEME PER CONFRONTARLI MEGLIO?}

```{r}
source("HMM_model_checking.R")
```

\section{Section 2. Univariate DLM}

\subsection{2.1. Linear growth model}

The second univariate model that we consider to examine $\text{CO}$ is a linear growth model \footnote{Even if aware of the results obtained in the previous section, we decided to specify a dlm model considering both a level and a trend, thus using the same first guess as explained in the introduction. This allows a proper comparison of the results and it works also as a double check of the presence of trend.}, defined as usual as following: 
\begin{eqnarray*} 
  & Y_t = \begin{bmatrix}1 & 0 \end{bmatrix} \theta_t + v_t \quad & v_t \overset{iid}\sim
  \mathcal{N}(0, \sigma_{V}^{2}) \\
  & \theta_t = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}\theta_{t-1} + w_t \quad & w_t
  \overset{iid}\sim \mathcal{N}_{2}(0, W = \begin{bmatrix}\sigma_{w_{1}}^{2} & 0 \\ 0 &  \sigma_{w_{1}}^{2} \end{bmatrix})
\end{eqnarray*}
where $\theta_t = \begin{pmatrix} \mu_{t} \\ \beta_{t} \end{pmatrix}$, $\theta_{0} \sim N_{2}(m_{0} = \begin{pmatrix} \hat{\mu_{0}} \\ \hat{\beta_{0}} \end{pmatrix}, C_{0})$ and $\theta_{0} \indep (v_{t}) \indep (w_{t})$. 

```{r echo=F}
# Build DLM with polynomial specification
build_LG <- function(param){dlmModPoly(order=2, dV=exp(param[1]), dW=exp(param[2:3]),m0=c(y[1],0))}

# Estimate MLE of parameters
fit_LG <- dlmMLE(y, rep(0.5,3), build_LG, hessian=TRUE)
#unlist(build_LG(fit_LG$par)[c("V","W")]) # convergence achieved

# Calculate standard errors of the MLE using delta method
estVarLog_LG <- solve(fit_LG$hessian)
estVar_LG <- diag(exp(fit_LG$par)) %*% estVarLog_LG %*% + diag(exp(fit_LG$par))
SE_LG <- sqrt(diag(estVar_LG))

# Alternative models
source("LG_alternative_models.R")

```

In Table \ref{tab:MLE} are presented the MLEs of the unknown coefficients and the associated standard errors.

```{r}
# MLE
MLE_sigmaV  <- paste0(round(exp(fit_LG$par[1]),5))
MLE_sigmaW1 <- paste0(round(exp(fit_LG$par[2]),9))
MLE_sigmaW2 <- paste0(round(exp(fit_LG$par[3]),9))

# se(MLE)
se_sigmaV  <- paste0('(', round(SE_LG[1],5), ')')
se_sigmaW1 <- paste0('(', round(SE_LG[2],9), ')')
se_sigmaW2 <- paste0('(', round(SE_LG[3],9), ')')

# Summary table
MLEsum <- data.frame(sigmaV=c(MLE_sigmaV, se_sigmaV),
                     sigmaW1=c(MLE_sigmaW1, se_sigmaW1),
                     sigmaW2=c(MLE_sigmaW2, se_sigmaW2))
knitr::kable(MLEsum, "latex", booktabs=T, align="c",
             col.names = c("$\\hat{\\sigma}^2_{v}$",
                           "$\\hat{\\sigma}^2_{w_1}$",
                           "$\\hat{\\sigma}^2_{w_2}$"),
             escape=F, caption="Maximum Likelihood estimates of the unknown
             parameters\\label{tab:MLE}") %>%
  column_spec(1:3, width="5em") %>%
  collapse_rows(columns=1, latex_hline="none") %>%
  footnote(general="Standard errors in parentheses.",
           general_title="Note:",
           footnote_as_chunk=T, escape=F) %>%
  kable_styling(latex_options="hold_position")
```

```{r}
# Put MLE in the model
LG <- build_LG(fit_LG$par)

# Kalman filter
LG_Filt <- dlmFilter(y, LG)

# Remove the first item (m_0) in the vector of filtered states
filt_est = dropFirst(LG_Filt$m)

# Compute the vector of state variances and standard deviations
list_c <- dlmSvd2var(LG_Filt$U.C, LG_Filt$D.C)
vol_C <- sqrt(unlist(list_c))

# One step ahead forecast
y_filt <- LG_Filt$f

# Signal-to-noise ratio
S_to_N <- matrix(NA,2,1)
S_to_N[1] <- exp(fit_LG$par[2]) * exp(-fit_LG$par[1])
S_to_N[2] <- exp(fit_LG$par[3]) * exp(-fit_LG$par[1])
```

\textcolor{red}{DATE OK. COMMENTO:SIGNAL-TO-NOISE RATIO MOLTO BASSO, AS A CONSEQUENCE...}


```{r}
# Structural DLM with LG model and seasonality
y_ts<-ts(y,frequency = 12)
BSM <- StructTS(y_ts,type = "BSM")
struct_fit <- BSM$fitted[,1]+BSM$fitted[,3]
```

\textcolor{red}{NEL CASO NON CI FOSSE ABBASTANZA SPAZIO POTREMMO TOGLIERE IL PLOT DEI RESIDUALS CHE DEI 3 MI SEMBRA IL MENO INFORMATIVO (LUCA: in realta il primo grafico del linear che hai plottato è molto interessante perchè si vede chiaramente che la varianza degli errori non è time invariant ma ha un bel change dal 2008. possiamo dirlo e provare a fare le prossime specificazioni con la varianza che cambia dal 2008, probabilmente questo change è catturato meglio dagli errori dell'HMM e qui si può commentare la differenza tra i due. domani provo a capire come specificare un dlm con time variant variance qui su R.)}

```{r}
source("LG_model_checking.R")
```

\subsection{2.2. Dynamic linear regression}

We take as explanatory variable $x_t$ Louisiana's coal consumption for electricity generation (in particular, for the totality of the electric power industry).

```{r}
# Import dataset
coal_cons <- read.csv(url("https://raw.githubusercontent.com/SimoneArrigoni/US_pollution/master/Louisiana_coal_consumption.csv"), sep=";", header=T, stringsAsFactors=F)
x <- as.numeric(coal_cons$tot_electric_power_industry)

# Build dynamic linear regression model
# note: initialize the algorithm using the initial level of y and the sample variance before the break
build_DR <- function(param){dlmModReg(x, addInt=TRUE, dV=exp(param[1]), dW=exp(param[2:3]), m0=c(y[1],0), C0=diag(rep(var(x[1:85])),2))}

# Estimate MLE of parameters
fit_DR <- dlmMLE(y, parm=rep(1,3), build_DR, method="BFGS", hessian=TRUE)  # convergence achieved
unlist(build_DR(fit_DR$par)[c("V","W")])

# Calculate standard errors based on numeriacally evaluated hessian matrix
estVarLog_DR <- solve(fit_DR$hessian)
estVar_DR <- diag(exp(fit_DR$par)) %*% estVarLog_DR %*% + diag(exp(fit_DR$par))
SE_DR <- sqrt(diag(estVar_DR))

# Put MLE in the model
DR <- build_DR(fit_DR$par)

# Kalman filter
DR_Filt <- dlmFilter(y, DR)

# Remove the first item (m_0) in the vector of filtered states
filt_est = dropFirst(DR_Filt$m)

# Compute the vector of state variances and standard deviations
list_c <- dlmSvd2var(DR_Filt$U.C, DR_Filt$D.C)
vol_C <- sqrt(unlist(list_c))

# One step ahead forecast
y_filt_DR <- DR_Filt$f
```



```{r, fig.width=7, fig.height=2, fig.cap="\\label{fig:filter_est} Filtering states estimates"}
# Build df
df_filt <- data.frame(louisiana_df$n_months, y, y_filt, struct_fit, y_filt_DR)
colnames(df_filt) = c("n_months", "y", "y_filt","struct_fit", "y_filt_DR")

# Plot
colors <- c("Observed data" = "black", "Linear Growth" = "orange", "Basic structural model"="blue", "Dynamic regression"="red")
ggplot(df_filt, aes(x=n_months)) +
  geom_line(aes(y=y, color="Observed data"), size=0.4) +
  geom_line(aes(y=y_filt, color = "Linear Growth"), size=0.4) +
  geom_line(aes(y=struct_fit, color="Basic structural model"), size=0.4) +
  geom_line(aes(y=y_filt_DR, color="Dynamic regression"), size=0.4) +
  #geom_vline(xintercept = as.Date("2008/1/1"), color= "black", size=0.3) +
  theme_classic(base_size=9.5) +
  theme(panel.grid.minor = element_line(size=0.5)) +
  labs(x="Year", y=expression(CO~(ppm)), color = "") +
  scale_y_continuous(breaks=seq(0,1.5,0.25)) +
  scale_x_continuous(breaks=seq(0,200,25),labels=format(seq(as.Date("2001/1/1"),as.Date("2017/6/1"), "2 years"), "%Y")) +
  theme(axis.title.x=element_text(family="Times", margin=margin(t=5)),
        axis.title.y=element_text(family="Times", margin=margin(r=5))) +
        scale_color_manual(values = colors) +
        theme(legend.justification = c("left", "top"),
              legend.box.just = "left",legend.margin = margin(2, 2, 2, 2))
```

\textcolor{red}{IL MODEL CHECKING LO LASCEREI SOLO PER HMM E LG, DIREI SOLTANTO "WE HAVE CHECKED THE RESIDUALS AND THEY..."}

```{r}
source("DR_model_checking.R")
```

\section{Section 3. Multivariate DLM}

\subsection{3.1. Seemingly unrelated regression (SUR)}

As a last step, we build a multivariate dynamic regression model. In addition to the $\text{CO}$ series, we consider now also the $\text{SO}_2$ monthly data. Assuming that the intercepts and the slopes are correlated across the two pollutants, we can define a \textit{seemingly unrelated regression model} (SUR) as follows:
\begin{align*}
  & Y_t = (F_t \otimes I_2) \theta_t + v_t, & v_t \overset{iid}\sim \mathcal{N}_2(0,V) \\
  & \theta_t = (G \otimes I_2) \theta_{t-1} + w_t, & w_t \overset{iid}\sim
  \mathcal{N}_4(0,W)
\end{align*}
with
\begin{equation*}
  \underset{2 \times 1}{Y_t} = \begin{bmatrix} Y_{CO,t} \\ Y_{SO_2,t} \end{bmatrix}, \quad
  \underset{4 \times 1}{\theta_t} = \begin{bmatrix} \\ \alpha_{CO,t} \\ \alpha_{SO_2,t}, \\ \beta_{CO,t} \\
  \beta_{SO_2,t} \end{bmatrix}, \quad
  \underset{1 \times 2}{F_t} = \begin{bmatrix} 1 & x_t \end{bmatrix}, \quad
  \underset{2 \times 2}{G} = I_2 \quad \text{ and } \quad
  \underset{4 \times 4}{W} = blockdiag(W_{\alpha}, W_{\beta}).
\end{equation*}
Note that, since both pollutants are by-products of incomplete fuel combustion, we consider $x_{i,t}=x_t$, that is we take the same explanatory variable for the two series.

```{r}
# Build df
y_m <- data.frame(louisiana_df$m_CO_mean, louisiana_df$m_SO2_mean)
y_m<-rename(y_m, c("CO"="louisiana_df.m_CO_mean", "SO2"="louisiana_df.m_SO2_mean"))
y_m<-data.matrix(y_m)

# Build SUR model
build_SUR <- function(param){
  mod <- dlmModReg(x, addInt=TRUE)
  mod$FF <- matrix(c(1,1), nrow=1, ncol=2) %x% diag(2)
  mod$JFF <- matrix(c(0,1), nrow=1, ncol=2) %x% diag(2)
  mod$GG <- diag(4)
  mod$V <- diag(c(exp(param[1]), exp(param[2])))
    Wa <- matrix(c(exp(param[3]), param[4], exp(param[5]), param[6]), nrow=2, ncol=2)
    Wb <- matrix(c(exp(param[7]), param[8], exp(param[9]), param[10]), nrow=2, ncol=2)
    W <- bdiag(Wa, Wb) 
  mod$W=W
  mod$m0 <- rep(1,2*2) # modificare
  mod$C0 <- diag(1*2) # modificare
  return(mod)
}

# Estimate MLE of parameters
fit_SUR <- dlmMLE(y_m, parm=rep(1,10), build_SUR, hessian=TRUE)
unlist(build_SUR(fit_SUR$par)[c("V","W")])

# Calculate standard errors of the MLE using delta method
estVarLog_SUR <- solve(fit_SUR$hessian) # Error: system is exactly singular
estVar_SUR <- diag(exp(fit_SUR$par)) %*% estVarLog_SUR %*% + diag(exp(fit_SUR$par))
SE_SUR <- sqrt(diag(estVar_SUR))

# Put MLE in the model
SUR <- build_SUR(fit_SUR$par)

# Kalman filter
SUR_Filt <- dlmFilter(y_m, SUR)
SUR_sd <- residuals(SUR_Filt)$sd
```

\subsection{(ADDITIONAL) Seemingly unrelated time series equations (SUTSE)}

Let us still consider the time series for $\text{CO}$ and $\text{SO}_2$. From visual inspection of Figure \ref{fig:louisiana_plots} it appears that the two series display a similar type of qualitative behavior, that can be modeled by a linear growth DLM. To set up a multivariate model for the two series, we combine the two linear growth models in a comprehensive \textit{Seemingly unrelated time series equations model} (SUTSE). This is written as:
\begin{align*}
  & Y_t = (F \otimes I_2) \theta_t + v_t, & v_t \sim \mathcal{N}_2(0,V) \\
  & \theta_t = (G \otimes I_2) \theta_{t-1} + w_t, & w_t \sim \mathcal{N}_4(0,W)
\end{align*}
with
\begin{equation*}
  \underset{2 \times 1}{Y_t} = \begin{bmatrix} Y_{CO,t} \\ Y_{SO_2,t} \end{bmatrix}, \quad
  \underset{4 \times 1}{\theta_t} = \begin{bmatrix} \\ \mu_{CO,t} \\ \mu_{SO_2,t}, \\ \beta_{CO,t} \\
  \beta_{SO_2,t} \end{bmatrix}, \quad
  \underset{1 \times 2}{F} = \begin{bmatrix} 1 & 0 \end{bmatrix}, \quad
  \underset{2 \times 2}{G} = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \quad \text{ and } \quad \underset{4
  \times 4}{W} = blockdiag(W_{\mu}, W_{\beta}).
\end{equation*}
We assume that growth $\beta_{i,t}$ is correlated among pollutants. Moreover, to simplify the model in order to reduce the overall number of parameters, we also assume that the two linear growth models are integrated random walks, that is $W_{\mu}=0$.

```{r}
# Build SUTSE model
build_SUTSE <- function(param){
  mod <- dlmModPoly(order=2)
  mod$FF <- matrix(c(1,0), nrow=1, ncol=2) %x% diag(2)
  mod$GG <- matrix(c(1,1,0,1), nrow=2, ncol=2) %x% diag(2)
  mod$V <- matrix(c(exp(param[1]), param[2], exp(param[3]), param[4]), nrow=2, ncol=2)
    Wu <- matrix(0, nrow=2, ncol=2)
    Wb <- matrix(c(exp(param[5]), param[5], exp(param[7]), param[8]), nrow=2, ncol=2)
    W <- bdiag(Wu, Wb)
  mod$W=W
  mod$m0 <- rep(1,2*2) # modificare
  mod$C0 <- diag(2*2) # modificare
  return(mod)
}

# Estimate MLE of parameters
fit_SUTSE <- dlmMLE(y_m, parm=rep(1,8), build_SUTSE, hessian=TRUE)
unlist(build_SUTSE(fit_SUTSE$par)[c("V","W")])

# Calculate standard errors of the MLE using delta method
estVarLog_SUTSE <- solve(fit_SUTSE$hessian) # Error: system is exactly singular
estVar_SUTSE <- diag(exp(fit_SUTSE$par)) %*% estVarLog_SUTSE %*% + diag(exp(fit_SUTSE$par))
SE_SUTSE <- sqrt(diag(estVar_SUTSE))

# Put MLE in the model
SUTSE <- build_SUTSE(fit_SUTSE$par)

# Kalman filter
SUTSE_Filt <- dlmFilter(y_m, SUTSE)
SUTSE_sd <- residuals(SUTSE_Filt)$sd
```

\section{Conclusion}

In conclusion, \dots

```{r, include=F}
source("forecasting_accuracy.R")
```

```{r, include=T}
# Forecasting accuracy: Summary table
tabf <- matrix(nrow=3, ncol=4)
rownames(tabf) <- c('MSE','MAE','MAPE')
colnames(tabf) <- c('HMM', 'Linear growth model', 'Dynamic regression', 'SUR', 'SUTSE')
tabf[1,1] <- MSE_LG
tabf[2,1] <- MAE_LG
tabf[3,1] <- MAPE_LG
tabf[1,2] <- MSE_DR
tabf[2,2] <- MAE_DR
tabf[3,2] <- MAPE_DR
tabf[1,3] <- MSE_SUR
tabf[2,3] <- MAE_SUR
tabf[3,3] <- MAPE_SUR
tabf[1,4] <- MSE_SUTSE
tabf[2,4] <- MAE_SUTSE
tabf[3,4] <- MAPE_SUTSE
kable(tabf, caption="Model checking")
```

\newpage
\section{References}