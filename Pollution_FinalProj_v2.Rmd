---
title: "20236 Time Series Analysis - Final project"
author:
- Simone Arrigoni (1794692)
- Luca Badolato (3086040)
- Simone Valle (3088281)
subtitle: "Bocconi University"
date: "5 June 2020"
output: pdf_document
geometry: margin=2cm
nocite: '@*'
bibliography: References.bib
header-includes:
  \usepackage[utf8]{inputenc}
  \usepackage{setspace}
  \usepackage{apacite}
  \usepackage{natbib}
  \usepackage{algpseudocode}
  \usepackage{algorithm}
  \usepackage{bm}
  \usepackage{amsmath}
  \usepackage{amssymb}
  \usepackage{graphicx}
  \usepackage{subfig}
  \usepackage{booktabs, caption}
  \usepackage{array}
  \usepackage{threeparttable}
  \usepackage{listings}
  \usepackage{physics}
  \usepackage{float}
  \floatplacement{figure}{H}
  \usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
  \definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
  \definecolor{mylilas}{RGB}{170,55,241}
  \definecolor{codegray}{RGB}{0.5,0.5,0.5}
  \definecolor{orange}{RGB}{255,165,0}
  \DeclareMathOperator*{\E}{\mathbb{E}}
  \DeclareMathOperator*{\Ec}{\mathbb{E}_t}
  \setlength\parindent{0pt}
  \newcommand{\indep}{\perp \!\!\! \perp}
---

```{r, include=F}
# Load useful packages
library(utf8)
library(labeling)
library(rmarkdown)
library(readxl)
library(httr)
library(knitr)
library(tseries)
library(tinytex)
library(scales)
library(dlm)
library(magrittr)
library(stringr)
library(depmixS4)
library(tidyverse)
library(ggthemes)
library(latex2exp)
library(kableExtra)
library(ggpubr)
library(reshape2)
library(cowplot)

# Settings
knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE,
                      echo    = FALSE,
                      include = TRUE,
                      fig.pos = "H",
                      fig.align = "center",
                      out.width='80%')
```

\section{Introduction}

Pollution is one of the most debated social issues of our times. It has been attracting more and more attention given its serious implications on the health of both the Earth and human beings. The economic activity under the model of production and consumption that has been characterizing the last century is, with no doubt, at the very basis of such complex phenomenon. Only in the last fifteen years some consciousness about the importance of pollution in the long-run has evolved and serious interventions have been made. Moreover, among the numerous component which pollution can be split into, air pollution is certainly one of the most important and tightly linked to strict economic activity.

Therefore, the purpose of the present work is to model through common statistical techniques the evolution of air pollution during this last crucial fifteen years in a developed economy, i.e. the United States, trying to grasp some intuition about this fundamental phenomenon. Hence, after some preliminary descriptive steps, we present the models we think are more appropriate to analyze such complex phenomenon. Finally, we conclude with a comparison to underline pros and cons of each specification. 

\section{Data}

The dataset \texttt{US\_pollution} contains monthly and daily data about air pollution for each state in the United States\footnote{The preliminary analysis that led us from the raw data (with frequency of 4-intra-day observations) to the current dataset is made available in the GitHub repository at \url{https://github.com/SimoneArrigoni/US_pollution}. We relied on a moving average algorithm of order $120$ to smooth the data, assuming months of 30 days and a year of 360 days, with a minor approximation relative to the original data structure based on a actual 365 day-counts.}. It is provided by the U.S. Environmental Protection Agency (EPA). Specifically, four key indicators of air pollution are considered: \textbf{Carbon monoxide ($\text{CO}$)}, \textbf{Sulfur dioxide ($\text{SO}_{2}$)},\textbf{Ground level ozone ($\text{O}_{3}$)} and \textbf{Nitrogen dioxide ($\text{NO}_{2}$)}. Particularly suitable for our analysis: 
\begin{itemize}
\item \textbf{Carbon monoxide ($\text{CO}$)}:  It forms from incomplete combustions mainly from vehicle exhausts (roughly 75\% of all carbon monoxide emissions nationwide and up to 95\% in cities), fuel combustion in industrial processes and natural sources. Cold temperatures make combustion less complete, therefore we expect also in this case a seasonal behavior. 
\item \textbf{Sulfur dioxide ($\text{SO}_{2}$)}: It is produced when sulfur-containing fuels are burned. It is common mainly in large industrial complexes. 
\end{itemize}

Very intuitively, the four variables of interest are interconnected and are the building blocks of a more general phenomenon, that is air pollution. Furthermore, they are related by a common latent process that can be summarized by several components such as industrialization, car traffic and regulation.

```{r echo=F}
# Import the dataset
louisiana_df <- read.csv(url("https://raw.githubusercontent.com/SimoneArrigoni/US_pollution/master/Louisiana_pollution.csv"), sep = ";", header=T)
```

We will be focusing on measures for \textbf{Louisiana}, recorded monthly from January 2001 to June 2016, for a total of `r length(louisiana_df$n_months)` observations. The main advantage of studying the evolution of air pollution in the same country over the years is that this allows us to control for fixed effects and to better study the latent process of interest that relates the four pollution components.

We report in Table \ref{tab:louisiana_corr} pollutants' correlation coefficients.

```{r}
# Correlation plot
datacor <- subset(louisiana_df, select=c(m_O3_mean, m_NO2_mean, m_CO_mean, m_SO2_mean))
cor.datacor <- cor(datacor, use="complete.obs")
cormat <- round(cor(datacor),2)
row.names(cormat) <- c("$\\text{O}_{3}$",
                       "$\\text{NO}_{2}$",
                       "$\\text{CO}$",
                       "$\\text{SO}_{2}$")

knitr::kable(cormat, "latex", booktabs=T, align="c", escape=F,
             caption="Correlation table \\label{tab:louisiana_corr}",
             col.names=c("$\\text{O}_{3}$",
                         "$\\text{NO}_{2}$",
                         "$\\text{CO}$",
                         "$\\text{SO}_{2}$")) %>%
  column_spec(1:4, width="5em") %>%
kable_styling(latex_options="hold_position")
```

Provided that the correlation between pollutants shows significant differences in regional patterns and that $\text{CO}$ is a good indicator for industrial and biomass burning pollution [@Logan1981], a positive and strong correlation with $\text{O}_{3}$ indicates that a region has experienced photochemical $\text{O}_{3}$ production from its precursors (including $\text{CO}$) and is aligned with previous studies in the field [@Voulgarakis2011]. $\text{CO}$ is also positively correlated with $\text{SO}_2$ which is justified by the fact that they share a main source of production, i.e. coal extraction and combustion\footnote{Other studies report that, in the US, $\text{O}_3$ is on average positively correlated with $\text{NO}_2$, which is opposite to our empirical findings about Louisiana, where they are slightly negatively correlated. A possible explanation can be given by substitution effects: an increase of nitrogen dioxide levels can be associated to the use of more fossil fuels that produce $\text{NO}_2$ instead of others. This might be related to Louisiana's fixed effects, however further research would be beyond the purpose of our study.}.

We plot in Figure \ref{fig:louisiana_plots} the time series for the four pollutants under study to give a first qualitative description of the phenomenon.

```{r, fig.width=20, fig.height=6, out.width='100%', fig.cap="\\label{fig:louisiana_plots}Air pollution in the state of Louisiana"}

# Time series plots
O3 <- ggplot(louisiana_df, aes(x=n_months, y=m_O3_mean)) +
  geom_line(color="black", size=0.7) +
  theme_bw(base_size=9) +
  labs(y=expression(O[3]~(ppm)), x="Year") +
  scale_x_continuous(breaks=seq(0,186,24),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0,0.5,0.5,0), "cm"))

NO2 <- ggplot(louisiana_df, aes(x=n_months, y=m_NO2_mean)) +
  geom_line(color="black", size=0.7) +
  theme_bw(base_size=9) +
  labs(y=expression(NO[2]~(ppb)), x="Year") +
  scale_x_continuous(breaks=seq(0,186,24),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0,0,0.5,0.5), "cm"))

CO <- ggplot(louisiana_df, aes(x=n_months, y=m_CO_mean)) +
  geom_line(color="black", size=0.7) +
  theme_bw(base_size=9) +
  labs(y=expression(CO~(ppm)), x="Year") +
  scale_x_continuous(breaks=seq(0,186,24),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0.5,0.5,0,0), "cm"))

SO2 <- ggplot(louisiana_df, aes(x=n_months, y=m_SO2_mean)) +
  geom_line(color="black", size=0.7) +
  theme_bw(base_size=9) +
  labs(y=expression(SO[2]~(ppb)), x="Year") +
  scale_x_continuous(breaks=seq(0,186,24),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),        
        plot.margin=unit(c(0.5,0,0,0.5), "cm"))

ggarrange(O3, NO2, CO, SO2, ncol=2, nrow=2, align="v")
```

The time series, each with its specific features, do not in general look stationary. Hence, they cannot be analized with the common models based on the assumption of covariance-stationarity, such as the well-known ARMA models.

\section{$\text{CO}$ series}

$\text{CO}$ is perhaps, among the pollutants, the most tightly linked with the economic and industrial network of a country. Consequently, we select this as the series of main interest for our analysis, being the purpose of this work tracking the evolution of air pollution relative to an underlying economic process.

As an exploratory step, we decompose the time series in its structural components using an additive model\footnote{For sake of brevity we do not report the plots of the decomposed components.}. The decomposition confirms the presence of a quarterly seasonal component and shows a negative trend up to a clear breakpoint between 2007 and 2009. The effect of this break seems to be twofold: first, the trend, that was negative in the pre-break period, disappears; second, the variance reduces markedly in the post-break period.

\section{Section 1. HMM}

These features suggest that the process may be described by a homogeneous \textit{hidden Markov model} (HMM) with a trend component. Therefore, we estimate a homogeneous HMM with two states, specified as:
\begin{align*}
\begin{cases}
  Y_{t}=\mu_{1} + \beta_{1}t + \varepsilon_{t}, &
  \varepsilon_{t}\overset{iid}{\sim}N(0,\sigma_{1}^2) \qquad \text{if} \; S_{t}=1 \\
  Y_{t}=\mu_{2} + \beta_{2}t + \varepsilon_{t}, &
  \varepsilon_{t}\overset{iid}{\sim}N(0,\sigma_{2}^2) \qquad \text{if} \; S_{t}=2 \\
\end{cases}
\end{align*}
where there are three state-dependent parameters, namely the level, $\mu_i$, the slope, $\beta_i$, and the variance, $\sigma_i$, for $i=1,2$. If our guess were correct, we would expect $\mu_1 > \mu_2$, $\beta_1<0$, $\beta_2=0$ and $\sigma_1^2>\sigma_2^2$.

```{r, include=F}
source("HMM_2states.R")
```

However, we noticed that two states are not sufficient to capture the actual behavior of the process\footnote{Again, results are not reported here but can be easily replicated using the provided R code.}. Indeed, a better fit can be obtained using a model with three hidden states that capture periods with negative trend and high volatility, periods with no trend and low volatility and periods with no trend and high volatility. The model is specified as above with an additional equation for $S=3$ and ideally $\mu_1 > \mu_2 > \mu_3$, $\beta_1<0$, $\beta_2=\beta_3=0$ and $\sigma_2^2<\sigma_3^2<\sigma_1^2$.

```{r echo=F, include=F}
# Model specification with 3 states
y <- as.numeric(louisiana_df$m_CO_mean)
nstates <- 3
set.seed(2)
HMM <- depmixS4::depmix(y ~ 1 + louisiana_df$n_months, data=data.frame(y), nstates=nstates)

# Estimation of the unknown parameters
HMM_fit <- depmixS4::fit(HMM)
```

```{r}
# MLE
mu1hat <- paste0(round(HMM_fit@response[[1]][[1]]@parameters[["coefficients"]][["(Intercept)"]],3))
mu2hat <- paste0(round(HMM_fit@response[[2]][[1]]@parameters[["coefficients"]][["(Intercept)"]],3))
mu3hat <- paste0(round(HMM_fit@response[[3]][[1]]@parameters[["coefficients"]][["(Intercept)"]],3))
beta1hat <- paste0(round(HMM_fit@response[[1]][[1]]@parameters[["coefficients"]][["louisiana_df$n_months"]],3))
beta2hat <- paste0(round(HMM_fit@response[[2]][[1]]@parameters[["coefficients"]][["louisiana_df$n_months"]],3))
beta3hat <- paste0(round(HMM_fit@response[[3]][[1]]@parameters[["coefficients"]][["louisiana_df$n_months"]],3))
sd1hat <- paste0(round(HMM_fit@response[[1]][[1]]@parameters$sd,3))
sd2hat <- paste0(round(HMM_fit@response[[2]][[1]]@parameters$sd,3))
sd3hat <- paste0(round(HMM_fit@response[[3]][[1]]@parameters$sd,3))

# se(MLE)
MLE_SE=depmixS4::standardError(HMM_fit)
SE_mu1hat <- paste0('(', round(MLE_SE$se[13],3), ')')
SE_mu2hat <- paste0('(', round(MLE_SE$se[16],3), ')')
SE_mu3hat <- paste0('(', round(MLE_SE$se[19],3), ')')
SE_beta1hat <- paste0('(', round(MLE_SE$se[14],3), ')')
SE_beta2hat <- paste0('(', round(MLE_SE$se[17],3), ')')
SE_beta3hat <- paste0('(', round(MLE_SE$se[20],3), ')')
SE_sd1hat <- paste0('(', round(MLE_SE$se[15],3), ')')
SE_sd2hat <- paste0('(', round(MLE_SE$se[18],3), ')')
SE_sd3hat <- paste0('(', round(MLE_SE$se[21],3), ')')

# Build a summary table for estimates
MLEsum <- data.frame(state=c(rep("S=1",2), rep("S=2",2), rep("S=3",2)),
                     mu=c(mu1hat, SE_mu1hat, mu2hat, SE_mu2hat,mu3hat, SE_mu3hat),
                     beta=c(beta1hat, SE_beta1hat, beta2hat, SE_beta2hat, beta3hat, SE_beta3hat),
                     sd=c(sd1hat, SE_sd1hat, sd2hat, SE_sd2hat, sd3hat, SE_sd3hat))

knitr::kable(MLEsum, "latex", booktabs=T, align="c",
             col.names=c("State",
                         "$\\hat{\\mu}_{MLE}$",
                         "$\\hat{\\beta}_{MLE}$",
                         "$\\hat{\\sigma}_{MLE}$"),
             escape=F, caption="Maximum Likelihood estimates of the state-dependent
             parameters \\label{tab:HMM_MLE}") %>%
  column_spec(1:4, width="5em") %>%
  collapse_rows(columns=1, latex_hline="none") %>%
  footnote(general="Standard errors in parentheses.",
           general_title="Note:",
           footnote_as_chunk=T, escape=F) %>%
  kable_styling(latex_options="hold_position")
```

Table \ref{tab:HMM_MLE} confirms our guess. Indeed, the slope estimate in the first state, $\hat{\beta_1}_{MLE}$, is negative and significant, whereas in the other states is null. Notice that, although apparently low, this coefficient implies a relatively high percentage change after comparing it with our data, which, roughly speaking, range in $(0,1)$. In fact, relating this estimate with a sample mean of $0.39$, we obtain an estimated monthly decline around $-0.015$ corresponding to around $18.3\%$ on an annual basis. Moreover, the model is able to capture shifts in the intecept as well as those in the variance as described before.

The series with HMM estimated state-dependent means and state-dependent standard deviations are shown in Figure \ref{fig:HMM_results} (left panel) along with the Viterbi states obtained through \textit{global decoding} and with the posterior probabilities (right panel).

```{r, include=F}
# Decoding
# Get the estimated state for each timestep 
estStates_HMM <- posterior(HMM_fit)
HMM_summary <- depmixS4::summary(HMM_fit)
```

```{r, fig.width=20, fig.height=6, out.width='100%', fig.cap="\\label{fig:HMM_results}State-dependent means and standard deviations"}
# Chart with state-dependent mean and standard deviations
HMM_main_CO <- data.frame(state=1:nstates,
                      mu=HMM_summary[,1],
                      beta=HMM_summary[,2],
                      sigma=HMM_summary[,3])
df_to_plot <- estStates_HMM %>%
  left_join(HMM_main_CO)
df_to_plot %<>%
  mutate(xtime=louisiana_df$n_months, yvalue=louisiana_df$m_CO_mean)

HMM_main <- ggplot(df_to_plot, aes(x=xtime, y=yvalue)) +
  geom_line(size=0.7) +
  geom_point(aes(x=xtime, y=mu+beta*xtime), col="blue", size=.8) +
  geom_ribbon(aes(ymin=mu+beta*xtime-2*sigma, ymax=mu+beta*xtime+2*sigma), alpha=.1) +
  theme_bw() +
  labs(y=expression(CO~(ppm)), x="Year") +
  scale_x_continuous(breaks=seq(0,186,24),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "2 years"), "%Y")) +
  scale_y_continuous(breaks=seq(0,1.5,0.25)) +
   theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=8)),
         axis.text.y=element_text(size=20, family="Times", margin=margin(r=8)),
         axis.title.x=element_text(size=20, family="Times", margin=margin(t=8)),
         axis.title.y=element_text(size=20, family="Times", margin=margin(r=8)),
         panel.grid.major=element_line(size=0.5),
         panel.grid.minor=element_blank(),
         plot.margin=unit(c(0.5,1,0.5,0.5), "cm"))

# Chart with Viterbi states and posterior probabilities
HMM_additional_CO <- data.frame(time_index=louisiana_df$n_months,
                           state=estStates_HMM[1],
                           S1=estStates_HMM[2],
                           S2=estStates_HMM[3]) %>% 
  gather("variable", "value", -time_index)

my_breaks <- function(x) { if (max(x)<4) seq(0,3,1) else seq(0,1,1) }
lab_names <- c('S1'="Post. S=1",'S2'="Post. S=2",'state'="Vit. states")

HMM_additional <- ggplot(HMM_additional_CO, aes(time_index, value)) +
  geom_line(color="black", size=0.7) +
  facet_wrap(variable~., scales="free_y", ncol=1,
             strip.position="left",
             labeller = as_labeller(lab_names)) +
  labs(x="Year") +
  scale_y_continuous(breaks=my_breaks) +
  scale_x_continuous(breaks=seq(0,186,36),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "3 years"), "%Y")) +
  theme_bw() +
  theme(panel.spacing=unit(1.5, "lines"),
        axis.text.x=element_text(size=20, family="Times", margin=margin(t=8)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=8, l=8)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=8)),
        axis.title.y=element_blank(),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        strip.text=element_text(size=20, family="Times", face="bold"),
        strip.background=element_rect(colour="black", fill=NA),
        strip.placement="outside")

# Option 1
## ggarrange(HMM_main, HMM_additional, ncol=1, widths = c(6,0.3), heights = c(1,0.8))

# Option 2
ggarrange(HMM_main, HMM_additional, widths=c(4,2), align="h")
```

Explanations of these results, which confirm a stable change in the evolution process of $\text{CO}$ in Louisiana, are several. First, the decline of carbon particles has been widely attributed to a shift from the use of coal to natural gas in U.S. electricity production which dates back exactly to 2007. This is also supported by a marked decline in US coal mines activity which started in that same year. Second, a high and positive correlation between economic growth is well documented, thus highlighting some relationship with the outbreak of the Great Recession. Moreover, 2007 was a crucial year when negotiations on greenhouse gas emission were made at international level (such as at the G8 summit and the Wien agreement)\footnote{A deeper analysis on the factors explaining this shift can be found in Feng et al. (2015), in which they analyse the series of $\text{CO}_2$.}. 

\section{Section 2. Univariate DLMs}

We now try to model the series of interest through \textit{dynamic linear model} (DLM) specifications. Indeed, DLMs are very attractive, since they allow a natural interpretation of the process as the combination of several components, such as trend, seasonal or regressive ones. As a consequence, we consider different specifications which, simply combined, are able to explain all the features previously described\footnote{Therefore we are aware (and we expect) that by construction each model will miss some process characteristics. For example, residuals checking for a linear growth model may shows a seasonal behaviour but this is fully in conformity with the additive structure we consider.}. 

\subsection{2.1. Linear growth model}

The first DLM specification we consider to examine $\text{CO}$ is a time invariant linear growth model, which is suitable to capture both a latent level, $\mu_{t}$, and a time varying splope in its dynamics, $\beta_{t}$. As usual, it is defined as following: 
\begin{eqnarray*}
  & Y_t = F \theta_t + v_t, \quad & v_t \overset{iid}\sim \mathcal{N}(0, \sigma_v^2) \\
  & \theta_t =  G \theta_{t-1} + w_t, \quad & w_t \overset{iid}\sim \mathcal{N}_{2}(0, W)
\end{eqnarray*}
where
\begin{equation*}
  \theta_t = \begin{pmatrix} \mu_{t} \\ \beta_{t} \end{pmatrix}, \quad
  F = \begin{bmatrix} 1 & 0 \end{bmatrix}, \quad
  G = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}, \quad
  W = \begin{bmatrix} \sigma_{w_1}^{2} & 0 \\ 0 & \sigma_{w_2}^{2} \end{bmatrix}, \quad
  \theta_0 \sim \mathcal{N}_{2}(m_0, C_0) \quad \text{ and } \quad \theta_{0} \indep (v_t) \indep (w_t).
\end{equation*}
Particularly, we decided to impose as starting condition $m_0 = \begin{pmatrix} y_{1} \\ 0 \end{pmatrix}$ and $C_0 = \begin{bmatrix} 0.001 & 0 \\ 0 & 0.001 \end{bmatrix}$, a relatively large variance to take into account our uncertainty. 

```{r echo=F, warning=F}
# Time invariant linear growth model
build_LG <- function(param){dlm(m0 = c(y[1],0), C0 = 0.001*diag(2), FF = matrix(c(1,0), nr=1), V = exp(param[1]), GG = matrix(c(1,0,1,1), nr=2), W = diag(c(exp(param[2]), exp(param[3]))))}

# Estimate MLE of parameters
fit_LG <- dlmMLE(y, rep(0.5,3), build_LG, hessian=TRUE)

# Calculate SE of the MLE using delta method
estVarLog_LG <- solve(fit_LG$hessian)
estVar_LG <- diag(exp(fit_LG$par)) %*% estVarLog_LG %*% + diag(exp(fit_LG$par))
SE_LG <- sqrt(diag(estVar_LG))

# Alternative models
source("LG_alternative_models.R")
```

In Table \ref{tab:MLE} are presented the MLEs of the unknown coefficients and the associated standard errors\footnote{We treat $\psi = (\sigma_{v}^{2}, \; \sigma_{w_1}^{2}, \;\sigma_{w_2}^{2})$ as unknown parameters and we estimate them by maximum likelihood. Then, we perform the analysis of interest plugging them in the model as if they were known costants. We follow this customary way to proceed also in the following sections.}, together with the ones of the time-variant model specified below.

```{r}
# MLE
MLE_sigmaV  <- paste0(round(exp(fit_LG$par[1]),4))
MLE_sigmaW1 <- paste0(round(exp(fit_LG$par[2]),9))
MLE_sigmaW2 <- paste0(round(exp(fit_LG$par[3]),7))

# se(MLE)
se_sigmaV  <- paste0('(', round(SE_LG[1],4), ')')
se_sigmaW1 <- paste0('(', round(SE_LG[2],6), ')')
se_sigmaW2 <- paste0('(', round(SE_LG[3],7), ')')

# Summary table
MLEsum_lg <- data.frame(sigmaV=c(MLE_sigmaV, se_sigmaV),
                     sigmaW1=c(MLE_sigmaW1, se_sigmaW1),
                     sigmaW2=c(MLE_sigmaW2, se_sigmaW2))
```

```{r}
# Put MLE in the model
LG <- build_LG(fit_LG$par)

# Kalman filter
LG_Filt <- dlmFilter(y, LG)

# Remove the first item (m_0) in the vector of filtered states
filt_est_LG = dropFirst(LG_Filt$m)

# Compute the vector of state variances and standard deviations
list_c_LG <- dlmSvd2var(LG_Filt$U.C, LG_Filt$D.C)
vol_C_LG <- sqrt(unlist(list_c_LG))

# One step ahead forecast
y_filt_LG <- LG_Filt$f

# Signal-to-noise ratio
S_to_N <- matrix(NA,2,1)
S_to_N[1] <- exp(fit_LG$par[2]) * exp(-fit_LG$par[1])
S_to_N[2] <- exp(fit_LG$par[3]) * exp(-fit_LG$par[1])
```

As expected, a first graphical inspection of the standardized one-step-ahead forecast error series (reported in Figure \ref{Fig:innovation process plot}(a)) shows both a seasonal behavior and a clear change in variance around 2008. We can take into account this second evidence allowing $\sigma_{v}^{2}$ to be time dependent.

For this purpose, we estimated a linear growth model specified as above but considering  $\sigma_{vt}^{2} = \sigma_{v1}^{2}$ for observations before 2008 and $\sigma_{vt}^{2} = \sigma_{v2}^{2}$ for observations after 2008.

```{r echo=F, warning=F}
# Time variant linear growth model
build_LG_v <- function(param){dlm(m0 = c(y[1],0), C0 = 0.001*diag(2), FF = matrix(c(1,0), nr=1), V = 1, GG = matrix(c(1,0,1,1), nr=2), W = diag(c(exp(param[3]), exp(param[4]))), JV = 1, X = rep(c(exp(param[1]), exp(param[2])), c(84, 102)))}

# Estimate MLE of parameters
fit_LG_v <- dlmMLE(y, rep(0.5, 4), build_LG_v, hessian=TRUE)

# Calculate SE of the MLE using delta method
estVarLog_LG_v <- solve(fit_LG_v$hessian)
estVar_LG_v <- diag(exp(fit_LG_v$par)) %*% estVarLog_LG_v %*% + diag(exp(fit_LG_v$par))
SE_LG_v <- sqrt(diag(estVar_LG_v))
```

```{r}
# MLE
MLE_sigmaV1_v  <- paste0(round(exp(fit_LG_v$par[1]),4))
MLE_sigmaV2_v  <- paste0(round(exp(fit_LG_v$par[2]),4))
MLE_sigmaW1_v  <- paste0(round(exp(fit_LG_v$par[3]),4))
MLE_sigmaW2_v  <- paste0(round(exp(fit_LG_v$par[4]),12))

# se(MLE)
se_sigmaV1_v  <- paste0('(', round(SE_LG_v[1],4), ')')
se_sigmaV2_v  <- paste0('(', round(SE_LG_v[2],4), ')')
se_sigmaW1_v  <- paste0('(', round(SE_LG_v[3],4), ')')
se_sigmaW2_v  <- paste0('(', round(SE_LG_v[4],9), ')')

# Summary table
MLEsum_lgv <- data.frame(sigmaV1=c(MLE_sigmaV1_v, se_sigmaV1_v),
                     sigmaV2=c(MLE_sigmaV2_v, se_sigmaV2_v),
                     sigmaW1=c(MLE_sigmaW1_v, se_sigmaW1_v),
                     sigmaW2=c(MLE_sigmaW2_v, se_sigmaW2_v))

MLEsum <- data.frame(MLEsum_lg, MLEsum_lgv)
knitr::kable(MLEsum, "latex", booktabs=T, align="c",
             col.names = c("$\\hat{\\sigma}^2_{v}$",
                           "$\\hat{\\sigma}^2_{w_1}$",
                           "$\\hat{\\sigma}^2_{w_2}$",
                           "$\\hat{\\sigma}^2_{v1}$",
                           "$\\hat{\\sigma}^2_{v2}$",
                           "$\\hat{\\sigma}^2_{w_1}$",
                           "$\\hat{\\sigma}^2_{w_2}$"),
             escape=F, caption="Maximum Likelihood estimates of the unknown
             parameters\\label{tab:MLE}") %>%
  column_spec(1:7, width="5em") %>%
  collapse_rows(columns=1, latex_hline="none") %>%
  footnote(general="Standard errors in parentheses.",
           general_title="Note:",
           footnote_as_chunk=T, escape=F) %>%
  kable_styling(latex_options="hold_position") #%>%
#add_header_above(c("Group 1" = 3, "Group 2" = 4)) 
```

```{r}
# Put MLE in the model
LG_v <- build_LG_v(fit_LG_v$par)

# Kalman filter
LG_v_Filt <- dlmFilter(y, LG_v)

# Remove the first item (m_0) in the vector of filtered states
filt_est_LG_v = dropFirst(LG_v_Filt$m)

# Compute the vector of state variances and standard deviations
list_c_LG_v <- dlmSvd2var(LG_v_Filt$U.C, LG_v_Filt$D.C)
vol_C_LG_v <- sqrt(unlist(list_c_LG_v))

# One step ahead forecast
y_filt_LG_v <- LG_v_Filt$f
```

In Figure \ref{Fig:innovation process plot} we compare the standardized innovation process of the two specifications.The second specification reports an improvement, as the innovation process does not report anymore a clear change in variance and looks more close to a random walk\footnote{Note that there is still a not captured seasonal behaviour and some other noises that we will try to capture through a regression component}. 

```{r, fig.width=20, fig.height=3.5, out.width='100%', fig.cap="\\label{Fig:innovation process plot}Standardized one-step-ahead forecast errors. (a) Time invariant model; (b) Time variant model."}

res_LG <- residuals(LG_Filt, sd=FALSE)
res_LG <- data.frame(res_LG,louisiana_df$n_months)
colnames(res_LG) <- c("res", "time")

res_LG_v <- residuals(LG_v_Filt, sd=FALSE)
res_LG_v = data.frame(res_LG_v,louisiana_df$n_months)
colnames(res_LG_v) = c("res", "time")

# Innovation process plot
A <- ggplot(res_LG, aes(x=time)) +
  geom_line(aes(y=res), size=0.5) +
  theme_bw(base_size=9) +
  labs(x="Year", y="Residuals") +
  scale_x_continuous(breaks=seq(0,186,24),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "2 years"), "%Y")) +
  scale_y_continuous(breaks=seq(-5,5,1), limits = c(-3,4)) +
  theme(axis.text.x=element_text(size=18, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=18, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=18, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=18, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0,0.5,0,0.8), "cm"))

B <- ggplot(res_LG_v, aes(x=time)) +
  geom_line(aes(y=res), size=0.5) +
  theme_bw(base_size=9) +
  labs(x="Year", y="Residuals") +
  scale_x_continuous(breaks=seq(0,186,24),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "2 years"), "%Y")) +
  scale_y_continuous(breaks=seq(-5,5,1), limits = c(-3,4)) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0,0,0,0.8), "cm"))

ggarrange(A, B, ncol=2, nrow=1, labels=c("(a)", "(b)"),
          font.label=list(size=20, family="Times"))
```

Finally, once the model is specified, it is possible to do perform the analysis of interest, such as filtering and k-step ahead forecasting. Furthermore, it is possible to check the model assumptions testing the properties of the innovation process e.g. through a correlogram and a qq-plot. We report in Figure \ref{Fig: summary} a summary related to the time-invariant linear growth model. 

```{r, fig.width=20, fig.height=3.5, out.width='100%', fig.cap="\\label{Fig: summary}Time-invariant linear growth model: (a) 10-step-ahead forecast; (b) ACF of one-step-ahead forecast errors; (c) Normal probability plot of standardized one-step-ahead forecast errors."}
#10-STEP AHEAD FORECAST
fore <- dlmForecast(LG_Filt, nAhead=10, sampleNew=3)

f_values <- matrix(NA, nrow=40, ncol=1)
pl <- matrix(NA, nrow=40, ncol=1)
pu <- matrix(NA, nrow=40, ncol=1)
obs <- matrix(NA, nrow=40, ncol=1)
filt <- matrix(NA, nrow=40, ncol=1)
f_values[31:40] <- fore$f #Forecasts
sqrtQ <- sapply(fore$Q, function(x) sqrt(x[1,1]))
pl[31:40] <- fore$f[,1] + qnorm(0.05, sd = sqrtQ)
pu[31:40] <- fore$f[,1] + qnorm(0.95, sd = sqrtQ)
obs[1:30] <- y[157:186]

new_sample <- matrix(NA, nrow=40, ncol=3)
for (t in 1:3){
  new_sample[31:40,t] <- fore$newObs[[t]]  
}

df_forecast <- data.frame(obs, f_values, pl, pu, new_sample)

# Plot
colors <- c("Observed data" = "black", "Forecasts" = "red", "Forecast sample"="steelblue")
f <- ggplot(df_forecast, aes(x=c(1:40))) +
  geom_line(aes(y=obs, color="Observed data"), size=0.4) +
  geom_line(aes(y=f_values, color = "Forecasts"), size=0.4) +
  geom_line(aes(y=pl, color = "Forecasts"), linetype ="dashed", size=0.4) +
  geom_line(aes(y=pu, color = "Forecasts"),  linetype ="dashed", size=0.4) +
  geom_line(aes(y=new_sample[,1], color = "Forecast sample"), size=0.3) +
  geom_line(aes(y=new_sample[,2], color = "Forecast sample"), size=0.3) +
  geom_line(aes(y=new_sample[,3], color = "Forecast sample"), size=0.3) +
  theme_bw(base_size=9) +
  labs(x="Year", y=expression(CO~(ppm)), color = "") +
  scale_y_continuous(breaks=seq(0,0.5,0.2)) +
  scale_x_continuous(breaks=seq(1,60,12),
                     labels=format(seq(as.Date("2014/1/1"),as.Date("2018/12/31"), "1 years"),"%Y")) +
  theme(axis.text.x=element_text(size=19, family="Times", margin=margin(t=2)),
        axis.text.y=element_text(family="Times", margin=margin(r=2)),
        axis.title.x=element_text(size=19, family="Times", margin=margin(t=2)),
        axis.title.y=element_text(size=19, family="Times", margin=margin(r=2)),
        panel.grid.major=element_line(size=0.02),
        panel.grid.minor=element_blank(),) +
  scale_color_manual(values=colors) +
  theme(legend.text=element_text(size=12),legend.position = c(0.05, 0.99), legend.justification = c("left", "top"),
        legend.box.just = "left",legend.margin = margin(2, 2, 2, 2))

source("LG_model_checking.R")

summary <- ggarrange(f, b, c, ncol = 3, labels = c("(a)","(b)", "(c)"))
summary                       

```
Concerning figure (a), as expected the forecast function is linear (particularly, we recall $f_{t} (k) = \hat{\mu}_{t} + k \hat{beta}_{t}$) and the variance increases with $k$ due to the more uncertainty.  Concerning figures (b) and (c) even if we report evidence of departure from normality assumptions, as previously clarify, this was also expected given the behavior of the process under analysis\footnote{Notice that together with the k-step-ahead point forecasts we plotted 3 samples from the forecast distribution. We present the k step ahead forecast only for this model, since the R package \textit{dlm} for the moment does not allow to compute it for non-constant models. In any case, a one-step in-sample forecast comparison will be provided in Section 2.4.  We decided to formally report the model checking only for the first model we are considering, however, we checked also all the subsequent specifications, available in the provided code. }. 
\subsection{2.2. Seasonal Dynamic Linear Model}

We now fit a seasonal DLM, suitable to capture the seasonal behavior of the process. As explained above, there are evidences that the frequency is 12 months. Indeed, monthly-specific factors influence the series, such as temperature, precipitations and economic activity. Hence, we consider 12 seasonal factors $\alpha_{1},\dots,\alpha_{12}$ imposing the usual restriction $\sum_{i=1}^{12} \alpha_{i} = 0$\footnote{This linear constraint implies that there are effectively only 11 free seasonal factors. Therefore, in our representation we use an 11-dimensional state vector}. As usual, the model is specified as: 
\begin{align*} 
   Y_t = \begin{bmatrix} 1 & 0 & \cdots & 0 \end{bmatrix} \theta_t + v_t,
   \quad & v_t \overset{iid}\sim \mathcal{N}(0, \sigma_{v}^{2}) 
  \\
   \theta_t = \begin{bmatrix} -1      &        & \cdots &        & -1     \\
                               1      & 0      & 0      & \cdots & 0      \\ 
                               0      & 1      & 0      & \cdots & 0      \\ 
                               \vdots &        & \ddots &        & \vdots \\
                               0      & \cdots & 0      & 1      & 0      \end{bmatrix} \theta_{t-1} + w_t,
   \quad & w_t \overset{iid}\sim \mathcal{N}_2(0, W=\begin{bmatrix} \sigma_w^2 & 0 & \cdots & 0      \\
                                                                    0          & 0 & \cdots & 0      \\ 
                                                                    \vdots     &   & \ddots & \vdots \\
                                                                    0          & 0 & \cdots & 0 \end{bmatrix})
\end{align*}

\textcolor{red}{WHICH ASSUMPTIONS?} Notice that the state vector $\theta_{t}$ contains 11 seasonal factors properly permutated through the matrix $G$. Once again, we estimate the unkown variances by MLE, obtaining in Table \ref{tab:MLE_seasonal}.

```{r}
# Seasonal DLM 
build_seas <- function(param){dlmModSeas(frequency = 12, dV = exp(param[1]), dW = c(exp(param[2]),0,0,0,0,0,0,0,0,0,0))}

# Estimate MLE of parameters
fit_seas <- dlmMLE(y, rep(0.5,2), build_seas, hessian=TRUE)
#fit_stagional$convergence

# Calculate SE of the MLE using delta method
estVarLog_seas <- solve(fit_seas$hessian)
estVar_seas <- diag(exp(fit_seas$par)) %*% estVarLog_seas %*% + diag(exp(fit_seas$par))
SE_seas <- sqrt(diag(estVar_seas))
```

```{r}
# MLE
MLE_sigmaV  <- paste0(round(exp(fit_seas$par[1]),4))
MLE_sigmaW <- paste0(round(exp(fit_seas$par[2]),9))

# se(MLE)
se_sigmaV  <- paste0('(', round(SE_seas[1],4), ')')
se_sigmaW <- paste0('(', round(SE_seas[2],6), ')')

# Summary table
MLEsum <- data.frame(sigmaV=c(MLE_sigmaV, se_sigmaV),
                     sigmaW=c(MLE_sigmaW, se_sigmaW))

knitr::kable(MLEsum, "latex", booktabs=T, align="c",
             col.names = c("$\\hat{\\sigma}^2_{v}$",
                           "$\\hat{\\sigma}^2_{w}$"),
             escape=F, caption="Maximum Likelihood estimates of the unknown
             parameters\\label{tab:MLE_seasonal}") %>%
  column_spec(1:2, width="8em") %>%
  collapse_rows(columns=1, latex_hline="none") %>%
  footnote(general="Standard errors in parentheses.",
           general_title="Note:",
           footnote_as_chunk=T, escape=F) %>%
  kable_styling(latex_options="hold_position")
```

In order to evaluate the improvement that a seasonal component has in our analysis, we combined it with the time variant linear growth model previously specified and plot in Figure \ref{Fig: Innovation_process_combined} the innovation process series. Comparing this plot with the previously reported one, it is possible to see that we are now able to capture a month-specific effects. The series looks like a random walk plus noise and shows only a marginal noise, that we try to capture in the following sections.    

```{r}
# Put MLE in the model + Add DLMs
Seas <- build_seas(fit_seas$par)
LG_v_seas <- LG_v + Seas

# Kalman filter
LG_v_seas_Filt <- dlmFilter(y, LG_v_seas)

# Remove the first item (m_0) in the vector of filtered states
filt_est_LG_v_seas = dropFirst(LG_v_seas_Filt$m)

# Compute the vector of state variances and standard deviations
list_c_LG_v_seas <- dlmSvd2var(LG_v_seas_Filt$U.C, LG_v_seas_Filt$D.C)
vol_C_LG_v_seas <- sqrt(unlist(list_c_LG_v_seas))

# One step ahead forecast
y_filt_LG_v_seas <- LG_v_seas_Filt$f
```

```{r, fig.width=10, fig.height=3.5, out.width='50%', fig.cap="\\label{Fig: Innovation_process_combined}Innovation process plot: time variant linear growth model + seasonal DLM."}

res_LG_v_seas <- residuals(LG_v_seas_Filt, sd=FALSE)
res_LG_v_seas = data.frame(res_LG_v_seas,louisiana_df$n_months)
colnames(res_LG_v_seas) = c("res", "time")

# Innovation process plot
ggplot(res_LG_v_seas, aes(x=time)) +
  geom_line(aes(y=res), size=0.5) +
  theme_bw(base_size=9) +
  labs(x="Year", y="Residuals") +
  scale_x_continuous(breaks=seq(0,186,24),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "2 years"), "%Y")) +
  scale_y_continuous(breaks=seq(-5,5,1), limits = c(-4,3)) +
  theme(axis.text.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.text.y=element_text(size=20, family="Times", margin=margin(r=5)),
        axis.title.x=element_text(size=20, family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=20, family="Times", margin=margin(r=5)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank(),
        plot.margin=unit(c(0,0,0,0.8), "cm"))
```

\subsection{2.3. Dynamic linear regression}

We take as explanatory variable $x_t$ Louisiana's coal consumption for electricity generation (in particular, for the totality of the electric power industry). The model we consider is the following:
\begin{eqnarray*} 
  & y_t = \alpha_t + \beta_t x_t + v_t, & v_t \overset{iid}\sim \mathcal{N}(0, \sigma_{v}^{2}) \\
  & \theta_t = G \theta_{t-1} + w_t, & w_t \overset{iid}\sim \mathcal{N}(0, W)
\end{eqnarray*}
where
\begin{equation*}
  \theta_t = \begin{bmatrix} \alpha_t & \beta_t \end{bmatrix}', \quad 
  \theta_0 \sim \mathcal{N}_2(m_0, C_0), \quad
  \theta_0 \indep (v_t) \indep (w_t)
\end{equation*}
and $G=I_2$ and $W=\text{diag}(\sigma_{w_1}^{2}, \sigma_{w_2}^{2})$, i.e the two time varying coefficients $\alpha_t$ and $\beta_t$ are modeled as independent random walks. 

```{r}
# Import dataset
coal_cons <- read.csv(url("https://raw.githubusercontent.com/SimoneArrigoni/US_pollution/master/Louisiana_coal_consumption.csv"), sep=";", header=T, stringsAsFactors=F)
x <- as.numeric(coal_cons$tot_electric_power_industry)

# Build dynamic linear regression model
# note: initialize the algorithm using the initial level of y and the sample variance before the break
build_DR <- function(param){dlmModReg(x, dV=exp(param[1]), dW=c(0, param[2]), m0=c(y[1],0))}

# Estimate MLE of parameters
fit_DR <- dlmMLE(y, c(0.5, 0.5), build_DR, hessian=TRUE) 
#fit_DR$convergence
parameters = unlist(build_DR(fit_DR$par)[c("V","W")])
#fit_DR$hessian 

# Calculate standard errors based on numeriacally evaluated hessian matrix
estVarLog_DR <- solve(fit_DR$hessian)
estVar_DR <- diag(exp(fit_DR$par)) %*% estVarLog_DR %*% + diag(exp(fit_DR$par))
SE_DR <- sqrt(diag(estVar_DR))

# Put MLE in the model
DR <- build_DR(fit_DR$par)

# Kalman filter
DR_Filt <- dlmFilter(y, DR)

# Remove the first item (m_0) in the vector of filtered states
filt_est_DR = dropFirst(DR_Filt$m)

# Compute the vector of state variances and standard deviations
list_c_DR <- dlmSvd2var(DR_Filt$U.C, DR_Filt$D.C)
vol_C_DR <- sqrt(unlist(list_c_DR))

# One step ahead forecast
y_filt_DR <- DR_Filt$f
```

In Table \ref{tab:MLE_dr} are presented the MLEs of the unknown coefficients and the associated standard errors.

```{r}
# MLE
MLE_sigmaV  <- paste0(round(parameters[1],4))
MLE_sigmaW1 <- paste0(round(parameters[2],9))
MLE_sigmaW2 <- paste0(round(parameters[5],9))

# se(MLE)
se_sigmaV  <- paste0('(', round(SE_DR[1],4), ')')
se_sigmaW1 <- paste0('(', round(SE_DR[2],5), ')')
se_sigmaW2 <- paste0('(', round(SE_DR[3],9), ')')

# Summary table
MLEsum <- data.frame(sigmaV=c(MLE_sigmaV, se_sigmaV),
                     sigmaW1=c(MLE_sigmaW1, se_sigmaW1),
                     sigmaW2=c(MLE_sigmaW2, se_sigmaW2))

knitr::kable(MLEsum, "latex", booktabs=T, align="c",
             col.names = c("$\\hat{\\sigma}^2_{v}$",
                           "$\\hat{\\sigma}^2_{w_1}$",
                           "$\\hat{\\sigma}^2_{w_2}$"),
             escape=F, caption="Maximum Likelihood estimates of the unknown
             parameters\\label{tab:MLE_dr}") %>%
  column_spec(1:3, width="5em") %>%
  collapse_rows(columns=1, latex_hline="none") %>%
  footnote(general="Standard errors in parentheses.",
           general_title="Note:",
           footnote_as_chunk=T, escape=F) %>%
  kable_styling(latex_options="hold_position")
```

Let's now compute the smoothing estimates of $\theta_t=[\alpha_t \; \beta_t]'$, for $t=0,\ldots,T$. The smoothing estimates of $\beta_t$ are plotted over time together with their $90\%$ credible intervals in Figure \ref{fig:smooth_est}.

```{r, fig.width=6, fig.height=2, out.width='50%', fig.cap="\\label{fig:smooth_est} Smoothing estimates of beta with credible intervals"}
# Smoothing estimates
cO_Smooth <- dlmSmooth(y, DR)
smooth_state_est <- dropFirst(cO_Smooth$s[,2])

# Smoothing variances
smooth_var <- dlmSvd2var(cO_Smooth$U.S, cO_Smooth$D.S)
smooth_sd <- (unlist(smooth_var))^.5
beta_sd <- vector(mode = "numeric", length = length(smooth_var))
for (i in 1:length(smooth_var)){ beta_sd[i] <- smooth_sd[i*4] }
smooth_sd_est <- dropFirst(beta_sd)

# C.I. upper and lower bounds
upper_smooth <- smooth_state_est + 1.64*smooth_sd_est # upper bound of 90% C.I.
lower_smooth <- smooth_state_est - 1.64*smooth_sd_est # lower bound of 90% C.I.

# Create df
df_co_ci <- data.frame(louisiana_df$n_months, smooth_state_est, upper_smooth, lower_smooth)
colnames(df_co_ci) = c("n_months", "smooth_state_est", "upper_smooth","lower_smooth")

# Plot the smoothing estimates with their credible intervals
ggplot(df_co_ci, aes(x=n_months)) +
  geom_line(aes(y=smooth_state_est), size=0.4) +
  geom_line(aes(y=upper_smooth), color="red", linetype="dashed", size=0.2) +
  geom_line(aes(y=lower_smooth), color="red", linetype="dashed", size=0.2) +
  geom_ribbon(aes(ymin=lower_smooth, ymax=upper_smooth), alpha=.1, fill="red") +
  theme_bw(base_size=9) +
  labs(x="Year", y="Beta", color="black") +
  scale_y_continuous() +
  scale_x_continuous(breaks=seq(0,186,24),
                     labels=format(seq(as.Date("2001/1/1"),as.Date("2016/6/1"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(family="Times", margin=margin(t=2)),
        axis.text.y=element_text(family="Times", margin=margin(r=2)),
        axis.title.x=element_text(size=8, family="Times", margin=margin(t=2)),
        axis.title.y=element_text(size=8, family="Times", margin=margin(r=2)),
        panel.grid.major=element_line(size=0.5),
        panel.grid.minor=element_blank())
```

```{r}
source("DR_model_checking.R")
```

\subsection{2.4. Performance comparison}

We treat the data under study as streaming data, periodically updated by EPA. Therefore, our main interest is forecasting, which is also a useful way to check the performance of the different specifications considered so far. Therefore, we plot the one-step-ahead obtained with the models fitted in the previous sections (Figure \ref{fig:one-step-forecast_est})\footnote{for sake of clarity we decided to report only the temporal window around the breakpoint.}.
```{r, fig.width=7, fig.height=2, out.width='75%', fig.cap="\\label{fig:one-step-forecast_est} One-step-ahead forecast estimates"}
# Build df
df_forecast <- data.frame(louisiana_df$n_months, y, y_filt_LG, y_filt_LG_v, y_filt_LG_v_seas, y_filt_DR) # + struct_fit,
colnames(df_forecast) <- c("n_months", "y", "y_filt_LG", "y_filt_LG_v", "y_filt_LG_v_seas", "y_filt_DR") # + "struct_fit",

# Plot
colors <- c("Observed data" = "black", "Time-invariant linear growth" = "springgreen4", 
            "Time-variant linear growth"="blue", "Time-variant linear growth + Seasonality"="orange",
            "Dynamic regression"="red")
ggplot(df_forecast[61:109,], aes(x=n_months)) +
  geom_line(aes(y=y, color="Observed data"), size=0.4) +
  geom_line(aes(y=y_filt_LG, color = "Time-invariant linear growth"), size=0.4) +
  geom_line(aes(y=y_filt_LG_v, color = "Time-variant linear growth"), size=0.4) +
  geom_line(aes(y=y_filt_LG_v_seas, color = "Time-variant linear growth + Seasonality"), size=0.4) +
  geom_line(aes(y=y_filt_DR, color="Dynamic regression"), size=0.4) +
  #geom_line(aes(y=struct_fit, color="Basic structural model"), size=0.4) +
  #geom_vline(xintercept = as.Date("2008/1/1"), color= "black", size=0.3) +
  theme_bw(base_size=9) +
  labs(x="Year", y=expression(CO~(ppm)), color = "") +
  scale_y_continuous(breaks=seq(0,1,0.2)) +
  scale_x_continuous(breaks=seq(61,109,24),
                     labels=format(seq(as.Date("2006/1/1"),as.Date("2011/12/31"), "2 years"), "%Y")) +
  theme(axis.text.x=element_text(family="Times", margin=margin(t=2)),
        axis.text.y=element_text(family="Times", margin=margin(r=2)),
        axis.title.x=element_text(size=8, family="Times", margin=margin(t=2)),
        axis.title.y=element_text(size=8, family="Times", margin=margin(r=2)),
        panel.grid.major=element_line(size=0.02),
        panel.grid.minor=element_blank(),) +
        scale_color_manual(values=colors) +
        theme(legend.justification=c("left", "top"), legend.text=element_text(family="Times"),
              legend.box.just="bottom", legend.margin=margin(2,0,2,2))

# Alternative legend position
#        theme(legend.position="bottom", legend.text=element_text(size=12, family="Times"),
#              legend.spacing.x=unit(1, "cm"))
```

It is possible to see the effect that the signal-to-noise ratio has on the one-step-ahead forecast . For example, the time invariant linear growth model reports a relative magnitude of $W$ on $V$ higher than the time variant one and, consequently, its point forecasts tend to follow more closely the data. AGGIUNGERE TABELLA MAE,MAPE,MSE

\section{Section 3. Multivariate DLMs}

\subsection{3.1. Seemingly unrelated regression (SUR)}

As a last step, we build a multivariate dynamic regression model. In addition to the $\text{CO}$ series, we consider now also the $\text{SO}_2$ monthly data. Assuming that the intercepts and the slopes are correlated across the two pollutants, we can define a \textit{seemingly unrelated regression model} (SUR) as follows:
\begin{eqnarray*}
  & Y_t = (F_t \otimes I_2) \; \theta_t + v_t, & v_t \overset{iid}\sim \mathcal{N}_{2}(0,V) \\
  & \theta_t = (G \otimes I_2) \; \theta_{t-1} + w_t, & w_t \overset{iid}\sim \mathcal{N}_{4}(0,W)
\end{eqnarray*}
with
\begin{equation*}
  \underset{2 \times 1}{Y_t} = \begin{bmatrix} Y_{CO,t} \\ Y_{SO_2,t} \end{bmatrix}, \quad
  \underset{4 \times 1}{\theta_t} = \begin{bmatrix} \alpha_{CO,t} \\ \alpha_{SO_2,t}, \\ \beta_{CO,t} \\
  \beta_{SO_2,t} \end{bmatrix}, \quad
  \underset{1 \times 2}{F_t} = \begin{bmatrix} 1 & x_t \end{bmatrix}, \quad
  \underset{2 \times 2}{G} = I_2 \quad \text{ and } \quad
  \underset{4 \times 4}{W} = \text{blockdiag}(W_{\alpha}, W_{\beta}).
\end{equation*}
Note that, since both pollutants are by-products of incomplete fuel combustion, we consider $x_{i,t}=x_t$, that is we take the same explanatory variable for the two series.

```{r, include=F}
# Build df
y_m <- data.frame(louisiana_df$m_CO_mean, louisiana_df$m_SO2_mean)
y_m <- rename(y_m, c("CO"="louisiana_df.m_CO_mean", "SO2"="louisiana_df.m_SO2_mean"))
y_m <- data.matrix(y_m)
m <- ncol(y_m)

# Build SUR model
build_SUR <- function(param){
  mod <- dlmModReg(x)
  mod$FF <- mod$FF %x% diag(m)
  mod$JFF <- mod$JFF %x% diag(m)
  mod$GG <- mod$GG %x% diag(m)
  mod$V <- mod$V %x% matrix(0, m, m)
  mod$V <- c(exp(param[1]), 0,
             0, exp(param[2]))
  ## Old version:
  #mod$V <- diag(c(exp(param[1]), exp(param[2])))
  mod$W <- mod$W %x% matrix(0, m, m)
    Wa <- matrix(c(exp(param[3]), 0, 0, exp(param[4])), 2, 2)
    Wb <- matrix(c(exp(param[5]), exp(param[6]), exp(param[7]), exp(param[8])), 2, 2)
    W <- bdiag(Wa, Wb)
  mod$W <- W
  ## Alternative version (why doesn't it work?):
  #mod$W <- c(exp(param[3]), 0, 0, 0,
  #           0, exp(param[4]), 0, 0,
  #           0, 0, exp(param[5]), exp(param[6]),
  #           0, 0, exp(param[7]), exp(param[8]))
  mod$m0 <- c(y_m[1,1], y_m[1,2], 0, 0)
  return(mod)
}

# Estimate MLE of parameters
fit_SUR <- dlmMLE(y_m, parm=rep(0.5, 8), build_SUR, hessian=TRUE)
unlist(build_SUR(fit_SUR$par)[c("V","W")])

# Calculate SE of the MLE using delta method
estVarLog_SUR <- solve(fit_SUR$hessian)
estVar_SUR <- diag(exp(fit_SUR$par)) %*% estVarLog_SUR %*% + diag(exp(fit_SUR$par))
SE_SUR <- sqrt(diag(estVar_SUR))

# Put MLE in the model
SUR <- build_SUR(fit_SUR$par)

# Kalman filter
SUR_Filt <- dlmFilter(y_m, SUR)
SUR_sd <- residuals(SUR_Filt)$sd

# Kalman smoother
SUR_Smooth <- dlmSmooth(y_m, SUR)
```

\subsection{3.2. Seemingly unrelated time series equations (SUTSE)}

Let us still consider the time series for $\text{CO}$ and $\text{SO}_2$. From visual inspection of Figure \ref{fig:louisiana_plots} it appears that the two series display a similar type of qualitative behavior, that can be modeled by a linear growth DLM. To set up a multivariate model for the two series, we combine the two linear growth models in a comprehensive \textit{seemingly unrelated time series equations model} (SUTSE). This is written as:
\begin{eqnarray*}
  & Y_t = (F \otimes I_2) \; \theta_t + v_t, & v_t \sim \mathcal{N}_{2}(0,V) \\
  & \theta_t = (G \otimes I_2) \; \theta_{t-1} + w_t, & w_t \sim \mathcal{N}_{4}(0,W)
\end{eqnarray*}
with
\begin{equation*}
  \underset{2 \times 1}{Y_t} = \begin{bmatrix} Y_{CO,t} \\ Y_{SO_2,t} \end{bmatrix}, \quad
  \underset{4 \times 1}{\theta_t} = \begin{bmatrix} \mu_{CO,t} \\ \mu_{SO_2,t}, \\ \beta_{CO,t} \\
  \beta_{SO_2,t} \end{bmatrix}, \quad
  \underset{1 \times 2}{F} = \begin{bmatrix} 1 & 0 \end{bmatrix}, \quad
  \underset{2 \times 2}{G} = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \quad \text{ and } \quad \underset{4
  \times 4}{W} = \text{blockdiag}(W_{\mu}, W_{\beta}).
\end{equation*}
We assume that growth $\beta_{i,t}$ is correlated among pollutants. Moreover, to simplify the model in order to reduce the overall number of parameters, we also assume that the two linear growth models are integrated random walks, that is $W_{\mu}=0$.

```{r, include=F}
# Build SUTSE model
build_SUTSE <- function(param){
  mod <- dlmModPoly(2)
  mod$FF <- mod$FF %x% diag(m)
  mod$GG <- mod$GG %x% diag(m)
  mod$V <- mod$V %x% matrix(0, m, m)
  mod$V <- c(exp(param[1]), exp(param[2]),
             exp(param[3]), exp(param[4]))
  ## Old version:
  #mod$V <- matrix(c(exp(param[1]), param[2], exp(param[3]), param[4]), m, m)
  mod$W <- mod$W %x% matrix(0, m, m)
  mod$W <- c(0, 0, 0, 0,
             0, 0, 0, 0,
             0, 0, exp(param[5]), exp(param[6]),
             0, 0, exp(param[7]), exp(param[8]))  
  ## Old version:  
    #Wu <- matrix(0, m, m)
    #Wb <- matrix(c(exp(param[5]), param[5], exp(param[7]), param[8]), m, m)
    #W <- bdiag(Wu, Wb)
  #mod$W <- W
  mod$m0 <- c(y_m[1,1], y_m[1,2], 0, 0)
  return(mod)
}
  
# Estimate MLE of parameters
fit_SUTSE <- dlmMLE(y_m, parm=rep(0.5, 8), build_SUTSE, hessian=TRUE)
unlist(build_SUTSE(fit_SUTSE$par)[c("V","W")])

# Calculate SE of the MLE using delta method
estVarLog_SUTSE <- solve(fit_SUTSE$hessian)
estVar_SUTSE <- diag(exp(fit_SUTSE$par)) %*% estVarLog_SUTSE %*% + diag(exp(fit_SUTSE$par))
SE_SUTSE <- sqrt(diag(estVar_SUTSE))

# Put MLE in the model
SUTSE <- build_SUTSE(fit_SUTSE$par)

# Kalman filter
SUTSE_Filt <- dlmFilter(y_m, SUTSE)
SUTSE_sd <- residuals(SUTSE_Filt)$sd
```

\section{Conclusion}

In conclusion, \dots

```{r, include=F}
source("forecasting_accuracy.R")
```

```{r, include=T}
# Forecasting accuracy: Summary table (v1)
for_accuracy <- data.frame(measure   = c("MSE",          "MAE",          "MAPE"),
                           #HMM      = c(MSE_HMM,        MAE_HMM,        MAPE_HMM),
                           LG        = c(MSE_LG,         MAE_LG,         MAPE_LG),
                           #LG_2     = c(MSE_LG_2,       MAE_LG_2,       MAPE_LG_2),
                           #LG_3     = c(MSE_LG_3,       MAE_LG_3,       MAPE_LG_3),
                           LG_v      = c(MSE_LG_v,       MAE_LG_v,       MAPE_LG_v),
                           LG_v_seas = c(MSE_LG_v_seas,  MAE_LG_v_seas,  MAPE_LG_v_seas),
                           DR        = c(MSE_DR,         MAE_DR,         MAPE_DR),
                           SUR       = c(MSE_SUR,        MAE_SUR,        MAPE_SUR),
                           SUTSE     = c(MSE_SUTSE,      MAE_SUTSE,      MAPE_SUTSE))

knitr::kable(for_accuracy, "latex", booktabs=T, align="c",
             col.names=c('', 'Time-invar LG', 'Time-var LG', 'Time-var LG + Seas', 'DR', 'SUR', 'SUTSE'),
             #col.names=c('Measure', 'HMM', 'Time-invar LG', 'Time-var LG', 'Time-var LG + Seas',
             #            'DR', 'SUR', 'SUTSE'),
             escape=F, caption="Measures of predictive accuracy") %>%
  column_spec(1:9, width="5em") %>%
  kable_styling(latex_options="hold_position")

########################## old version of the table below here ##########################

# Forecasting accuracy: Summary table (v2)
#tabf <- matrix(nrow=3, ncol=8)
#rownames(tabf) <- c('MSE','MAE','MAPE')
#colnames(tabf) <- c('HMM', 'Lin. growth', 'Const. speed', 'Int. RW', 'BSM','Dyn. regr.', 'SUR', 'SUTSE')

#tabf[1,1] <- MSE_HMM
#tabf[2,1] <- MAE_HMM
#tabf[3,1] <- MAPE_HMM

#tabf[1,2] <- MSE_LG
#tabf[2,2] <- MAE_LG
#tabf[3,2] <- MAPE_LG

#tabf[1,3] <- MSE_LG_2
#tabf[2,3] <- MAE_LG_2
#tabf[3,3] <- MAPE_LG_2

#tabf[1,4] <- MSE_LG_3
#tabf[2,4] <- MAE_LG_3
#tabf[3,4] <- MAPE_LG_3

#tabf[1,5] <- MSE_BSM
#tabf[2,5] <- MAE_BSM
#tabf[3,5] <- MAPE_BSM

#tabf[1,6] <- MSE_DR
#tabf[2,6] <- MAE_DR
#tabf[3,6] <- MAPE_DR

#tabf[1,7] <- MSE_SUR
#tabf[2,7] <- MAE_SUR
#tabf[3,7] <- MAPE_SUR

#tabf[1,8] <- MSE_SUTSE
#tabf[2,8] <- MAE_SUTSE
#tabf[3,8] <- MAPE_SUTSE

#kable(tabf, caption="Measures of predictive accuracy")
```

\newpage
\section{References}