---
title: "20236 Time Series Analysis - Final project"
author:
- Simone Arrigoni (1794692)
- Luca Badolato (3086040)
- Simone Valle (3088281)
subtitle: "Bocconi University"
date: "21/4/2020"
output: pdf_document
nocite: '@*'
bibliography: References.bib
header-includes:
  \usepackage[utf8]{inputenc}
  \usepackage{setspace}
  \usepackage{apacite}
  \usepackage{natbib}
  \usepackage{algpseudocode}
  \usepackage{algorithm}
  \usepackage{bm}
  \usepackage{amsmath}
  \usepackage{amssymb}
  \usepackage{graphicx}
  \usepackage{subfig}
  \usepackage{booktabs, caption}
  \usepackage{array}
  \usepackage{threeparttable}
  \usepackage{listings}
  \usepackage{physics}
  \usepackage{float}
  
  \floatplacement{figure}{H}
  \usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
  \definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
  \definecolor{mylilas}{RGB}{170,55,241}
  \definecolor{codegray}{RGB}{0.5,0.5,0.5}
  \definecolor{orange}{RGB}{255,165,0}
  \DeclareMathOperator*{\E}{\mathbb{E}}
  \DeclareMathOperator*{\Ec}{\mathbb{E}_t}
  \setlength\parindent{0pt}
  
---

```{r, include=FALSE}

# Load useful packages
library(utf8)
library(labeling)
library(rmarkdown)
library(httr)
library(knitr)
library(tseries)
library(tinytex)
library(scales)
library(dlm)
library(magrittr)
library(stringr)
library(depmixS4)
library(tidyverse)
library(ggthemes)
library(latex2exp)
library(kableExtra)
library(ggpubr)
library(reshape2)
library(dlm)
# Settings
knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE,
                      echo    = FALSE,
                      include = TRUE,
                      fig.pos = "H",
                      fig.align = "center",
                      out.width='60%')
```

\abstract{INSERT ABSTRACT HERE}

\section{Introduction}

The dataset \texttt{US\_pollution} contains monthly and daily data about air pollution for each state in the United States. It is provided by the United States Environmental Protection Agency (EPA). Specifically, four key indicators of air pollution are considered: 

\begin{itemize}
\item \textbf{Ground level ozone $\text{O}_{3}$} (\texttt{O3}): Bad ozone forms near the ground when pollutants (emitted by sources such as cars, power plants, industrial boilers, refineries, and chemical plants) react chemically in sunlight. Since it is more likely to form during warmer months, we expect to see a seasonal component. 
\item \textbf{Nitrogen dioxide $\text{NO}_{2}$} (\texttt{NO2}): It is produced as a result of road traffic and other fossil fuel combustion processes. Furthermore, its presence in air contributes to the formation and modification of other air pollutants, such as particle pollution.
\item \textbf{Carbon monoxide $\text{CO}$} (\texttt{CO}):  It forms from incomplete combustions mainly from vehicle exhausts (roughly 75\% of all carbon monoxide emissions nationwide and up to 95\% in cities), fuel combustion in industrial processes and natural sources. Cold temperatures make combustion less complete, therefore we expect also in this case a seasonal behavior. 
\item \textbf{Sulfur dioxide $\text{SO}_{2}$} (\texttt{SO2}): It is produced when sulfur-containing fuels are burned. It is common mainly in large industrial complexes. 
\end{itemize}
\medskip

Hence, the four variables of interest are interconnected and are the building blocks of a more general phenomenon, that is air pollution. Furthermore, they are related by a common latent process that can be summarized by several components such as industrialization, car traffic and regulation. 

\section{Data}

The preliminary analysis that led us from the raw data to the current dataset is made available in the GitHub repository https://github.com/SimoneArrigoni/US_pollution. 

We will be focusing on measures for \textbf{Louisiana}, recorded weekly from 01/01/2000 to 27/04/2016, for a total of 852 observations. The main advantage of studying the evolution of air pollution in the same country over the years is that this allows us to control for fixed effects and to better study the latent process of interest that relates the four pollution components.The first five observations of the dataset are reported in Table \ref{tab:louisiana_df}.

```{r echo=FALSE}
# Import the dataset
louisiana_df <- read.csv(url("https://raw.githubusercontent.com/SimoneArrigoni/US_pollution/master//Louisiana_dataset.csv"), sep = ",", header=T)
```

```{r}
# How the dataset looks like
knitr::kable(louisiana_df[1:5, 1:5], "latex", booktabs=T,
             caption="Dataset structure\\label{tab:louisiana_df}") %>%
  kable_styling(latex_options="hold_position")
```
\bigskip

As explained in above, the four variables are related by a common latent process. Hence, we report in Table \ref{tab:louisiana_corr} their correlation coefficients.

Provided that the correlation between pollutants show significant differences in regional patterns and that $\text{CO}$ is a good indicator for industrial and biomass burning pollution [@Logan1981], a positive and strong correlation with $\text{O}_{3}$ indicates that a region has experienced photochemical $\text{O}_{3}$ production from its precursors (including $\text{CO}$) and is aligned with previous studies in the field [@Voulgarakis2011].

Other studies report that, in the US, $\text{O}_3$ is on average positively correlated with $\text{NO}_2$, which is opposite to our empirical findings about Louisiana, where they are slightly negatively correlated. A possible explanation can be given by substitution effects: an increase of nitrogen dioxide levels can be associated to the use of more fossil fuels that produce $\text{NO}_2$ instead of others. This might be related to Louisiana's fixed effects, however further rasearches would be beyond the purpose of our study.

```{r}
# Correlation plot
datacor = subset(louisiana_df, select=c(weekly_O3_mean, weekly_NO2_mean, weekly_CO_mean, weekly_SO2_mean))
cor.datacor = cor(datacor, use="complete.obs")
cormat = round(cor(datacor),2)
row.names(cormat) <- c("$\\text{O}_{3}$",
                       "$\\text{NO}_{2}$",
                       "$\\text{CO}$",
                       "$\\text{SO}_{2}$")

knitr::kable(cormat, "latex", booktabs=T, align="c", escape=F,
             caption="Correlation table \\label{tab:louisiana_corr}",
             col.names=c("$\\text{O}_{3}$",
                         "$\\text{NO}_{2}$",
                         "$\\text{CO}$",
                         "$\\text{SO}_{2}$")) %>%
  column_spec(1:4, width="5em") %>%
kable_styling(latex_options="hold_position")
```


We plot in Figure \ref{fig:louisiana_plots} the time series for the four pollutants under study to give a first qualitative description of the phenomenon.

```{r, fig.width=20, fig.height=6, out.width='100%', fig.cap="\\label{fig:louisiana_plots} Air pollution in the state of Louisiana"}

# Time series plot
O3 <- ggplot(louisiana_df, aes(x=n_weeks, y=weekly_O3_mean)) +
  geom_line(color="steelblue") +
  theme_classic(base_size=9) +
  labs(y=expression(O[3]~(ppm)), x="Week") +
  scale_x_continuous(breaks=seq(0,900,100)) +
  theme(axis.text=element_text(size=17),
        axis.title.x=element_text(size=19,family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=19,family="Times", margin=margin(r=5)))

NO2 <- ggplot(louisiana_df, aes(x=n_weeks, y=weekly_NO2_mean)) +
  geom_line(color="steelblue") +
  theme_classic(base_size=9) +
  labs(y=expression(NO[2]~(ppb)), x="Week") +
  scale_x_continuous(breaks=seq(0,900,100)) +
  theme(axis.text=element_text(size=17),
        axis.title.x=element_text(size=19,family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=19, family="Times", margin=margin(r=5)))

CO <- ggplot(louisiana_df, aes(x=n_weeks, y=weekly_CO_mean)) +
  geom_line(color="steelblue") +
  theme_classic(base_size=9) +
  labs(y=expression(CO~(ppm)), x="Week") +
  scale_x_continuous(breaks=seq(0,900,100)) +
  theme(axis.text=element_text(size=17),
        axis.title.x=element_text(size=19,family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=19,family="Times", margin=margin(r=5)))

SO2 <- ggplot(louisiana_df, aes(x=n_weeks, y=weekly_SO2_mean)) +
  geom_line(color="steelblue") +
  theme_classic(base_size=9) +
  labs(y=expression(SO[2]~(ppb)), x="Week") +
  scale_x_continuous(breaks=seq(0,900,100)) +
  theme(axis.text=element_text(size=17),
        axis.title.x=element_text(size=19,family="Times", margin=margin(t=5)),
        axis.title.y=element_text(size=19,family="Times", margin=margin(r=5)))

ggarrange(O3, NO2, CO, SO2,
          ncol=2, nrow=2)
```
The time series, each with its specific complexities, do not in general look stationary. For instance, time_varying mean and variances can be identified basically in all the series with different changing patterns. Hence, they cannot be analized with the well-known ARMA models, which require covariance-stationarity.

\section{$\text{SO}_{2}$ series}

Looking at Figure \ref {fig:louisiana_plots} we can already guess the presence of non-stationarity due to change points and not constant mean and variance functions.

Firstly, as exploratory step, we decompose the time series in its structural component, namely trend, seasonality and erratic, by using an additive model. Figure \ref{fig:decompose_SO2}, that plots together the time series with such components, shows that, in fact, a seasonal factor seems to be present. Secondly, the series is characterized by a non-constant and non-monotonic trend, suggesting that the process may be summarized by a time-varying mean function, $\mu(t)$. Finally, the random component clearly shows that the process has a variance function, $\sigma^2(t)$, that changes over time.

These preliminary findings confirm the idea of non-stationarity of the series that we guessed by simply looking at the time series plot. Hence, we will further analyze it with models that allow to treat non-stationarity. 

```{r, fig.width=8, fig.height=5, fig.cap="\\label{fig:decompose_SO2}Additive decomposition into structural components"}
ts_SO2<-ts(louisiana_df$weekly_SO2_mean,start=c(2000,1,1),end = c(2016,4,27),frequency = 48)
dec_SO2<-decompose(ts_SO2,type=c("additive"))
plot(dec_SO2)

#des_ts_SO2<-ts_SO2 - dec_SO2$seasonal
```

\subsection{Homogeneous Hidden Markov Model}

A first possibility is to model the series of $\text{SO}_{2}$ using a \textit{homogeneous} HMM, with a latent process $(S_{t})_{t\geq0}$ that represents the industrialization activity or traffic level, following what explained in the introduction.

Let's assume:
\begin{itemize}
\item three hidden states, that may be interpreted as high, medium and low comprehensive air pollution. Therefore, the state space of the latent process is $\mathcal{Y}=\{1,2,3\}$;
\item Gaussian emission distributions, with state-dependent mean $\mu_i$ and variance $\sigma_i$ with $i=1,2,3$.
\end{itemize}

\begin{align*}
\begin{cases}
Y_{t}=\mu_{1}+\varepsilon_{t}, \quad \varepsilon_{t}\overset{i.i.d.}{\sim}N(0,\sigma_{1}^2) \qquad \text{if} \; S_{t}=1 \\
Y_{t}=\mu_{2}+\varepsilon_{t}, \quad \varepsilon_{t}\overset{i.i.d.}{\sim}N(0,\sigma_{2}^2) \qquad \text{if} \; S_{t}=2 \\
Y_{t}=\mu_{3}+\varepsilon_{t}, \quad \varepsilon_{t}\overset{i.i.d.}{\sim}N(0,\sigma_{3}^2) \qquad \text{if} \; S_{t}=3
\end{cases}
\end{align*}

```{r}
# 1) Model specification
y <- as.numeric(louisiana_df$weekly_SO2_mean)
nstates_us <- 3
set.seed(61)
model_SO2 <- depmixS4::depmix(y ~ 1, data=data.frame(y), nstates=nstates_us)
```

We estimate the unknown parameters by maximum likelihood, using an \textit{Expectation-Maximization (EM) algorithm}. In Table \ref{tab:HMM_MLE_SO2} we report the estimated means and standard deviations of the three states with the associated standard errors. 

```{r include=FALSE}
# 2) Estimation of the unknown parameters
fmodel_SO2 <- depmixS4::fit(model_SO2)
states_mu_sigma_SO2 <- depmixS4::summary(fmodel_SO2)
```
```{r}
# MLE
mu1hat_SO2 <- paste0(round(fmodel_SO2@response[[1]][[1]]@parameters$coefficients,3))
mu2hat_SO2 <- paste0(round(fmodel_SO2@response[[2]][[1]]@parameters$coefficients,3))
mu3hat_SO2 <- paste0(round(fmodel_SO2@response[[3]][[1]]@parameters$coefficients,3))
sd1hat_SO2 <- paste0(round(fmodel_SO2@response[[1]][[1]]@parameters$sd,3))
sd2hat_SO2 <- paste0(round(fmodel_SO2@response[[2]][[1]]@parameters$sd,3))
sd3hat_SO2 <- paste0(round(fmodel_SO2@response[[3]][[1]]@parameters$sd,3))
# se(MLE)
MLEse_SO2=depmixS4::standardError(fmodel_SO2)
se_mu1hat_SO2 <- paste0('(', round(MLEse_SO2$se[13],3), ')')
se_mu2hat_SO2 <- paste0('(', round(MLEse_SO2$se[15],3), ')')
se_mu3hat_SO2 <- paste0('(', round(MLEse_SO2$se[17],3), ')')
se_sd1hat_SO2 <- paste0('(', round(MLEse_SO2$se[14],3), ')')
se_sd2hat_SO2 <- paste0('(', round(MLEse_SO2$se[16],3), ')')
se_sd3hat_SO2 <- paste0('(', round(MLEse_SO2$se[18],3), ')')
```

```{r}
# Summary table creation
MLEsum_SO2 <- data.frame(state=c(rep("S=1",2), rep("S=2",2), rep("S=3",2)), 
                         mu=c(mu1hat_SO2, se_mu1hat_SO2, mu2hat_SO2, se_mu2hat_SO2, 
                              mu3hat_SO2, se_mu3hat_SO2),
                         sd=c(sd1hat_SO2, se_sd1hat_SO2, sd2hat_SO2, se_sd2hat_SO2,
                              sd3hat_SO2, se_sd3hat_SO2))
knitr::kable(MLEsum_SO2, "latex", booktabs=T, align="c",
             col.names = c("State", "$\\hat{\\mu}_{MLE}$", "$\\hat{\\sigma}_{MLE}$"),
             escape=F, caption="Maximum Likelihood estimates of the state-dependent
             mean and standard deviation\\label{tab:HMM_MLE_SO2}") %>%
  column_spec(1:3, width="5em") %>%
  collapse_rows(columns=1, latex_hline="none") %>%
  footnote(general="Standard errors in parentheses",
           general_title="Note:",
           footnote_as_chunk=T, escape=F) %>%
  kable_styling(latex_options="hold_position")
```

Finally, we find the "optimal" state sequence associated with the observed $\text{SO}_{2}$ time series, the so called \textit{decoding} procedure. The optimization problem is solved using the \textit{Viterbi algorithm} and the results are shown in Figure \ref{fig:model_fit_ost_SO2}.

```{r}
# Get the estimated state for each timestep 
estStates_SO2 <- posterior(fmodel_SO2)
```

```{r, fig.width=7, fig.height=3, fig.cap="\\label{fig:model_fit_ost_SO2}Optimal state trajectory"}

# Plot the optimal path, that is stored in the first column of estStates
results_df_SO2 <- data.frame(time_index=louisiana_df$n_weeks,
                         SO2=louisiana_df$weekly_SO2_mean,
                         state=estStates_SO2[1]) %>% 
  gather("variable", "value", -time_index)

my_breaks <- function(x) { if (max(x)<4) seq(0,3,1) else seq(0,900,100) }
lab_names <- c(
  'SO2'="SO2 (ppb)",
  'state'="Viterbi state")

plotobj_SO2 <- ggplot(results_df_SO2, aes(time_index, value)) +
  geom_line(color="black") +
  facet_wrap(variable~., scales="free", ncol=1,
             strip.position="left",
             labeller = as_labeller(lab_names)) +
  labs(x="Week") +
  scale_y_continuous(breaks=my_breaks) +
  scale_x_continuous(breaks=seq(0,900,100))+
  theme_classic(base_size=9) +
  theme(axis.text=element_text(size=13),
        axis.title.x=element_text(size=16,family="Times", margin=margin(5,0,0,0)),
        axis.title.y=element_blank(),
        strip.text.x=element_text(size=16,family="Times", face="bold",
                                  margin=margin(10,0,0,0)),
        strip.background=element_rect(colour="black", fill=NA),
        strip.placement="outside")

plot(plotobj_SO2)
```

The series with HMM estimated state-dependent means and state-dependent standard deviations are shown in Figure \ref{fig:model_fit_rsm_SO2}.

```{r, fig.width=7, fig.height=3, fig.cap="\\label{fig:model_fit_rsm_SO2}State-dependent means and standard deviations"}

# Chart with state-dependent mean and standard deviations
info_df_SO2 <- data.frame(state=1:nstates_us,
                      mu_SO2=states_mu_sigma_SO2[,1],
                      sigma_SO2=states_mu_sigma_SO2[,2])
df_to_plot_SO2 <- estStates_SO2 %>%
  left_join(info_df_SO2)
df_to_plot_SO2 %<>%
  mutate(xtime=louisiana_df$n_weeks, yvalue=louisiana_df$weekly_SO2_mean)

ggplot(df_to_plot_SO2, aes(x=xtime, y=yvalue)) +
  geom_line(size=.2,aplha=.3) +
  geom_point(aes(x=xtime, y=mu_SO2), col="blue", size=.05) +
  geom_ribbon(aes(ymin=mu_SO2-2*sigma_SO2, ymax=mu_SO2+2*sigma_SO2), alpha=.1) +
  theme_classic(base_size=9) +
  theme(plot.title=element_text(hjust=0.5)) +
  labs(y=expression(SO[2]~(ppb)), x="Weeks") +
  scale_x_continuous(breaks=seq(0,900,100)) +
  theme(plot.title=element_text(size=10, face="bold", margin=margin(0,0,10,0)),
        axis.title.x=element_text(family="Times", margin=margin(t=5)),
        axis.title.y=element_text(family="Times", margin=margin(r=5)))
```

\subsection{DLM - Linear growth model}
Given the complexity of the series, the flexibility provided by \textit{Dinamic Linear models} may lead to a better fit compared with the HMM estimated in the previous section.

```{r}
# Polinomial specification of the RW + noise
rw_SO2 <- StructTS(y,type=c('level'))
rw_SO2$coef

ggplot(data.frame(louisiana_df$n_weeks, rw_SO2$data,rw_SO2$fitted), aes(x=louisiana_df$n_weeks))+
  geom_line(aes(y=rw_SO2$data))+
  geom_line(aes(y=rw_SO2$fitted), col='red')
```

\bigskip
\section{$\text{CO}$ series}
As exploratory step, we decompose the time series in its structural component by using an additive model. Figure \ref{fig:decompose_CO}, that plots together the time series with such components, shows that, in fact, a seasonal factor seems to be present. Secondly, the series is characterized by a negative and slightly noisy trend up to a clear breakponit around $2010$. After this break, indeed, the trend line flattens and the trend disappears, as could be guessed also looking at the observed series. Finally, the random component, although seeming really erratic, clearly shows another braekpoint affecting in this case the variance slightly before $2010$. These features suggest that the process may be well described by a Hidden Markov Model.  

```{r fig.width=8, fig.height=5, fig.cap="\\label{fig:decompose_CO}Additive decomposition into structural components"}

ts_CO<-ts(louisiana_df$weekly_CO_mean,start=c(2000,1,1),end = c(2016,4,27),frequency = 48)
dec_CO<-decompose(ts_CO,type=c("additive"))
plot(dec_CO)
```

\newpage
\section{Some experiments}
Given the $CO$ decomposition seen in last paragraph and the presence of a latent state process discussed above it is suitable to explore the flexibility of DLMs to take into account such results. We have seen that an HMM can be a good first way to take into account the latent state process, hovewer, it has the limit of considering the latent state process $\theta_t$ discrete. Given the the nature of the latent state process that we are considering, i.e. car traffic or level of economic activities, it seems natural to consider a continuos latent state process. In our first application we look at the family of structural DLMs. Linearity and Gaussian assumptions simplify the computations and have been proved powerful and usuful tools in many applications. Furthermore, we can easily include in our model a trend and a seasonal component. \bigskip

Iniziamo con modelli semplici e poi mano a mano li complichiamo per renderli più adeguati alla nostra serie. Inizio ad adattare il random walk plus noise dell'esercizio 5:  \bigskip

\section{HMM with trend}
The series shows a clear break around week 400

We could specify the model as

\begin{align*}
\begin{cases}
Y_{t}=\mu_{1} + \beta_{1,t}t + \varepsilon_{t}, \quad \varepsilon_{t}\overset{i.i.d.}{\sim}N(0,\sigma_{1}^2) \qquad \text{if} \; S_{t}=1 \\
Y_{t}=\mu_{2}+ + \beta_{2,t}t + \varepsilon_{t}, \quad \varepsilon_{t}\overset{i.i.d.}{\sim}N(0,\sigma_{2}^2) \qquad \text{if} \; S_{t}=2 \\
\end{cases}
\end{align*}

we will expect in particular a $\mu_1 > \mu_2$ and $\beta_1<0$  $\beta_2=0$

\section{Randon walk plus noise model}

Let's model the series with a random walk plus noise (or local level) model. This is a time-invariant DLM with univariate Gaussian observation and state process, i.e. $\underset{1 \times 1}{Y_t}$ and $\underset{1 \times 1}{\theta_t}$, as well as with constant system matrices $\underset{1 \times 1}F=1$ and $\underset{1 \times 1}G=1$:
\begin{eqnarray*} 
& Y_t = \theta_t + v_t \quad            & v_t \overset{i.i.d.}\sim N(0, V) \\
& \theta_t = \theta_{t-1} + w_t \quad   & v_t \overset{i.i.d.}\sim N(0, W)
\end{eqnarray*}
where the variances of both processes are constant and the prior distribution is also assumed to be Gaussian with mean $m_0$ and variance $C_0$. Furthermore, we assume that the error sequences $(v_t)$ and $(w_t)$ are independent, both within them and between them (pg. 42). We estimate the unknown variances $V=\sigma_v^2$ and $W=\sigma_w^2$ by maximum likelihood setting, with a slightly abuse of freedom, the mean of the initial distribution $m_0$ equal to the first observation.Indeed, the physical representation of the underlying state of the dynamic system in our study is not so evident, and as a consequence we can't rely on known variances. Notice that the advantage of considering this model with respect with the HMM is that now the latent state process $\theta_t$ is a continuous random varible and hence, as previously discussed, allows to better caputure the phenomenon of interest. Another possible interpretation that we can give to the random walk plus noise model is to consider the observed $CO$ level as measurements of the unobservable average $CO$ level plus a random error.
\subsection{Estimation of unknown parameters by MLE}

```{r} 
#definition of the funtion of the rw which for each possible parameters build the dlm of interest. We assume that our initial distibution is centered at the first actual observation for simplicity. 
buildrw <- function(param){
    dlmModPoly(order=1, dV=param[1], dW=param[2], m0=louisiana_df$weekly_CO_mean[1])}

#MLE computation
outMLE <- dlmMLE(louisiana_df$weekly_CO_mean, parm = rep(200, 2), buildrw, lower=c(0.00001, 0), hessian=TRUE)

#output of the optimization algorithm:
#names(outMLE)
#outMLE$par  # MLE estimates 
#outMLE$value  # negative loglikelihood $-loglik(MLE)$
#outMLE$counts # number of iterations of the optimization algorithm
#outMLE$convergence # an integer code. 0 indicates successful convergence
#outMLE$hessian #Hessian matrix of - liglik(MLE)
AsymCov=solve(outMLE$hessian) #asymptotic covariance matrix of the MLEs
#sqrt(diag(AsymCov)) # asymptotic standard errors

#model
rw_CO <- buildrw(outMLE$par)

```

We can estimate, using the package *dlm*, the unknown variances, that we report with the asymptotic standard errors: 

\begin{eqnarray*} 
v_t = V = \sigma^2_v = 0.00954 \quad (\pm 0.0011) \\
w_t = w = \sigma^2_w = 100 0.00468 \quad (\pm 0.00095)
\end{eqnarray*}

Particularly, we check the consistency of our estimates trying different initial values of the optimization algorithm and verifyng its convergence.  

\subsection{Model estimation and filtering}

Our first goal is to compute the \textit{filtering states estimates} $m_t =\mathbb{E}\;(\theta_t \mid y_{1:t})$ obtained by implementing the Kalman filter. Indeed, our data are suppose to arrive sequentially in time every week. In particular, we recall that the Kalman filter procedure, given the \textit{filtering distribution} in the previous iteration, $\theta_{t-1} \mid y_{1:t-1} \sim N(m_{t-1},C_{t-1})$, follows three steps:

\begin{itemize}
\item State forecasting step: 
$\theta_t \mid y_{1:t-1} \sim N(a_t, R_t), \quad \mbox{state prediction}$
\item Observation forecasting step:
$Y_t \mid y_{1:t-1} \sim N(f_t, Q_t), \quad \mbox{observation prediction}$
\item Filtering step: once $y_t$ becomes available
$\theta_t \mid y_{1:t} \sim N(m_t, C_t), \quad \mbox{filtering distribution}.$
\end{itemize}


```{r}
# Kalman filter
filt_CO <- dlmFilter(louisiana_df$weekly_CO_mean,rw_CO)

# Remove the first item (m_0) in the vector of filtered states
filt_est=dropFirst(filt_CO$m)

# Compute the vector of state variances from the Singular Variance Decomposition matrices
list_c <- dlmSvd2var(filt_CO$U.C, filt_CO$D.C)

# Compute standard deviations
vol_C <- sqrt(unlist(list_c))

# Kalman smoother
smooth_CO <- dlmSmooth(louisiana_df$weekly_CO_mean,rw_CO)

# Remove the first item (m_0) in the vector of moothed states
smooth_est <- dropFirst(smooth_CO$s)
```
\medskip

Importantly, notice that we can consider to have observations on the the $CO$ time series for a certain period with the aim of retrospectively study the behavior of the system underlying the observations. As such, it seems at least desirable to consider the problem of smoothing, solved computing the conditional distribution of $\theta_{1:t}$ given $y_{1:t}$. Hence, even if we focus our attention on the filtering estimates, we also apply to our data the Kalman smoother.We report in Figure \ref{fig:filter_est} the \textit{filtering states estimates} $(m_1, \ldots, m_T)$ (red line) and the \textit{smoothing states estimates} $(m_1, \ldots, m_T)$ (blue line)  together with the observed series (black line) (è quello che viene fatto a pagina 65, che ne pensate? meglio solo filter/solo smoothing? l'ho fatto per sperimentare un pò).
Note that the initial guess on the location parameter of the state distribution, $m_0$, has to be removed from the series of states estimates and is, therefore, not included in the plot. 
\bigskip

```{r, fig.width=7, fig.height=2, fig.cap="\\label{fig:filter_est} Filtering states estimates"}
# Df
df_CO_filt <-data.frame(louisiana_df$n_weeks, louisiana_df$weekly_CO_mean, filt_est, smooth_est)
colnames(df_CO_filt) <- c("week", "CO", "filt_est", "smooth_est")
# Plot
colors <- c("Observed data" = "black", "Filtering estimates" = "red", "Smoothing estimates" = "blue")
ggplot(df_CO_filt, aes(x=week)) +
  geom_line(aes(y=CO, color="Observed data"), size=0.4) +
  geom_line(aes(y=filt_est, color = "Filtering estimates"), size=0.4) +
  geom_line(aes(y=smooth_est, color = "Smoothing estimates"), size=0.4) +
  theme_classic(base_size=9.5) +
  theme(panel.grid.minor = element_line(size=0.5)) +
  labs(y=expression(CO~(ppm)), x="Week", color = "") +
  scale_x_continuous(breaks=seq(0,900,100)) +
  theme(axis.title.x=element_text(family="Times", margin=margin(t=5)),
        axis.title.y=element_text(family="Times", margin=margin(r=5)))+
        scale_color_manual(values = colors)+
        theme(legend.justification = c("left", "top"),
              legend.box.just = "left",legend.margin = margin(2, 2, 2, 2))
```

Then, we compute the \textit{filtering variances} $(C_1, \ldots, C_T)$.
In principle, they could be computed through the recursive formulas of Kalman filter. However, due to potential numerical issues in the application of such recursive approach, the computation of state variances relies on the Singular Value Decomposition procedure.

In Figure \ref{fig:filter_sd} we report the filtered standard deviations $\sqrt{C_{t}} = V(\theta_t \mid y_{1:t-1})^{\frac{1}{2}}$. 
\bigskip

```{r}
vol_C_plot <- vol_C[2:853]
```

```{r, fig.width=5, fig.height=2, fig.cap="\\label{fig:filter_sd} Standard deviations of states estimates"}

df_CO_sd <-data.frame(louisiana_df$n_weeks, vol_C_plot)
colnames(df_CO_sd) <- c("week", "vol_C_plot")
 ggplot(df_CO_sd, aes(x=week))+
  geom_line(aes(y=vol_C_plot), color = "darkgreen", size=0.4) +
  theme_classic(base_size=9.5) +
  theme(panel.grid.minor = element_line(size=0.5)) +
  labs(x="Year", y="Standard deviation") +
  labs(y="Standard deviation", x="Week", color = "") +
  scale_x_continuous(breaks=seq(0,900,100)) +
  theme(axis.title.x=element_text(family="Times", margin=margin(t=5)),
        axis.title.y=element_text(family="Times", margin=margin(r=5)))
```

Interestingly, the filtered standard deviations decrease with a fast pace until converging to a limit value equal to `r round(vol_C_plot[7], 2)` after few iterations.

Finally, we compute the $(1-\alpha)$ Bayesian credible intervals for the filtering state estimates, which are given by:
\begin{align*}
\bigg[ m_t - Z_{1 - \frac{\alpha}{2}} \sqrt{C_{t}}, \quad m_t - Z_{1 - \frac{\alpha}{2}}\sqrt{C_{t}} \bigg]
\end{align*}
with $\alpha=0.05$.

We plot in Figure \ref{fig:filter_est_ci} the observations with their filtering state estimates and their 95% level credible intervals.
\bigskip

```{r, fig.width=7, fig.height=2, fig.cap="\\label{fig:filter_est_ci} Filtering estimates with credible intervals"}

upper <- filt_est + 1.96*vol_C_plot 
lower <- filt_est - 1.96*vol_C_plot 

df_CO_ci <-data.frame(louisiana_df$n_weeks, louisiana_df$weekly_CO_mean, filt_est, upper, lower)
colnames(df_CO_ci) <- c("week", "CO", "filt_est", "upper", "lower")  
colors <- c("Observed data" = "black", "Filtering estimates with C.I." = "red")

 ggplot(df_CO_ci, aes(x=week))+
  geom_line(aes(y=CO, color = "Observed data"), size=0.3) +
  geom_line(aes(y=filt_est, color = "Filtering estimates with C.I."), size=0.5) +
  geom_line(aes(y=upper), color = "red", linetype = "dashed", size=0.1) +
  geom_line(aes(y=lower), color = "red", linetype = "dashed", size=0.1) +
  geom_ribbon(aes(ymin=lower, ymax=upper), alpha=.1, fill="red") +
  theme_classic(base_size=9.5) +
  theme(panel.grid.minor = element_line(size=0.5)) +
  labs(y=expression(CO~(ppm)), x="Week") +
  scale_x_continuous(breaks=seq(0,900,100)) +
  theme(axis.title.x=element_text(family="Times", margin=margin(t=5)),
        axis.title.y=element_text(family="Times", margin=margin(r=5)))+
   scale_color_manual(values = colors)+
        theme(legend.justification = c("left", "top"),
              legend.box.just = "left",legend.margin = margin(2, 2, 2, 2))
```

We now plot in Figure \ref{fig:forecast_ci} the Lousiana CO time series alongside with the one-step-ahead forecasts for the observation process (blue line), i.e. $\hat{y}_{t}=\mathbb{E}\;({Y_{t} \mid y_{1:t-1})}=f_t$, where $f_t$ is the location parameter of the observation's forecast distribution. The credible intervals is also showed, where the credible intervals at the $(1-\alpha)$ level are given by:
\begin{align*}
\bigg[ f_t - Z_{1 - \frac{\alpha}{2}} \sqrt{Q_{t}}, \quad f_t - Z_{1 - \frac{\alpha}{2}}\sqrt{Q_{t}} \bigg]
\end{align*}
with $\alpha=0.05$.

The variances $Q_t's$ are just a byproduct of the procedure outlined in Question 1. Indeed, they are obtained as:
\begin{align*}
Q_t = F_t \, R_t \, F_{t}'+ V_t = C_{t-1}+\sigma_{v}^2+\sigma_{w}^2
\end{align*}
with $F_t=1$.
\bigskip

```{r}
# Create the vector of means for the observation process.
# Notice that here we do not have to remove the first element of the series
mean_forecast<-filt_CO$f

# Obtain the variance Q
filt_Q <- unlist(list_c) + as.vector(rw_CO$V) + as.vector(rw_CO$W)
filt_Q <- filt_Q[-1]

# Create upper and lower bounds of the credible interval
upper_for <- mean_forecast + 1.96*sqrt(filt_Q) 
lower_for <- mean_forecast - 1.96*sqrt(filt_Q) 
```

```{r, fig.width=7, fig.height=2, fig.cap="\\label{fig:forecast_ci} One-step ahead forecasts with credible intervals"}
# Df
df_CO_for <-data.frame(louisiana_df$n_weeks, louisiana_df$weekly_CO_mean,  mean_forecast, upper_for, lower_for)

colnames(df_CO_for) <- c("week", "CO", "mean_forecast", "upper_for", "lower_for")
colors <- c("Observed data" = "black", "Forecast with C.I." = "blue")

# Plot
ggplot(df_CO_for, aes(x=week)) +
  geom_line(aes(y=CO, color = "Observed data"), size=0.3) +
  geom_line(aes(y=mean_forecast, color = "Forecast with C.I."), size=0.5) +
  geom_line(aes(y=upper_for), color = "blue", linetype = "dashed", size=0.1) +
  geom_line(aes(y=lower_for), color = "blue", linetype = "dashed", size=0.1) +
  geom_ribbon(aes(ymin=lower_for, ymax=upper_for), alpha=.1, fill="blue") +
  theme_classic(base_size=9.5) +
  theme(panel.grid.minor = element_line(size=0.5)) +
  labs(y=expression(CO~(ppm)), x="Week") +
  scale_x_continuous(breaks=seq(0,900,100)) +
  theme(axis.title.x=element_text(family="Times", margin=margin(t=5)),
        axis.title.y=element_text(family="Times", margin=margin(r=5)))+
  scale_color_manual(values = colors)+
      theme(legend.justification = c("left", "top"),
              legend.box.just = "left",legend.margin = margin(2, 2, 2, 2))
```

COMMENTI: \bigskip
Commentare quello che otteniamo \\
Ruolo signal to noise \\
aggiungere smoothing (possiamo pensare di avere batch data)   \bigskip
.......................

We can expand the previous model considering a linear growth model, which has the same observation equation as the local level model above, but includes a time-varying slope in the dynamics for $\theta_t$ (R code at page 44, but have to understand how to estimate the variances with the MLE algorithm). Possiamo provare a considerare la varianza non costante, infatti si vede bene che a metà diventa molto minore. per questo possiamo provare a vedere il codice a pagina 47. A pagina 66 c'è un codice R figo per fare un DLM con local level plus a quarterly seasonal component con un bel grafico, da approfondire. 



........................
PROVE
Modelling the trend: linear growth model.

\begin{eqnarray*} 
& Y_t = \mu_t + v_t \quad            & v_t \overset{i.i.d.}\sim N(0, \sigma_{V}^{2}) \\
& \mu_t = \mu_{t-1} + \beta_{t-1} + w_{1t} \quad   & w_t \overset{i.i.d.}\sim N(0,  \sigma_{w1}^{2})\\
& \beta_t = \beta_{t-1} + w_t \quad   & w_{2t} \overset{i.i.d.}\sim N(0,  \sigma_{w2}^{2})
\end{eqnarray*}
(+ assumptions)

```{r}
# MOdel for the trend, varianze messe a caso come nell'help giusto per provare, poi userei gli MLE 
trend <- dlmModPoly(order = 2, dV = 1, dW = c(0, 1),
           m0 = c(1,1), C0 = 1e+07 * diag(nrow = 2))
```

Modelling the seasonal factor:\bigskip

The frequency is 52, the number of weeks

```{r}
# Model for the seasonal component, varianze messe a caso come nell'help giusto per provare, poi userei gli MLE 
seas <- dlmModSeas(frequency = 52, dV = 1, dW = c(1, rep(0, 50)),
           m0 = rep(0, 51),
           C0 = 1e+07 * diag(nrow =51))
```



```{r}
# Model 
# mod = trend + seas
# 
# # Kalman filter
# filt_mod <- dlmFilter(louisiana_df$weekly_CO_mean, mod)
# 
# # Remove the first item (m_0) in the vector of filtered states
# filt_est=dropFirst(filt_mod$m)
# 
# # Compute the vector of state variances from the Singular Variance Decomposition matrices
# list_c <- dlmSvd2var(filt_mod$U.C, filt_mod$D.C)
# 
# # Compute standard deviations
# vol_C <- sqrt(unlist(list_c))

```

```{r, fig.width=7, fig.height=2, fig.cap="\\label{fig:filter_est} Filtering states estimates"}
# Df
# df_mod_filt <-data.frame(louisiana_df$n_weeks, louisiana_df$weekly_CO_mean, filt_est)
# 
# # Plot
# colors <- c("Observed data" = "black", "Filtering estimates" = "red")
# ggplot(df_mod_filt, aes(x=louisiana_df.n_weeks)) +
#   geom_line(aes(y=louisiana_df.weekly_CO_mean, color="Observed data"), size=0.4) +
#   geom_line(aes(y=filt_est, color = "Filtering estimates"), size=0.4) +
#   geom_vline(xintercept = as.Date('1899/01/01'), color= "black", size=0.3) +
#   theme_classic(base_size=9.5) +
#   theme(panel.grid.minor = element_line(size=0.5)) +
#   labs(x="Year", y="State level", color = "") +
#   scale_x_date(date_breaks="20 years", limits=as.Date(c('1871/01/01','1970/01/01')),
#                labels=date_format("%Y")) +
#   scale_y_continuous(breaks = seq(500,1500,300), limits = c(400,1500)) +
#   theme(axis.title.x=element_text(family="Times", margin=margin(t=5)),
#         axis.title.y=element_text(family="Times", margin=margin(r=5)))+
#         scale_color_manual(values = colors)+
#         theme(legend.justification = c("left", "top"),
#               legend.box.just = "left",legend.margin = margin(2, 2, 2, 2))
```



\newpage
\section{References}